{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "o3G7ok8Usi3I",
      "metadata": {
        "id": "o3G7ok8Usi3I"
      },
      "source": [
        "# Cross-Lingual Word Embedding Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KvCofcfsz1Ds",
      "metadata": {
        "id": "KvCofcfsz1Ds"
      },
      "source": [
        "## Step 01: Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7974e8c1-7391-431f-96bb-91827cd7f3a2",
      "metadata": {
        "id": "7974e8c1-7391-431f-96bb-91827cd7f3a2"
      },
      "source": [
        "## 1-a: pre-trained FastText embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AjHe8tYks5au",
      "metadata": {
        "id": "AjHe8tYks5au"
      },
      "source": [
        "### Download pre-trained FastText embeddings\n",
        "\n",
        "Pre-trained monolingual FastText word embedding models for English (`cc.en.300.bin.gz`) and Hindi (`cc.hi.300.bin.gz`) have been downloaded from [FastText](https://fasttext.cc/docs/en/crawl-vectors.html). These models, provided by Facebook AI, contain 300-dimensional word vectors trained on Common Crawl data. After downloading, the `.gz` files are decompressed using `gunzip` to obtain the binary `.bin` files for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3de9978-edce-46d3-bf74-78d96d667992",
      "metadata": {
        "id": "a3de9978-edce-46d3-bf74-78d96d667992",
        "outputId": "6e167be4-a670-4c79-fcfe-8d21fc591e17"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f5b06d-c492-4ffa-b1ce-3161fd8b82ae",
      "metadata": {
        "id": "a8f5b06d-c492-4ffa-b1ce-3161fd8b82ae",
        "outputId": "0ad7b774-34e1-4ff6-dc5a-e2faa6f81da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-04 12:27:28--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.158.20.120, 108.158.20.43, 108.158.20.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.158.20.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4371554972 (4.1G) [application/octet-stream]\n",
            "Saving to: ‘cc.hi.300.bin.gz’\n",
            "\n",
            "cc.hi.300.bin.gz    100%[===================>]   4.07G  16.4MB/s    in 3m 14s  \n",
            "\n",
            "2025-04-04 12:30:43 (21.5 MB/s) - ‘cc.hi.300.bin.gz’ saved [4371554972/4371554972]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01ecf4a-e3b8-4aef-9697-fcda35bc8e25",
      "metadata": {
        "id": "d01ecf4a-e3b8-4aef-9697-fcda35bc8e25"
      },
      "outputs": [],
      "source": [
        "!gunzip cc.en.300.bin.gz\n",
        "!gunzip cc.hi.300.bin.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kov9u8O5s-aa",
      "metadata": {
        "id": "Kov9u8O5s-aa"
      },
      "source": [
        "### Install Required Libraries\n",
        "\n",
        "`fasttext`: For loading and working with pre-trained FastText word embedding models.\n",
        "\n",
        "`torch`, `torchvision`, and `torchaudio`: libraries from the PyTorch ecosystem, used here to handle the computation effectively, the installation uses the CUDA 11.8-compatible wheel index for GPU acceleration support.\n",
        "\n",
        "`scikit-learn`: Provides tools for evaluation metrics and nearest neighbor search.\n",
        "\n",
        "`scipy`: Offers efficient linear algebra utilities.\n",
        "\n",
        "`matplotlib`: Used for visualizing aligned embeddings and similarity graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da94caa5-a90c-40e3-995b-fad4b3b763de",
      "metadata": {
        "id": "da94caa5-a90c-40e3-995b-fad4b3b763de",
        "outputId": "8bd2ca2f-0db4-4df6-bffb-ec46d79767f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (68.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.24.1)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296259 sha256=e2d3767aaf115e28f77b9e935910af8badc36d92b557a6b8f3acc2eaac09eedb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65a431a-b701-4fc4-af1f-362b83057e84",
      "metadata": {
        "id": "a65a431a-b701-4fc4-af1f-362b83057e84",
        "outputId": "8b082601-efa3-47ee-a25e-6a754bb1aa6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "Using cached matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib\n",
            "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.1 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -U scikit-learn scipy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_hBFG8ovjye",
      "metadata": {
        "id": "H_hBFG8ovjye"
      },
      "source": [
        "### Load and Use Pre-trained FastText Model\n",
        "\n",
        "Now, we load the pre-trained FastText English word embedding model (cc.en.300.bin) and retrieves the 300-dimensional vector for the word \"king\". FastText can generate embeddings even for out-of-vocabulary (OOV) words using subword information. The execution time is also measured to give an idea of loading and inference speed. Current inference time is 3.9783 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae42e030-0ca5-47be-b5c4-01757a797c83",
      "metadata": {
        "id": "ae42e030-0ca5-47be-b5c4-01757a797c83",
        "outputId": "fced25a7-a731-481b-c8c9-92dae0b74516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-2.63642855e-02 -4.38338369e-02 -5.22461310e-02  2.49765869e-02\n",
            "  1.59946546e-01  4.98980191e-03  2.51637166e-03 -1.62712112e-02\n",
            " -6.62135556e-02 -1.67888845e-03 -1.39499649e-01 -5.72493225e-02\n",
            " -1.45975351e-01 -1.56568401e-02  3.75731173e-03  8.14326331e-02\n",
            "  9.02080238e-02 -6.22668210e-03 -1.21208653e-01  8.42568502e-02\n",
            "  6.83858395e-02  1.01658493e-01 -5.07243127e-02  9.16049480e-02\n",
            "  5.08386921e-03  6.28780201e-02  5.67676872e-02  1.91132650e-01\n",
            "  4.35085818e-02  1.80901110e-01 -1.74744725e-02  7.06654340e-02\n",
            " -6.06337450e-02  3.89074199e-02  1.44602428e-03 -1.25214964e-01\n",
            "  8.63592885e-03 -7.98915625e-02 -1.00960366e-01  4.66771051e-02\n",
            "  5.39167747e-02  4.82006092e-03 -2.03307956e-01 -1.17739499e-01\n",
            " -1.37199834e-01 -4.92817685e-02 -1.87217459e-01 -7.17959851e-02\n",
            " -1.86646730e-02 -9.93231237e-02 -5.15213236e-02 -1.93316743e-01\n",
            " -8.94939303e-02 -1.71539113e-01 -1.03669807e-01 -7.04649240e-02\n",
            "  1.29511207e-01  5.56146055e-02 -4.56965044e-02 -7.34248012e-03\n",
            "  6.97860867e-02  1.69947833e-01  3.10327169e-02  5.91522716e-02\n",
            " -8.91570747e-02  1.04047880e-01 -5.08228168e-02  1.56981260e-01\n",
            "  1.38354123e-01 -9.97450016e-03  2.92297285e-02  2.03259155e-01\n",
            " -3.15062096e-03 -6.43643364e-02 -2.74570972e-01  1.08004212e-01\n",
            " -7.50373602e-02 -3.31273973e-02  1.68551207e-02  3.04881241e-02\n",
            " -3.62115651e-02  8.74452889e-02  1.11329509e-02  2.68645063e-02\n",
            " -7.08571821e-03 -2.58087125e-02 -4.53866273e-03 -6.63428903e-02\n",
            " -5.25312535e-02 -2.38072332e-02 -1.16000466e-01  1.33487135e-01\n",
            " -2.33370923e-02  1.01321653e-01 -2.84816176e-02  6.99471235e-02\n",
            "  5.94794080e-02 -3.12442761e-02  9.96296033e-02 -9.60462987e-02\n",
            " -3.27447765e-02  1.32649690e-01 -7.74053261e-02 -3.05452459e-02\n",
            " -4.82974909e-02 -9.31103155e-03  4.92358953e-02  1.27702221e-01\n",
            "  1.52136413e-02 -6.89754635e-02  3.81627008e-02  1.48964554e-01\n",
            "  2.59358287e-02  3.15686166e-02 -3.75460275e-02 -3.22045460e-02\n",
            "  6.52550533e-03  6.29867092e-02 -4.78249863e-02 -8.24730843e-02\n",
            " -3.23293060e-02 -1.20065711e-01  2.90956944e-02  6.78243339e-02\n",
            "  8.16046149e-02  2.02703606e-02 -1.39618635e-01  9.63046476e-02\n",
            " -1.42421201e-01 -4.51344661e-02 -1.27089359e-02  1.10854320e-02\n",
            " -3.85885164e-02  2.25833915e-02 -1.08815163e-01 -1.36975110e-01\n",
            "  9.15990099e-02  1.36979327e-01  2.05856919e-01  1.61201283e-02\n",
            " -8.97449777e-02 -8.50699991e-02  1.80081427e-02 -1.45724982e-01\n",
            " -6.40569925e-02 -2.87944600e-02 -1.85657188e-01 -1.05159611e-01\n",
            "  8.60162973e-02 -1.19275272e-01  3.13830888e-03  4.35946211e-02\n",
            " -2.94638574e-02 -5.80169484e-02 -7.43187964e-02 -6.81279004e-02\n",
            "  4.28178627e-03  2.47925576e-02 -5.52196279e-02  4.89809215e-02\n",
            "  2.41583195e-02 -5.86985052e-03  1.50820628e-01 -1.71622664e-01\n",
            " -2.79987380e-02 -8.41945410e-02  2.45981030e-02 -5.13311736e-02\n",
            "  8.11809674e-02 -4.38897684e-02 -3.67027484e-02  8.58394802e-02\n",
            "  1.89167447e-02  9.81604904e-02 -2.10215524e-02 -9.18451697e-02\n",
            "  4.16197814e-02  3.72997858e-02 -2.53814217e-02  7.56689906e-02\n",
            " -5.52651398e-02  5.15166819e-02  5.78585193e-02 -1.13078147e-01\n",
            " -1.31233573e-01  1.98879652e-02  5.17351776e-02 -1.33150861e-01\n",
            "  8.34213104e-03 -3.06397229e-02  2.26650923e-01  1.93806678e-01\n",
            "  2.68921461e-02 -7.12896511e-02  6.03743792e-02  9.77210030e-02\n",
            "  6.09308891e-02 -2.61368863e-02 -5.99689856e-02 -5.24861179e-02\n",
            " -4.83104885e-02  7.28225857e-02 -1.82350785e-01 -5.07587008e-02\n",
            "  6.97027445e-02 -1.01331189e-01  1.07080504e-01  1.28510892e-01\n",
            "  9.44370925e-02 -1.64818019e-03  3.74368392e-02  2.60068737e-02\n",
            " -1.58898175e-01  4.27626036e-02 -1.41406000e-01 -1.06626293e-02\n",
            " -8.82082805e-02  1.92979965e-02  5.96629195e-02  5.97821996e-02\n",
            " -5.79583198e-02  3.44996750e-02 -6.58642352e-02  4.48877513e-02\n",
            " -1.86586022e-01 -1.24391131e-01  1.32317677e-01 -4.36709300e-02\n",
            " -4.57718074e-02 -2.56884605e-01 -9.85114276e-02 -1.60375610e-04\n",
            "  1.09038558e-02  1.38050532e-02  9.97474417e-02  5.01477830e-02\n",
            "  1.03953376e-01  1.58712223e-01  7.36273751e-02 -8.92454609e-02\n",
            " -2.21837722e-02 -1.07683539e-02 -5.13628125e-02 -1.06365465e-01\n",
            "  1.24920994e-01 -2.97540985e-02  3.98862213e-02 -4.22692448e-02\n",
            " -9.01315361e-02  4.86209616e-02  2.54198909e-02 -9.57448930e-02\n",
            " -3.86590175e-02  2.03618214e-01  1.18185788e-01  5.32859191e-02\n",
            " -2.04965442e-01 -2.05633745e-01 -6.00707345e-02 -1.80176944e-02\n",
            " -2.09152456e-02 -7.20111728e-02 -4.49218564e-02  4.39144894e-02\n",
            " -5.37205487e-03  4.73969951e-02 -8.87135863e-02  2.18831580e-02\n",
            "  2.87797116e-02 -1.29535403e-02 -1.03977367e-01  7.81352371e-02\n",
            " -1.22202337e-01 -1.03033511e-02 -6.56237006e-02 -9.67411846e-02\n",
            " -6.48837984e-02 -3.11636981e-02 -9.04959068e-02 -2.95870658e-02\n",
            "  4.36350964e-02  7.04264343e-02 -5.88431470e-02 -6.41893744e-02\n",
            "  3.23273055e-02 -2.15162765e-02 -5.02031967e-02  5.04927151e-02\n",
            "  7.30866790e-02 -4.80472436e-03 -6.68786094e-02 -9.35646519e-02\n",
            " -2.37394542e-01  7.00090528e-02  5.92781082e-02  1.29139364e-01\n",
            " -2.11542413e-01  2.16273785e-01 -8.78579915e-02 -2.73688175e-02]\n",
            "Vector length: 300\n",
            "Execution time: 3.9783 seconds\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "import time\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Load pre-trained binary model\n",
        "model_en = fasttext.load_model(\"cc.en.300.bin\")\n",
        "\n",
        "# Get vector for a word (works even for OOV words!)\n",
        "vec = model_en.get_word_vector(\"king\")\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(vec)\n",
        "print(\"Vector length:\", len(vec))\n",
        "print(f\"Execution time: {end_time - start_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69af3ab-691d-4bb0-ae29-006f8d7cedaf",
      "metadata": {
        "id": "b69af3ab-691d-4bb0-ae29-006f8d7cedaf"
      },
      "source": [
        "## 1-B: Limiting vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfXBH_F9wJzj",
      "metadata": {
        "id": "bfXBH_F9wJzj"
      },
      "source": [
        "###  Limit vocabulary to the top 100,000 most frequent words in each language: Prepare FastText Embedding Matrix\n",
        "\n",
        "Here, we extracts the top 100,000 most frequent English words from the FastText model and retrieves their corresponding 300-dimensional word vectors. These vectors are then converted into a PyTorch tensor (`embedding_matrix_en`) and moved to the GPU for efficient computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc82a26a-663b-4cac-bccc-539863f6f2f0",
      "metadata": {
        "id": "dc82a26a-663b-4cac-bccc-539863f6f2f0",
        "outputId": "75851744-d9fc-4460-d231-913a0fb4f25d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_107317/2511608841.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  embedding_matrix_en = torch.tensor(vectors_en, dtype=torch.float32).cuda()  # Move to GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "top_100k_words_en = model_en.get_words()[:100000]\n",
        "vectors_en = [model_en.get_word_vector(w) for w in top_100k_words_en]\n",
        "embedding_matrix_en = torch.tensor(vectors_en, dtype=torch.float32).cuda()  # Move to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AY4sv7Powlq5",
      "metadata": {
        "id": "AY4sv7Powlq5"
      },
      "source": [
        "Further, we retrieve the 300-dimensional FastText vector for the word `\"king\"`, converts it into a PyTorch tensor, and moves it to the GPU for accelerated computation. Now, The execution time is measured and found to be 0.0028 seconds (from 3.9783 seconds as we have seen last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f573fae-d030-42bf-954d-6c26ee20cb26",
      "metadata": {
        "id": "5f573fae-d030-42bf-954d-6c26ee20cb26",
        "outputId": "2e0b1188-9bae-4bc5-eeb4-0b3b74cd20d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6364e-02, -4.3834e-02, -5.2246e-02,  2.4977e-02,  1.5995e-01,\n",
            "         4.9898e-03,  2.5164e-03, -1.6271e-02, -6.6214e-02, -1.6789e-03,\n",
            "        -1.3950e-01, -5.7249e-02, -1.4598e-01, -1.5657e-02,  3.7573e-03,\n",
            "         8.1433e-02,  9.0208e-02, -6.2267e-03, -1.2121e-01,  8.4257e-02,\n",
            "         6.8386e-02,  1.0166e-01, -5.0724e-02,  9.1605e-02,  5.0839e-03,\n",
            "         6.2878e-02,  5.6768e-02,  1.9113e-01,  4.3509e-02,  1.8090e-01,\n",
            "        -1.7474e-02,  7.0665e-02, -6.0634e-02,  3.8907e-02,  1.4460e-03,\n",
            "        -1.2521e-01,  8.6359e-03, -7.9892e-02, -1.0096e-01,  4.6677e-02,\n",
            "         5.3917e-02,  4.8201e-03, -2.0331e-01, -1.1774e-01, -1.3720e-01,\n",
            "        -4.9282e-02, -1.8722e-01, -7.1796e-02, -1.8665e-02, -9.9323e-02,\n",
            "        -5.1521e-02, -1.9332e-01, -8.9494e-02, -1.7154e-01, -1.0367e-01,\n",
            "        -7.0465e-02,  1.2951e-01,  5.5615e-02, -4.5697e-02, -7.3425e-03,\n",
            "         6.9786e-02,  1.6995e-01,  3.1033e-02,  5.9152e-02, -8.9157e-02,\n",
            "         1.0405e-01, -5.0823e-02,  1.5698e-01,  1.3835e-01, -9.9745e-03,\n",
            "         2.9230e-02,  2.0326e-01, -3.1506e-03, -6.4364e-02, -2.7457e-01,\n",
            "         1.0800e-01, -7.5037e-02, -3.3127e-02,  1.6855e-02,  3.0488e-02,\n",
            "        -3.6212e-02,  8.7445e-02,  1.1133e-02,  2.6865e-02, -7.0857e-03,\n",
            "        -2.5809e-02, -4.5387e-03, -6.6343e-02, -5.2531e-02, -2.3807e-02,\n",
            "        -1.1600e-01,  1.3349e-01, -2.3337e-02,  1.0132e-01, -2.8482e-02,\n",
            "         6.9947e-02,  5.9479e-02, -3.1244e-02,  9.9630e-02, -9.6046e-02,\n",
            "        -3.2745e-02,  1.3265e-01, -7.7405e-02, -3.0545e-02, -4.8297e-02,\n",
            "        -9.3110e-03,  4.9236e-02,  1.2770e-01,  1.5214e-02, -6.8975e-02,\n",
            "         3.8163e-02,  1.4896e-01,  2.5936e-02,  3.1569e-02, -3.7546e-02,\n",
            "        -3.2205e-02,  6.5255e-03,  6.2987e-02, -4.7825e-02, -8.2473e-02,\n",
            "        -3.2329e-02, -1.2007e-01,  2.9096e-02,  6.7824e-02,  8.1605e-02,\n",
            "         2.0270e-02, -1.3962e-01,  9.6305e-02, -1.4242e-01, -4.5134e-02,\n",
            "        -1.2709e-02,  1.1085e-02, -3.8589e-02,  2.2583e-02, -1.0882e-01,\n",
            "        -1.3698e-01,  9.1599e-02,  1.3698e-01,  2.0586e-01,  1.6120e-02,\n",
            "        -8.9745e-02, -8.5070e-02,  1.8008e-02, -1.4572e-01, -6.4057e-02,\n",
            "        -2.8794e-02, -1.8566e-01, -1.0516e-01,  8.6016e-02, -1.1928e-01,\n",
            "         3.1383e-03,  4.3595e-02, -2.9464e-02, -5.8017e-02, -7.4319e-02,\n",
            "        -6.8128e-02,  4.2818e-03,  2.4793e-02, -5.5220e-02,  4.8981e-02,\n",
            "         2.4158e-02, -5.8699e-03,  1.5082e-01, -1.7162e-01, -2.7999e-02,\n",
            "        -8.4195e-02,  2.4598e-02, -5.1331e-02,  8.1181e-02, -4.3890e-02,\n",
            "        -3.6703e-02,  8.5839e-02,  1.8917e-02,  9.8160e-02, -2.1022e-02,\n",
            "        -9.1845e-02,  4.1620e-02,  3.7300e-02, -2.5381e-02,  7.5669e-02,\n",
            "        -5.5265e-02,  5.1517e-02,  5.7859e-02, -1.1308e-01, -1.3123e-01,\n",
            "         1.9888e-02,  5.1735e-02, -1.3315e-01,  8.3421e-03, -3.0640e-02,\n",
            "         2.2665e-01,  1.9381e-01,  2.6892e-02, -7.1290e-02,  6.0374e-02,\n",
            "         9.7721e-02,  6.0931e-02, -2.6137e-02, -5.9969e-02, -5.2486e-02,\n",
            "        -4.8310e-02,  7.2823e-02, -1.8235e-01, -5.0759e-02,  6.9703e-02,\n",
            "        -1.0133e-01,  1.0708e-01,  1.2851e-01,  9.4437e-02, -1.6482e-03,\n",
            "         3.7437e-02,  2.6007e-02, -1.5890e-01,  4.2763e-02, -1.4141e-01,\n",
            "        -1.0663e-02, -8.8208e-02,  1.9298e-02,  5.9663e-02,  5.9782e-02,\n",
            "        -5.7958e-02,  3.4500e-02, -6.5864e-02,  4.4888e-02, -1.8659e-01,\n",
            "        -1.2439e-01,  1.3232e-01, -4.3671e-02, -4.5772e-02, -2.5688e-01,\n",
            "        -9.8511e-02, -1.6038e-04,  1.0904e-02,  1.3805e-02,  9.9747e-02,\n",
            "         5.0148e-02,  1.0395e-01,  1.5871e-01,  7.3627e-02, -8.9245e-02,\n",
            "        -2.2184e-02, -1.0768e-02, -5.1363e-02, -1.0637e-01,  1.2492e-01,\n",
            "        -2.9754e-02,  3.9886e-02, -4.2269e-02, -9.0132e-02,  4.8621e-02,\n",
            "         2.5420e-02, -9.5745e-02, -3.8659e-02,  2.0362e-01,  1.1819e-01,\n",
            "         5.3286e-02, -2.0497e-01, -2.0563e-01, -6.0071e-02, -1.8018e-02,\n",
            "        -2.0915e-02, -7.2011e-02, -4.4922e-02,  4.3914e-02, -5.3721e-03,\n",
            "         4.7397e-02, -8.8714e-02,  2.1883e-02,  2.8780e-02, -1.2954e-02,\n",
            "        -1.0398e-01,  7.8135e-02, -1.2220e-01, -1.0303e-02, -6.5624e-02,\n",
            "        -9.6741e-02, -6.4884e-02, -3.1164e-02, -9.0496e-02, -2.9587e-02,\n",
            "         4.3635e-02,  7.0426e-02, -5.8843e-02, -6.4189e-02,  3.2327e-02,\n",
            "        -2.1516e-02, -5.0203e-02,  5.0493e-02,  7.3087e-02, -4.8047e-03,\n",
            "        -6.6879e-02, -9.3565e-02, -2.3739e-01,  7.0009e-02,  5.9278e-02,\n",
            "         1.2914e-01, -2.1154e-01,  2.1627e-01, -8.7858e-02, -2.7369e-02],\n",
            "       device='cuda:0')\n",
            "Vector length: 300\n",
            "Execution time: 0.0028 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "query_word = \"king\"\n",
        "query_vec = torch.tensor(model_en.get_word_vector(query_word), dtype=torch.float32).cuda()\n",
        "query_vec\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(query_vec)\n",
        "print(\"Vector length:\", len(query_vec))\n",
        "print(f\"Execution time: {end_time - start_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cj8cXFCBxK_L",
      "metadata": {
        "id": "cj8cXFCBxK_L"
      },
      "source": [
        "Similarly, we extracts the top 100,000 most frequent hindi words from the FastText model and retrieves their corresponding 300-dimensional word vectors. These vectors are then converted into a PyTorch tensor (`embedding_matrix_hi`) and moved to the GPU for efficient computation. We retrieve the 300-dimensional FastText vector for the word `\"राजा\"`, converts it into a PyTorch tensor, and moves it to the GPU for accelerated computation. Now, The execution time is measured and found to be 0.0014 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb40e857-02ed-4546-a37b-c992117be911",
      "metadata": {
        "id": "cb40e857-02ed-4546-a37b-c992117be911"
      },
      "outputs": [],
      "source": [
        "model_hi = fasttext.load_model(\"cc.hi.300.bin\")\n",
        "top_100k_words_hi = model_hi.get_words()[:100000]\n",
        "\n",
        "vectors_hi = [model_hi.get_word_vector(w) for w in top_100k_words_hi]\n",
        "embedding_matrix_hi = torch.tensor(vectors_hi, dtype=torch.float32).cuda()  # Move to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdeda32a-5ed9-4812-b8e8-e0d3bf0fabb7",
      "metadata": {
        "id": "bdeda32a-5ed9-4812-b8e8-e0d3bf0fabb7",
        "outputId": "5e650aec-855a-4f1f-cc80-6caf0578c794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0381,  0.0658, -0.0262, -0.0013,  0.0302,  0.0290, -0.0197, -0.0188,\n",
            "         0.0217, -0.0263,  0.0129, -0.0857,  0.0192,  0.0464,  0.0132, -0.0430,\n",
            "         0.0247, -0.0350,  0.0430,  0.0828,  0.0459,  0.0189,  0.0074, -0.0555,\n",
            "        -0.0606,  0.0246,  0.0334, -0.0684, -0.0458, -0.0366,  0.0253,  0.0053,\n",
            "         0.0383,  0.0292, -0.0343,  0.0736,  0.0115, -0.0532,  0.0301,  0.0354,\n",
            "         0.0369,  0.0095,  0.0501, -0.0206, -0.0202,  0.1286, -0.0876,  0.0126,\n",
            "         0.0399, -0.0640, -0.0162, -0.0055,  0.0123,  0.0489, -0.0661, -0.0215,\n",
            "         0.0391,  0.0033, -0.0087,  0.0350, -0.0118, -0.1038, -0.0528,  0.0834,\n",
            "        -0.0562, -0.0765, -0.0211,  0.0343,  0.0010, -0.0471,  0.0162,  0.0221,\n",
            "         0.0354,  0.0382,  0.1104,  0.0478, -0.0202, -0.0301,  0.0247, -0.0402,\n",
            "         0.0616, -0.0026, -0.0198, -0.0566, -0.0233, -0.0236, -0.0028, -0.0192,\n",
            "         0.0483, -0.1813, -0.0385,  0.0091,  0.0518,  0.1070, -0.0480, -0.0135,\n",
            "         0.0309,  0.0436, -0.0243, -0.0423, -0.0086, -0.0125, -0.0620, -0.0693,\n",
            "         0.0036,  0.0204,  0.0439,  0.0530, -0.0420,  0.0510, -0.0576, -0.0146,\n",
            "         0.0099,  0.0378,  0.0723, -0.0222,  0.0450,  0.0551,  0.0199, -0.0608,\n",
            "         0.0173,  0.0778,  0.0071, -0.0158,  0.0078,  0.0315, -0.0087,  0.0122,\n",
            "         0.0533,  0.0092, -0.0386,  0.0176,  0.0596,  0.0373,  0.1173, -0.0878,\n",
            "        -0.0466, -0.0108,  0.0179,  0.0217,  0.0246, -0.0996, -0.0500, -0.0054,\n",
            "        -0.0243, -0.0065, -0.0016, -0.0298,  0.0567, -0.0012, -0.0201, -0.0385,\n",
            "        -0.0572, -0.0765, -0.0328,  0.0214,  0.0291,  0.0219, -0.0332,  0.1126,\n",
            "        -0.0156, -0.0970,  0.0105, -0.0103,  0.0663,  0.0489, -0.0068, -0.0539,\n",
            "        -0.0594,  0.0034,  0.1541, -0.0161, -0.0689, -0.0756, -0.0136,  0.0292,\n",
            "         0.0440,  0.1244,  0.0422,  0.0185, -0.0064, -0.0101, -0.0384, -0.0080,\n",
            "         0.0482, -0.0366,  0.0644, -0.0445, -0.0917, -0.0192, -0.0291, -0.0595,\n",
            "        -0.0346,  0.1615, -0.0369, -0.0100, -0.0355, -0.0317,  0.0950, -0.0613,\n",
            "        -0.0487,  0.0207, -0.0455, -0.0146, -0.0598,  0.0180, -0.0214, -0.0151,\n",
            "         0.0143,  0.0232, -0.0456, -0.0631, -0.0142, -0.0292, -0.0744, -0.0523,\n",
            "        -0.0937,  0.0604,  0.0321,  0.0235,  0.0790,  0.0186,  0.0013,  0.0021,\n",
            "         0.0099, -0.0459, -0.0330, -0.0237,  0.0449, -0.0434,  0.1142, -0.0804,\n",
            "        -0.0297, -0.0676,  0.0015, -0.0334, -0.0159, -0.0267,  0.0320,  0.0348,\n",
            "         0.0406,  0.0375, -0.0491, -0.0978, -0.0587, -0.0470,  0.0680,  0.0473,\n",
            "         0.0586,  0.0524,  0.0365,  0.0178, -0.0264,  0.0058, -0.0752,  0.0151,\n",
            "         0.0090, -0.0173,  0.0617,  0.0369, -0.0122, -0.0312,  0.0315, -0.0241,\n",
            "         0.0245,  0.0273, -0.0626, -0.0006,  0.0097, -0.0650,  0.0214, -0.0401,\n",
            "         0.0674, -0.0121, -0.0350,  0.0559, -0.0215,  0.0296, -0.0689, -0.0747,\n",
            "        -0.0997,  0.0102,  0.0215, -0.0379, -0.0703,  0.0215, -0.0056,  0.0242,\n",
            "        -0.0168,  0.0347,  0.0309,  0.0112, -0.0501,  0.0037, -0.0309,  0.0159,\n",
            "         0.0477, -0.0916, -0.0232,  0.0386], device='cuda:0')\n",
            "Vector length: 300\n",
            "Execution time: 0.0014 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "query_word = \"राजा\"\n",
        "query_vec = torch.tensor(model_hi.get_word_vector(query_word), dtype=torch.float32).cuda()\n",
        "query_vec\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(query_vec)\n",
        "print(\"Vector length:\", len(query_vec))\n",
        "print(f\"Execution time: {end_time - start_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LzP5kpzsxscX",
      "metadata": {
        "id": "LzP5kpzsxscX"
      },
      "source": [
        "### Find Top-N Similar Words Using Cosine Similarity on GPU\n",
        "The function, `find_similar_gpu`, retrieves the `top-N` (here, N=5) most similar words to a given query word (e.g., `\"king\"`, `\"राजा\"`) using cosine similarity. It has following process:\n",
        "\n",
        "1.   Checks if the query word exists in the top-100k vocabulary.\n",
        "2.   Computes the vector for the query word and moves it to the GPU.\n",
        "3.   Calculates cosine similarity between the query vector and all vectors in the embedding matrix (on GPU).\n",
        "4.   Returns the top-N most similar words, excluding the query word itself.\n",
        "\n",
        "\n",
        "\n",
        "This approach is optimized for fast retrieval using GPU acceleration, hence the computation is very fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7690b5d7-9750-4260-94f0-7b1922ed9247",
      "metadata": {
        "id": "7690b5d7-9750-4260-94f0-7b1922ed9247",
        "outputId": "80364013-698c-45fa-ef80-e2be91deb9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('kings', 0.7550358176231384), ('queen', 0.7068519592285156), ('King', 0.6591265201568604), ('prince', 0.6495252847671509), ('monarch', 0.6183921098709106)]\n"
          ]
        }
      ],
      "source": [
        "def find_similar_gpu(query_word, top_words, embedding_matrix, top_n=5):\n",
        "    if query_word not in top_words:\n",
        "        print(f\"{query_word} not in top-k vocab.\")\n",
        "        return []\n",
        "\n",
        "    # Convert query to vector (still CPU, small cost)\n",
        "    query_vec = torch.tensor(model_en.get_word_vector(query_word), dtype=torch.float32).cuda()\n",
        "\n",
        "    # Compute cosine similarity (GPU)\n",
        "    sim = torch.nn.functional.cosine_similarity(query_vec.unsqueeze(0), embedding_matrix, dim=1)\n",
        "\n",
        "    # Top N indices (excluding itself)\n",
        "    topk = sim.topk(top_n + 1)\n",
        "    similar = [(top_words[i], sim[i].item()) for i in topk.indices if top_words[i] != query_word]\n",
        "\n",
        "    return similar[:top_n]\n",
        "\n",
        "print(find_similar_gpu(\"king\", top_100k_words_en, embedding_matrix_en, top_n=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baaa782e-3d45-4afb-a038-d6cf30f6f2f0",
      "metadata": {
        "id": "baaa782e-3d45-4afb-a038-d6cf30f6f2f0",
        "outputId": "0813c6a0-2be7-4442-c5ee-0710e149af18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('प्रजा', 0.5774529576301575), ('राजाओं', 0.57224041223526), ('महाराजा', 0.5474495887756348), ('महाराजाओं', 0.5429219603538513), ('रानी', 0.5357876420021057)]\n"
          ]
        }
      ],
      "source": [
        "def find_similar_gpu(query_word, top_words, embedding_matrix, top_n=5):\n",
        "    if query_word not in top_words:\n",
        "        print(f\"{query_word} not in top-k vocab.\")\n",
        "        return []\n",
        "\n",
        "    # Convert query to vector (still CPU, small cost)\n",
        "    query_vec = torch.tensor(model_hi.get_word_vector(query_word), dtype=torch.float32).cuda()\n",
        "\n",
        "    # Compute cosine similarity (GPU)\n",
        "    sim = torch.nn.functional.cosine_similarity(query_vec.unsqueeze(0), embedding_matrix, dim=1)\n",
        "\n",
        "    # Top N indices (excluding itself)\n",
        "    topk = sim.topk(top_n + 1)\n",
        "    similar = [(top_words[i], sim[i].item()) for i in topk.indices if top_words[i] != query_word]\n",
        "\n",
        "    return similar[:top_n]\n",
        "\n",
        "print(find_similar_gpu(\"राजा\", top_100k_words_hi, embedding_matrix_hi, top_n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QVbkP_OtzGnx",
      "metadata": {
        "id": "QVbkP_OtzGnx"
      },
      "source": [
        "## 1-c: English-Hindi Bilingual Lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDMYh7yTyWUH",
      "metadata": {
        "id": "yDMYh7yTyWUH"
      },
      "source": [
        "### Load English-Hindi Bilingual Lexicon\n",
        "\n",
        "Now, we download a pre-compiled bilingual dictionary (`en-hi.txt`) containing\n",
        "38221 English-Hindi word pairs from the [MUSE project](https://github.com/facebookresearch/MUSE). The `load_bilingual_lexicon` function reads the file line by line and stores each English-Hindi word pair as a tuple in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2397602-f6bf-4af7-b5bd-86c02f04b1a5",
      "metadata": {
        "id": "a2397602-f6bf-4af7-b5bd-86c02f04b1a5",
        "outputId": "c86972cf-16ba-432d-c9a4-dfa96323d30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-04 12:38:34--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.158.20.21, 108.158.20.43, 108.158.20.120, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.158.20.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt’\n",
            "\n",
            "en-hi.txt           100%[===================>] 909.04K  1.28MB/s    in 0.7s    \n",
            "\n",
            "2025-04-04 12:38:35 (1.28 MB/s) - ‘en-hi.txt’ saved [930856/930856]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4cd6224-0a1f-4c88-885e-15c0ee0f76d2",
      "metadata": {
        "id": "c4cd6224-0a1f-4c88-885e-15c0ee0f76d2",
        "outputId": "a8811189-733e-4af4-f06a-4dd912ba2060",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('and', 'और'),\n",
              " ('was', 'था'),\n",
              " ('was', 'थी'),\n",
              " ('for', 'लिये'),\n",
              " ('that', 'उस'),\n",
              " ('that', 'कि'),\n",
              " ('with', 'साथ'),\n",
              " ('from', 'से'),\n",
              " ('from', 'इससे'),\n",
              " ('this', 'ये'),\n",
              " ('this', 'यह'),\n",
              " ('this', 'इस'),\n",
              " ('utc', 'यूटीसी'),\n",
              " ('utc', 'utc'),\n",
              " ('his', 'उसकी'),\n",
              " ('his', 'उसका'),\n",
              " ('his', 'उसके'),\n",
              " ('not', 'नही'),\n",
              " ('not', 'नहीं'),\n",
              " ('are', 'हैं'),\n",
              " ('talk', 'बात'),\n",
              " ('which', 'जिससे'),\n",
              " ('also', 'भी'),\n",
              " ('has', 'रै'),\n",
              " ('were', 'यहूद'),\n",
              " ('but', 'परन्तु'),\n",
              " ('but', 'लेकिन'),\n",
              " ('but', 'लेकीन'),\n",
              " ('but', 'मगर'),\n",
              " ('but', 'लकिन'),\n",
              " ('one', 'एक'),\n",
              " ('new', 'नया'),\n",
              " ('new', 'नई'),\n",
              " ('first', 'प्रथम'),\n",
              " ('first', 'पहली'),\n",
              " ('first', 'पहले'),\n",
              " ('first', 'पहला'),\n",
              " ('page', 'पृष्ठ'),\n",
              " ('page', 'पेज'),\n",
              " ('you', 'आपको'),\n",
              " ('you', 'आप'),\n",
              " ('you', 'तुम'),\n",
              " ('they', 'उन्होंने'),\n",
              " ('they', 'वे'),\n",
              " ('had', 'था'),\n",
              " ('article', 'लेख'),\n",
              " ('article', 'आलेख'),\n",
              " ('who', 'जिसने'),\n",
              " ('who', 'कौन'),\n",
              " ('who', 'जो'),\n",
              " ('all', 'सभी'),\n",
              " ('all', 'सब'),\n",
              " ('their', 'उनकी'),\n",
              " ('their', 'इनकी'),\n",
              " ('their', 'उनका'),\n",
              " ('their', 'उनके'),\n",
              " ('there', 'वहाँ'),\n",
              " ('there', 'वहां'),\n",
              " ('there', 'वहा'),\n",
              " ('there', 'वहीँ'),\n",
              " ('been', 'महरूम'),\n",
              " ('made', 'बनाया'),\n",
              " ('made', 'बना'),\n",
              " ('its', 'इसका'),\n",
              " ('its', 'इसकी'),\n",
              " ('its', 'इसके'),\n",
              " ('people', 'लोग'),\n",
              " ('people', 'लोगों'),\n",
              " ('may', 'मई'),\n",
              " ('may', 'may'),\n",
              " ('after', 'बाद'),\n",
              " ('other', 'अन्य'),\n",
              " ('should', 'चाहिए'),\n",
              " ('two', 'दो'),\n",
              " ('score', 'अंक'),\n",
              " ('score', 'स्कोर'),\n",
              " ('her', 'उसे'),\n",
              " ('her', 'उसका'),\n",
              " ('her', 'उसकी'),\n",
              " ('her', 'उसके'),\n",
              " ('would', 'होता'),\n",
              " ('more', 'ज्यादा'),\n",
              " ('more', 'आदि'),\n",
              " ('more', 'अधिक'),\n",
              " ('she', 'वो'),\n",
              " ('about', 'परिचय'),\n",
              " ('when', 'कब'),\n",
              " ('when', 'जब'),\n",
              " ('time', 'वक़्त'),\n",
              " ('time', 'वक्त'),\n",
              " ('time', 'समय'),\n",
              " ('team', 'टीम'),\n",
              " ('team', 'टोली'),\n",
              " ('american', 'अमेरिकन'),\n",
              " ('american', 'अमेरिकी'),\n",
              " ('american', 'अमरीकी'),\n",
              " ('such', 'ऐसी'),\n",
              " ('such', 'ऐसे'),\n",
              " ('discussion', 'परिचर्चा'),\n",
              " ('discussion', 'चर्चा'),\n",
              " ('links', 'लिंक'),\n",
              " ('links', 'लिंक्स'),\n",
              " ('links', 'कड़ियाँ'),\n",
              " ('links', 'कड़ी'),\n",
              " ('only', 'सिर्फ'),\n",
              " ('only', 'केवल'),\n",
              " ('only', 'तभी'),\n",
              " ('some', 'कुछ'),\n",
              " ('see', 'देख'),\n",
              " ('see', 'देखें'),\n",
              " ('see', 'देखिए'),\n",
              " ('united', 'यूनाइटेड'),\n",
              " ('united', 'संयुक्त'),\n",
              " ('years', 'सालों'),\n",
              " ('years', 'वर्ष'),\n",
              " ('years', 'साल'),\n",
              " ('years', 'वर्षों'),\n",
              " ('years', 'सालो'),\n",
              " ('school', 'विद्यालय'),\n",
              " ('school', 'school'),\n",
              " ('school', 'स्कूल'),\n",
              " ('world', 'दुनिया'),\n",
              " ('world', 'विश्व'),\n",
              " ('world', 'जग'),\n",
              " ('world', 'संसार'),\n",
              " ('university', 'विश्वविद्यालय'),\n",
              " ('university', 'यूनिवर्सिटी'),\n",
              " ('university', 'विश्वविद्यालयों'),\n",
              " ('during', 'दौरान'),\n",
              " ('out', 'बाहर'),\n",
              " ('out', 'आउट'),\n",
              " ('state', 'अवस्था'),\n",
              " ('state', 'स्टेट'),\n",
              " ('state', 'स्थिति'),\n",
              " ('state', 'राज्य'),\n",
              " ('states', 'राज्यों'),\n",
              " ('states', 'राज्यो'),\n",
              " ('states', 'स्टेट्स'),\n",
              " ('states', 'राज्य'),\n",
              " ('national', 'राष्टीय'),\n",
              " ('national', 'राष्ट्रीय'),\n",
              " ('national', 'नेशनल'),\n",
              " ('wikipedia', 'विकिपीडिया'),\n",
              " ('year', 'वर्ष'),\n",
              " ('year', 'साल'),\n",
              " ('most', 'अधिकांश'),\n",
              " ('most', 'सर्वाधिक'),\n",
              " ('most', 'मोस्ट'),\n",
              " ('most', 'सबसे'),\n",
              " ('city', 'शहर'),\n",
              " ('city', 'सिटी'),\n",
              " ('city', 'नगर'),\n",
              " ('used', 'प्रयुक्त'),\n",
              " ('then', 'फिर'),\n",
              " ('then', 'तब'),\n",
              " ('than', 'than'),\n",
              " ('county', 'लैन'),\n",
              " ('county', 'काउंटी'),\n",
              " ('external', 'बाहरी'),\n",
              " ('external', 'बाह्य'),\n",
              " ('where', 'कहाँ'),\n",
              " ('where', 'कहां'),\n",
              " ('where', 'जहां'),\n",
              " ('where', 'जहाँ'),\n",
              " ('will', 'करेंगे'),\n",
              " ('will', 'होगी'),\n",
              " ('will', 'करेगा'),\n",
              " ('will', 'करेगी'),\n",
              " ('will', 'होगा'),\n",
              " ('what', 'क्या'),\n",
              " ('what', 'कया'),\n",
              " ('delete', 'हटाएँ'),\n",
              " ('any', 'कोई'),\n",
              " ('these', 'ये'),\n",
              " ('january', 'जनवरी'),\n",
              " ('march', 'मार्च'),\n",
              " ('march', 'कूच'),\n",
              " ('august', 'अगस्त'),\n",
              " ('july', 'जुलाई'),\n",
              " ('being', 'होने'),\n",
              " ('film', 'फ़िल्म'),\n",
              " ('film', 'फिल्म'),\n",
              " ('film', 'फिल्मी'),\n",
              " ('him', 'उससे'),\n",
              " ('him', 'उसको'),\n",
              " ('him', 'उसे'),\n",
              " ('many', 'कई'),\n",
              " ('many', 'अनेक'),\n",
              " ('south', 'दक्षिणी'),\n",
              " ('south', 'साउथ'),\n",
              " ('south', 'दक्षिण'),\n",
              " ('september', 'सितम्बर'),\n",
              " ('september', 'सितंबर'),\n",
              " ('like', 'like'),\n",
              " ('like', 'जैसे'),\n",
              " ('october', 'अक्टूबर'),\n",
              " ('october', 'अक्तूबर'),\n",
              " ('three', 'तीन'),\n",
              " ('june', 'जून'),\n",
              " ('well', 'कुआँ'),\n",
              " ('well', 'खैर'),\n",
              " ('well', 'खूब'),\n",
              " ('use', 'उपयोग'),\n",
              " ('war', 'युद्ध'),\n",
              " ('under', 'तहत'),\n",
              " ('under', 'अधीन'),\n",
              " ('under', 'अंतर्गत'),\n",
              " ('them', 'उनसे'),\n",
              " ('them', 'उनके'),\n",
              " ('them', 'उनको'),\n",
              " ('them', 'उन्हें'),\n",
              " ('april', 'अप्रैल'),\n",
              " ('born', 'जन्म'),\n",
              " ('born', 'जन्मे'),\n",
              " ('born', 'born'),\n",
              " ('december', 'दिसंबर'),\n",
              " ('december', 'दिसम्बर'),\n",
              " ('link', 'लिंक'),\n",
              " ('link', 'कड़ी'),\n",
              " ('while', 'जबकी'),\n",
              " ('while', 'जबकि'),\n",
              " ('later', 'बाद'),\n",
              " ('part', 'हिस्से'),\n",
              " ('part', 'हिस्सा'),\n",
              " ('part', 'पार्ट'),\n",
              " ('part', 'भाग'),\n",
              " ('november', 'नवंबर'),\n",
              " ('november', 'नवम्बर'),\n",
              " ('players', 'खिलाडी'),\n",
              " ('players', 'खिलाड़ी'),\n",
              " ('list', 'सूची'),\n",
              " ('please', 'please'),\n",
              " ('please', 'प्लीज'),\n",
              " ('please', 'कृपया'),\n",
              " ('february', 'फरवरी'),\n",
              " ('february', 'फ़रवरी'),\n",
              " ('known', 'ज्ञात'),\n",
              " ('second', 'दूसरा'),\n",
              " ('second', 'सेकेंड'),\n",
              " ('second', 'द्वितीय'),\n",
              " ('second', 'सेकण्ड'),\n",
              " ('second', 'सेकंड'),\n",
              " ('second', 'दूसरी'),\n",
              " ('name', 'नाम'),\n",
              " ('group', 'ग्रुप'),\n",
              " ('group', 'समूह'),\n",
              " ('history', 'इतिहास'),\n",
              " ('series', 'श्रेणी'),\n",
              " ('just', 'सिर्फ'),\n",
              " ('just', 'बस'),\n",
              " ('north', 'उत्तर'),\n",
              " ('work', 'कार्यालय'),\n",
              " ('work', 'काम'),\n",
              " ('work', 'कार्य'),\n",
              " ('before', 'पहले'),\n",
              " ('since', 'जबसे'),\n",
              " ('since', 'चूंकि'),\n",
              " ('season', 'मौसम'),\n",
              " ('season', 'ऋतु'),\n",
              " ('both', 'दोनों'),\n",
              " ('both', 'दोनो'),\n",
              " ('high', 'हाई'),\n",
              " ('high', 'आलीशान'),\n",
              " ('high', 'उच्च'),\n",
              " ('high', 'ऊँची'),\n",
              " ('district', 'जिला'),\n",
              " ('district', 'डिस्ट्रिक्ट'),\n",
              " ('district', 'जिलाध्यक्ष'),\n",
              " ('district', 'ज़िला'),\n",
              " ('now', 'अब'),\n",
              " ('now', 'अभी'),\n",
              " ('comments', 'टिप्पणियां'),\n",
              " ('comments', 'टिप्पणियाँ'),\n",
              " ('comments', 'टिप्पणी'),\n",
              " ('because', 'क्योंकि'),\n",
              " ('because', 'क्योकि'),\n",
              " ('because', 'क्युकी'),\n",
              " ('football', 'फुटबॉल'),\n",
              " ('football', 'फुटबाल'),\n",
              " ('football', 'फ़ुटबॉल'),\n",
              " ('music', 'संगीत'),\n",
              " ('music', 'म्यूज़िक'),\n",
              " ('century', 'शतक'),\n",
              " ('century', 'शताब्दी'),\n",
              " ('century', 'सदी'),\n",
              " ('league', 'लीग'),\n",
              " ('edits', 'संपादन'),\n",
              " ('debate', 'बहस'),\n",
              " ('title', 'शीर्षक'),\n",
              " ('articles', 'लेख'),\n",
              " ('articles', 'आलेख'),\n",
              " ('john', 'जॉन'),\n",
              " ('same', 'समान'),\n",
              " ('same', 'वही'),\n",
              " ('including', 'सहित'),\n",
              " ('english', 'english'),\n",
              " ('english', 'अंग्रेज'),\n",
              " ('english', 'अंग्रेजी'),\n",
              " ('english', 'अंग्रेज़ी'),\n",
              " ('album', 'एलबम'),\n",
              " ('album', 'अलबम'),\n",
              " ('album', 'एल्बम'),\n",
              " ('number', 'संख्या'),\n",
              " ('number', 'नंबर'),\n",
              " ('number', 'क्रमांक'),\n",
              " ('against', 'खिलाफ'),\n",
              " ('against', 'विरुद्ध'),\n",
              " ('against', 'विरूद्ध'),\n",
              " ('family', 'फैमिली'),\n",
              " ('family', 'परिवार'),\n",
              " ('user', 'यूजर'),\n",
              " ('user', 'प्रयोक्ता'),\n",
              " ('user', 'उपयोगकर्ता'),\n",
              " ('user', 'उपयोक्ता'),\n",
              " ('based', 'आधारित'),\n",
              " ('area', 'क्षेत्र'),\n",
              " ('area', 'क्षेत्रफल'),\n",
              " ('york', 'यॉर्क'),\n",
              " ('life', 'जिंदगी'),\n",
              " ('life', 'ज़िन्दगी'),\n",
              " ('life', 'ज़िंदगी'),\n",
              " ('life', 'जिन्दगी'),\n",
              " ('life', 'जीवन'),\n",
              " ('british', 'ब्रिटिश'),\n",
              " ('british', 'अंग्रेजों'),\n",
              " ('international', 'अंतर्राष्ट्रीय'),\n",
              " ('international', 'अन्तर्राष्ट्रीय'),\n",
              " ('international', 'अंतरराष्ट्रीय'),\n",
              " ('game', 'गेम'),\n",
              " ('game', 'खेल'),\n",
              " ('above', 'अरदास'),\n",
              " ('above', 'ऊपर'),\n",
              " ('club', 'क्लब'),\n",
              " ('club', 'क्लबों'),\n",
              " ('your', 'आपकी'),\n",
              " ('your', 'आपके'),\n",
              " ('your', 'आपका'),\n",
              " ('your', 'अपने'),\n",
              " ('your', 'अपना'),\n",
              " ('until', 'तक'),\n",
              " ('until', 'जबतक'),\n",
              " ('early', 'जल्दी'),\n",
              " ('best', 'बेस्ट'),\n",
              " ('best', 'सर्वश्रेष्ठ'),\n",
              " ('best', 'बेहतरीन'),\n",
              " ('west', 'पश्चिम'),\n",
              " ('west', 'वेस्ट'),\n",
              " ('house', 'घर'),\n",
              " ('house', 'मकान'),\n",
              " ('house', 'हाउस'),\n",
              " ('company', 'कंपनी'),\n",
              " ('company', 'कम्पनी'),\n",
              " ('general', 'जनरल'),\n",
              " ('general', 'साधारण'),\n",
              " ('general', 'सामान्य'),\n",
              " ('left', 'बायां'),\n",
              " ('left', 'शेष'),\n",
              " ('left', 'बाएं'),\n",
              " ('left', 'बाएँ'),\n",
              " ('very', 'बहुत'),\n",
              " ('here', 'यहां'),\n",
              " ('here', 'यहा'),\n",
              " ('here', 'यहाँ'),\n",
              " ('don', 'डॉन'),\n",
              " ('living', 'सजीव'),\n",
              " ('living', 'जीना'),\n",
              " ('day', 'दिवस'),\n",
              " ('day', 'दिन'),\n",
              " ('several', 'कई'),\n",
              " ('several', 'अनेक'),\n",
              " ('place', 'जगह'),\n",
              " ('place', 'स्थान'),\n",
              " ('place', 'प्लेस'),\n",
              " ('party', 'party'),\n",
              " ('party', 'दल'),\n",
              " ('party', 'पार्टी'),\n",
              " ('college', 'कॉलेज'),\n",
              " ('result', 'रिजल्ट'),\n",
              " ('result', 'परिणाम'),\n",
              " ('keep', 'रखें'),\n",
              " ('keep', 'रखना'),\n",
              " ('appropriate', 'उपयुक्त'),\n",
              " ('appropriate', 'उचित'),\n",
              " ('four', 'चार'),\n",
              " ('even', 'सम'),\n",
              " ('even', 'भी'),\n",
              " ('class', 'कक्षा'),\n",
              " ('class', 'वर्ग'),\n",
              " ('class', 'क्लास'),\n",
              " ('government', 'सरकार'),\n",
              " ('government', 'सरकारी'),\n",
              " ('how', 'केसे'),\n",
              " ('how', 'कैसे'),\n",
              " ('how', 'कैसा'),\n",
              " ('called', 'बुलाया'),\n",
              " ('called', 'पुकारा'),\n",
              " ('did', 'किया'),\n",
              " ('each', 'प्रत्येक'),\n",
              " ('found', 'मिली'),\n",
              " ('found', 'मिले'),\n",
              " ('found', 'पाये'),\n",
              " ('found', 'mile'),\n",
              " ('found', 'पाया'),\n",
              " ('found', 'मिला'),\n",
              " ('center', 'केन्द्र'),\n",
              " ('center', 'केंद्र'),\n",
              " ('center', 'मध्य'),\n",
              " ('center', 'बीच'),\n",
              " ('per', 'स्वादानुसार'),\n",
              " ('per', 'प्रति'),\n",
              " ('style', 'शैली'),\n",
              " ('style', 'style'),\n",
              " ('style', 'अंदाज़'),\n",
              " ('style', 'स्टाइल'),\n",
              " ('com', 'com'),\n",
              " ('long', 'लंबी'),\n",
              " ('long', 'लंबा'),\n",
              " ('long', 'लम्बी'),\n",
              " ('long', 'long'),\n",
              " ('country', 'देश'),\n",
              " ('back', 'पीछे'),\n",
              " ('back', 'बैक'),\n",
              " ('back', 'पिछे'),\n",
              " ('back', 'वापस'),\n",
              " ('way', 'तरीका'),\n",
              " ('way', 'राह'),\n",
              " ('way', 'मार्ग'),\n",
              " ('way', 'रास्ते'),\n",
              " ('way', 'रास्ता'),\n",
              " ('does', 'करता'),\n",
              " ('www', 'www'),\n",
              " ('modify', 'सुधारें'),\n",
              " ('end', 'अंत'),\n",
              " ('end', 'समाप्ति'),\n",
              " ('end', 'समाप्त'),\n",
              " ('make', 'बनाओ'),\n",
              " ('make', 'बना'),\n",
              " ('make', 'मेक'),\n",
              " ('public', 'सार्वजनिक'),\n",
              " ('public', 'जनता'),\n",
              " ('public', 'लोक'),\n",
              " ('public', 'पब्लिक'),\n",
              " ('played', 'निभाई'),\n",
              " ('won', 'जीता'),\n",
              " ('won', 'जीते'),\n",
              " ('won', 'जीती'),\n",
              " ('support', 'समर्थन'),\n",
              " ('support', 'सहायता'),\n",
              " ('support', 'support'),\n",
              " ('games', 'गेम्स'),\n",
              " ('games', 'गेम'),\n",
              " ('games', 'खेल'),\n",
              " ('former', 'पूर्व'),\n",
              " ('church', 'गिरजाघर'),\n",
              " ('church', 'चर्च'),\n",
              " ('church', 'गिरिजाघर'),\n",
              " ('east', 'पूर्वी'),\n",
              " ('east', 'पूर्व'),\n",
              " ('east', 'ईस्ट'),\n",
              " ('line', 'लाइन'),\n",
              " ('line', 'पंक्ति'),\n",
              " ('line', 'रेखा'),\n",
              " ('line', 'लकीर'),\n",
              " ('major', 'मेजर'),\n",
              " ('major', 'प्रमुख'),\n",
              " ('members', 'सदस्य'),\n",
              " ('members', 'सदस्यों'),\n",
              " ('good', 'good'),\n",
              " ('good', 'अच्छे'),\n",
              " ('good', 'गुड'),\n",
              " ('good', 'शुभ'),\n",
              " ('image', 'छवि'),\n",
              " ('image', 'प्रतिबिम्ब'),\n",
              " ('image', 'चित्र'),\n",
              " ('show', 'दिखाओ'),\n",
              " ('show', 'शो'),\n",
              " ('show', 'दिखाएं'),\n",
              " ('show', 'दिखाएँ'),\n",
              " ('still', 'स्टिल'),\n",
              " ('think', 'सोचो'),\n",
              " ('below', 'नीचे'),\n",
              " ('town', 'बस्ती'),\n",
              " ('town', 'नगर'),\n",
              " ('town', 'कस्बा'),\n",
              " ('last', 'अंतिम'),\n",
              " ('last', 'आखरी'),\n",
              " ('last', 'आखिरी'),\n",
              " ('last', 'पिछला'),\n",
              " ('system', 'तंत्र'),\n",
              " ('system', 'सिस्टम'),\n",
              " ('system', 'तन्त्र'),\n",
              " ('system', 'प्रणाली'),\n",
              " ('right', 'सही'),\n",
              " ('song', 'गीत'),\n",
              " ('song', 'गाना'),\n",
              " ('song', 'सोंग'),\n",
              " ('non', 'नॉन'),\n",
              " ('non', 'गैर'),\n",
              " ('section', 'धारा'),\n",
              " ('section', 'खंड'),\n",
              " ('section', 'अनुभाग'),\n",
              " ('section', 'खण्ड'),\n",
              " ('single', 'अकेला'),\n",
              " ('single', 'एकल'),\n",
              " ('single', 'single'),\n",
              " ('single', 'सिंगल'),\n",
              " ('included', 'शामिल'),\n",
              " ('align', 'पंक्तिबद्ध'),\n",
              " ('align', 'क्षारीय'),\n",
              " ('home', 'मुखपृष्ठ'),\n",
              " ('home', 'गृह'),\n",
              " ('home', 'होम'),\n",
              " ('home', 'घर'),\n",
              " ('women', 'औरतों'),\n",
              " ('women', 'स्त्रियाँ'),\n",
              " ('women', 'महिलाएँ'),\n",
              " ('women', 'महिलाएं'),\n",
              " ('women', 'महिलाओं'),\n",
              " ('television', 'टेलीविजन'),\n",
              " ('television', 'टीवी'),\n",
              " ('television', 'दूरदर्शन'),\n",
              " ('television', 'टेलीविज़न'),\n",
              " ('seed', 'बीज'),\n",
              " ('member', 'सदस्य'),\n",
              " ('goals', 'लक्ष्यों'),\n",
              " ('goals', 'लक्ष्य'),\n",
              " ('sources', 'स्रोत'),\n",
              " ('sources', 'स्त्रोत'),\n",
              " ('book', 'बुक'),\n",
              " ('book', 'किताबे'),\n",
              " ('book', 'पुस्तक'),\n",
              " ('book', 'किताब'),\n",
              " ('station', 'स्टेशन'),\n",
              " ('order', 'क्रम'),\n",
              " ('order', 'ऑर्डर'),\n",
              " ('order', 'आदेश'),\n",
              " ('order', 'अनुक्रम'),\n",
              " ('old', 'ओल्ड'),\n",
              " ('old', 'पुराने'),\n",
              " ('old', 'पुरानी'),\n",
              " ('old', 'पुराना'),\n",
              " ('old', 'वृद्ध'),\n",
              " ('old', 'बूढ़ा'),\n",
              " ('information', 'सुचना'),\n",
              " ('information', 'एनोटेशन'),\n",
              " ('information', 'जानकारी'),\n",
              " ('information', 'सूचना'),\n",
              " ('set', 'सेट'),\n",
              " ('set', 'समुच्चय'),\n",
              " ('set', 'नियत'),\n",
              " ('own', 'अपनी'),\n",
              " ('own', 'खुद'),\n",
              " ('own', 'स्वयं'),\n",
              " ('own', 'अपने'),\n",
              " ('own', 'अपना'),\n",
              " ('text', 'पाठ'),\n",
              " ('text', 'टेक्स्ट'),\n",
              " ('band', 'band'),\n",
              " ('band', 'बैंड'),\n",
              " ('point', 'बिंदु'),\n",
              " ('point', 'बिन्दु'),\n",
              " ('point', 'पॉइंट'),\n",
              " ('point', 'पाइंट'),\n",
              " ('local', 'स्थानीय'),\n",
              " ('local', 'लोकल'),\n",
              " ('river', 'नदी'),\n",
              " ('top', 'उपर'),\n",
              " ('top', 'ऊपर'),\n",
              " ('top', 'टॉप'),\n",
              " ('top', 'शीर्ष'),\n",
              " ('main', 'मुख्य'),\n",
              " ('main', 'मेन'),\n",
              " ('main', 'प्रमुख'),\n",
              " ('language', 'भाषा'),\n",
              " ('french', 'फ़्रेंच'),\n",
              " ('french', 'फ़्रांसीसी'),\n",
              " ('french', 'फ्रांसीसी'),\n",
              " ('french', 'फ्रेंच'),\n",
              " ('https', 'https'),\n",
              " ('named', 'नामित'),\n",
              " ('off', 'बन्द'),\n",
              " ('off', 'बंद'),\n",
              " ('note', 'व्याख्या'),\n",
              " ('note', 'नोट'),\n",
              " ('career', 'कैरियर'),\n",
              " ('career', 'आजीविका'),\n",
              " ('career', 'करियर'),\n",
              " ('original', 'मूल'),\n",
              " ('original', 'मौलिक'),\n",
              " ('original', 'असली'),\n",
              " ('age', 'उम्र'),\n",
              " ('age', 'आयु'),\n",
              " ('service', 'सेवा'),\n",
              " ('established', 'स्थापित'),\n",
              " ('located', 'स्थित'),\n",
              " ('located', 'स्तिथ'),\n",
              " ('said', 'कहा'),\n",
              " ('website', 'वेबसाइट'),\n",
              " ('website', 'जालस्थल'),\n",
              " ('population', 'जनसंख्या'),\n",
              " ('population', 'आबादी'),\n",
              " ('population', 'जनसांख्यिकी'),\n",
              " ('air', 'वायु'),\n",
              " ('air', 'एयर'),\n",
              " ('german', 'जर्मन'),\n",
              " ('german', 'जर्मनी'),\n",
              " ('law', 'कानून'),\n",
              " ('law', 'विधि'),\n",
              " ('military', 'मिलिट्री'),\n",
              " ('military', 'सैनिक'),\n",
              " ('military', 'फौजी'),\n",
              " ('military', 'सैन्य'),\n",
              " ('great', 'महान'),\n",
              " ('great', 'बड़ा'),\n",
              " ('great', 'ग्रेट'),\n",
              " ('within', 'अन्दर'),\n",
              " ('within', 'भीतर'),\n",
              " ('clubs', 'चिड़ी'),\n",
              " ('clubs', 'क्लब'),\n",
              " ('published', 'प्रकाशित'),\n",
              " ('president', 'प्रदेशाध्यक्ष'),\n",
              " ('president', 'राष्ट्रपति'),\n",
              " ('president', 'अध्यक्ष'),\n",
              " ('park', 'उद्यान'),\n",
              " ('park', 'पार्क'),\n",
              " ('official', 'अधिकारी'),\n",
              " ('official', 'सरकारी'),\n",
              " ('official', 'आधिकारिक'),\n",
              " ('case', 'केस'),\n",
              " ('case', 'मामला'),\n",
              " ('london', 'लंदन'),\n",
              " ('times', 'बार'),\n",
              " ('times', 'टाइम्स'),\n",
              " ('although', 'हालाँकि'),\n",
              " ('although', 'हालांकि'),\n",
              " ('small', 'छोटे'),\n",
              " ('small', 'छोटा'),\n",
              " ('small', 'छोटी'),\n",
              " ('third', 'तृतीय'),\n",
              " ('third', 'तीसरे'),\n",
              " ('third', 'तीसरा'),\n",
              " ('different', 'भिन्न'),\n",
              " ('different', 'अलग'),\n",
              " ('get', 'पाएँ'),\n",
              " ('get', 'मिलेंगी'),\n",
              " ('village', 'गांव'),\n",
              " ('village', 'गाँव'),\n",
              " ('closed', 'बन्द'),\n",
              " ('closed', 'बंद'),\n",
              " ('art', 'कला'),\n",
              " ('art', 'आर्ट'),\n",
              " ('player', 'खिलाड़ी'),\n",
              " ('final', 'फ़ाइनल'),\n",
              " ('final', 'अंतिम'),\n",
              " ('final', 'फाइनल'),\n",
              " ('community', 'समुदाय'),\n",
              " ('community', 'जातियाँ'),\n",
              " ('held', 'आयोजित'),\n",
              " ('army', 'आर्मी'),\n",
              " ('army', 'फ़ौज'),\n",
              " ('army', 'सेना'),\n",
              " ('army', 'फौज'),\n",
              " ('award', 'अवार्ड'),\n",
              " ('award', 'पुरस्कार'),\n",
              " ('without', 'बिना'),\n",
              " ('without', 'बगैर'),\n",
              " ('death', 'निधन'),\n",
              " ('death', 'मोत'),\n",
              " ('death', 'मृत्यु'),\n",
              " ('death', 'मौत'),\n",
              " ('built', 'बनवाया'),\n",
              " ('built', 'बनवाई'),\n",
              " ('built', 'निर्मित'),\n",
              " ('men', 'पुरुष'),\n",
              " ('men', 'पुरुषों'),\n",
              " ('men', 'मर्द'),\n",
              " ('men', 'मेन'),\n",
              " ('large', 'बड़े'),\n",
              " ('large', 'बड़ा'),\n",
              " ('large', 'बड़ी'),\n",
              " ('large', 'विशाल'),\n",
              " ('site', 'साइटें'),\n",
              " ('site', 'साइट'),\n",
              " ('site', 'साईट'),\n",
              " ('deletion', 'हटाना'),\n",
              " ('white', 'सफेद'),\n",
              " ('white', 'सफ़ेद'),\n",
              " ('white', 'श्वेत'),\n",
              " ('along', 'साथ'),\n",
              " ('five', 'पांच'),\n",
              " ('five', 'पाँच'),\n",
              " ('central', 'केंद्रीय'),\n",
              " ('central', 'मध्य'),\n",
              " ('central', 'सेंट्रल'),\n",
              " ('road', 'रोड'),\n",
              " ('road', 'सड़क'),\n",
              " ('free', 'निशुल्क'),\n",
              " ('free', 'मुफ्त'),\n",
              " ('free', 'मुक्त'),\n",
              " ('free', 'फ्री'),\n",
              " ('free', 'आजाद'),\n",
              " ('free', 'फ़्री'),\n",
              " ('england', 'इंग्लैंड'),\n",
              " ('england', 'इंग्लैण्ड'),\n",
              " ('include', 'शामिल'),\n",
              " ('association', 'सम्बद्धता'),\n",
              " ('association', 'एसोसिएशन'),\n",
              " ('association', 'संघ'),\n",
              " ('down', 'नीचे'),\n",
              " ('down', 'bas'),\n",
              " ('given', 'दी'),\n",
              " ('given', 'दिया'),\n",
              " ('given', 'दिए'),\n",
              " ('source', 'स्रोत'),\n",
              " ('source', 'स्त्रोत'),\n",
              " ('source', 'सोर्स'),\n",
              " ('california', 'कैलिफोर्निया'),\n",
              " ('california', 'कैलिफ़ोर्निया'),\n",
              " ('man', 'पुरुष'),\n",
              " ('man', 'मनुष्य'),\n",
              " ('man', 'मैन'),\n",
              " ('man', 'आदमी'),\n",
              " ('man', 'इन्सान'),\n",
              " ('man', 'man'),\n",
              " ('version', 'वर्जन'),\n",
              " ('version', 'संस्करण'),\n",
              " ('written', 'लिखी'),\n",
              " ('written', 'लिखा'),\n",
              " ('created', 'बनाया'),\n",
              " ('created', 'बनाए'),\n",
              " ('created', 'निर्मित'),\n",
              " ('media', 'मिडिया'),\n",
              " ('media', 'मीडिया'),\n",
              " ('black', 'काले'),\n",
              " ('black', 'काला'),\n",
              " ('black', 'ब्लैक'),\n",
              " ('though', 'यद्यपि'),\n",
              " ('though', 'हालांकि'),\n",
              " ('php', 'पीएचपी'),\n",
              " ('report', 'रिपोर्ट'),\n",
              " ('report', 'रपट'),\n",
              " ('building', 'बिल्डिंग'),\n",
              " ('building', 'भवन'),\n",
              " ('take', 'लें'),\n",
              " ('take', 'ले'),\n",
              " ('take', 'लो'),\n",
              " ('division', 'विभाग'),\n",
              " ('division', 'विभाजन'),\n",
              " ('division', 'प्रभाग'),\n",
              " ('comment', 'comment'),\n",
              " ('comment', 'टिप्पणी'),\n",
              " ('king', 'राजा'),\n",
              " ('king', 'बादशाह'),\n",
              " ('king', 'किंग'),\n",
              " ('edit', 'संपादन'),\n",
              " ('stadium', 'स्टेडियम'),\n",
              " ('ship', 'जहाज'),\n",
              " ('ship', 'शिप'),\n",
              " ('ship', 'जलयान'),\n",
              " ('research', 'अनुसंधान'),\n",
              " ('research', 'शोध'),\n",
              " ('record', 'रिकॉर्ड'),\n",
              " ('record', 'रेकार्ड'),\n",
              " ('record', 'रिकार्ड'),\n",
              " ('archive', 'संग्रह'),\n",
              " ('archive', 'अभिलेखागार'),\n",
              " ('archive', 'अभिलेख'),\n",
              " ('places', 'स्थान'),\n",
              " ('cup', 'कप'),\n",
              " ('records', 'रिकॉर्ड'),\n",
              " ('often', 'अकसर'),\n",
              " ('often', 'अक्सर'),\n",
              " ('often', 'प्रायः'),\n",
              " ('received', 'प्राप्त'),\n",
              " ('side', 'पार्श्व'),\n",
              " ('side', 'तरफ'),\n",
              " ('side', 'पक्ष'),\n",
              " ('power', 'पावर'),\n",
              " ('power', 'शक्ति'),\n",
              " ('education', 'शिक्षा'),\n",
              " ('know', 'जानिये'),\n",
              " ('know', 'जानिए'),\n",
              " ('know', 'जान'),\n",
              " ('category', 'वर्ग'),\n",
              " ('category', 'श्रेणी'),\n",
              " ('water', 'वाटर'),\n",
              " ('water', 'पानी'),\n",
              " ('water', 'जल'),\n",
              " ('political', 'राजनीतिक'),\n",
              " ('political', 'राजनैतिक'),\n",
              " ('species', 'प्रजातियां'),\n",
              " ('species', 'प्रजाति'),\n",
              " ('field', 'फील्ड'),\n",
              " ('field', 'फ़ील्ड'),\n",
              " ('near', 'निकट'),\n",
              " ('australia', 'आस्ट्रेलिया'),\n",
              " ('australia', 'ऑस्ट्रेलिया'),\n",
              " ('video', 'विडियो'),\n",
              " ('video', 'video'),\n",
              " ('video', 'वीडियो'),\n",
              " ('island', 'आइलैंड'),\n",
              " ('island', 'द्वीप'),\n",
              " ('form', 'फ़ॉर्म'),\n",
              " ('form', 'प्रपत्र'),\n",
              " ('form', 'फॉर्म'),\n",
              " ('form', 'फार्म'),\n",
              " ('form', 'रूप'),\n",
              " ('find', 'ढूंढें'),\n",
              " ('find', 'ढूँढें'),\n",
              " ('served', 'परोसा'),\n",
              " ('play', 'खेल'),\n",
              " ('project', 'परियोजना'),\n",
              " ('project', 'प्रोजेक्ट'),\n",
              " ('radio', 'रेडियो'),\n",
              " ('proposed', 'प्रस्तावित'),\n",
              " ('every', 'हर'),\n",
              " ('every', 'प्रत्येक'),\n",
              " ('development', 'विकास'),\n",
              " ('example', 'उदाहरण'),\n",
              " ('live', 'लाइव'),\n",
              " ('live', 'सजीव'),\n",
              " ('union', 'यूनियन'),\n",
              " ('union', 'संघ'),\n",
              " ('india', 'भारत'),\n",
              " ('india', 'इंडिया'),\n",
              " ('india', 'इण्डिया'),\n",
              " ('next', 'अगला'),\n",
              " ('special', 'ख़ास'),\n",
              " ('special', 'विशेष'),\n",
              " ('special', 'खास'),\n",
              " ('court', 'कोर्ट'),\n",
              " ('court', 'न्यायालय'),\n",
              " ('court', 'अदालत'),\n",
              " ('region', 'क्षेत्र'),\n",
              " ('region', 'प्रदेश'),\n",
              " ('little', 'छोटे'),\n",
              " ('little', 'छोटा'),\n",
              " ('little', 'लिटिल'),\n",
              " ('little', 'छोटी'),\n",
              " ('short', 'छोटा'),\n",
              " ('short', 'छोटी'),\n",
              " ('william', 'विलियम'),\n",
              " ('province', 'प्रान्त'),\n",
              " ('province', 'प्रांतों'),\n",
              " ('province', 'प्रांत'),\n",
              " ('western', 'पश्चिमी'),\n",
              " ('son', 'बेटा'),\n",
              " ('son', 'लाडला'),\n",
              " ('son', 'सोन'),\n",
              " ('son', 'पुत्र'),\n",
              " ('son', 'बेटे'),\n",
              " ('france', 'फ्रांस'),\n",
              " ('france', 'फ़्रांस'),\n",
              " ('council', 'मंत्रिपरिषद'),\n",
              " ('council', 'काउंसिल'),\n",
              " ('council', 'परिषद'),\n",
              " ('others', 'अन्य'),\n",
              " ('royal', 'नवाबी'),\n",
              " ('royal', 'रॉयल'),\n",
              " ('royal', 'शाही'),\n",
              " ('royal', 'royal'),\n",
              " ('current', 'धारा'),\n",
              " ('current', 'मौजूदा'),\n",
              " ('current', 'मौज़ूदा'),\n",
              " ('current', 'वर्तमान'),\n",
              " ('street', 'सड़क'),\n",
              " ('street', 'गली'),\n",
              " ('full', 'फुल'),\n",
              " ('full', 'पूर्ण'),\n",
              " ('full', 'पूरा'),\n",
              " ('red', 'लाल'),\n",
              " ('red', 'रेड'),\n",
              " ('too', 'भी'),\n",
              " ('too', 'too'),\n",
              " ('department', 'विभाग'),\n",
              " ('san', 'सान'),\n",
              " ('san', 'सैन'),\n",
              " ('help', 'मदद'),\n",
              " ('help', 'सहायता'),\n",
              " ('james', 'जेम्स'),\n",
              " ('open', 'खुला'),\n",
              " ('open', 'ओपन'),\n",
              " ('open', 'खोलें'),\n",
              " ('force', 'बल'),\n",
              " ('force', 'फ़ोर्स'),\n",
              " ('position', 'पद'),\n",
              " ('position', 'स्थिति'),\n",
              " ('head', 'दिमाग़'),\n",
              " ('head', 'सिर'),\n",
              " ('director', 'निर्देशक'),\n",
              " ('director', 'निदेशक'),\n",
              " ('father', 'चचा'),\n",
              " ('father', 'बाप'),\n",
              " ('father', 'father'),\n",
              " ('father', 'पिता'),\n",
              " ('track', 'ट्रैक'),\n",
              " ('track', 'रील'),\n",
              " ('track', 'track'),\n",
              " ('http', 'http'),\n",
              " ('http', 'एचटीटीपी'),\n",
              " ('canada', 'कनाडा'),\n",
              " ('australian', 'ऑस्ट्रेलियाई'),\n",
              " ('george', 'जॉर्ज'),\n",
              " ('george', 'जार्ज'),\n",
              " ('jpg', 'jpg'),\n",
              " ('level', 'स्तर'),\n",
              " ('late', 'देरी'),\n",
              " ('late', 'देर'),\n",
              " ('late', 'लेट'),\n",
              " ('summer', 'summer'),\n",
              " ('summer', 'ग्रीष्म'),\n",
              " ('summer', 'गर्मियों'),\n",
              " ('summer', 'गर्मी'),\n",
              " ('summer', 'गर्मियाँ'),\n",
              " ('society', 'सोसायटी'),\n",
              " ('society', 'समाज'),\n",
              " ('moved', 'स्थानांतरित'),\n",
              " ('office', 'आफिस'),\n",
              " ('office', 'ऑफिस'),\n",
              " ('office', 'कार्यालय'),\n",
              " ('office', 'ऑफ़िस'),\n",
              " ('office', 'दफ्तर'),\n",
              " ('period', 'अवधि'),\n",
              " ('period', 'काल'),\n",
              " ('championship', 'चैम्पियनशिप'),\n",
              " ('championship', 'चैंपियनशिप'),\n",
              " ('round', 'दौर'),\n",
              " ('round', 'गोल'),\n",
              " ('round', 'राउंड'),\n",
              " ('round', 'गोलाकार'),\n",
              " ('story', 'कहानी'),\n",
              " ('story', 'स्टोरी'),\n",
              " ('songs', 'गाने'),\n",
              " ('songs', 'गीत'),\n",
              " ('songs', 'गीतों'),\n",
              " ('various', 'विभिन्न'),\n",
              " ('file', 'फाइल'),\n",
              " ('file', 'फ़ाइल'),\n",
              " ('file', 'संचिका'),\n",
              " ('days', 'दिन'),\n",
              " ('days', 'दिनों'),\n",
              " ('land', 'भूमि'),\n",
              " ('land', 'land'),\n",
              " ('land', 'लैंड'),\n",
              " ('land', 'जमीन'),\n",
              " ('business', 'व्यवसाय'),\n",
              " ('business', 'बिज़नेस'),\n",
              " ('business', 'दुकानदारी'),\n",
              " ('business', 'व्यापार'),\n",
              " ('business', 'कारोबार'),\n",
              " ('reason', 'कारण'),\n",
              " ('america', 'अमरीका'),\n",
              " ('america', 'अमेरिका'),\n",
              " ('million', 'मिलियन'),\n",
              " ('million', 'लाख'),\n",
              " ('european', 'यूरोपियाई'),\n",
              " ('european', 'यूरोपीय'),\n",
              " ('term', 'पद'),\n",
              " ('six', 'छः'),\n",
              " ('six', 'छक्का'),\n",
              " ('six', 'छह'),\n",
              " ('post', 'पोस्ट'),\n",
              " ('why', 'क्यूँ'),\n",
              " ('why', 'क्यों'),\n",
              " ('produced', 'उत्पादित'),\n",
              " ('making', 'बनाने'),\n",
              " ('making', 'बनाना'),\n",
              " ('subject', 'विषय'),\n",
              " ('young', 'युवा'),\n",
              " ('young', 'जवान'),\n",
              " ('young', 'यंग'),\n",
              " ('total', 'कुल'),\n",
              " ('david', 'डेविड'),\n",
              " ('david', 'दाउद'),\n",
              " ('science', 'विज्ञान'),\n",
              " ('related', 'संबंधित'),\n",
              " ('related', 'सम्बन्धित'),\n",
              " ('rock', 'शैल'),\n",
              " ('rock', 'रॉक'),\n",
              " ('rock', 'चट्टान'),\n",
              " ('archived', 'संग्रहीत'),\n",
              " ('railway', 'रेलवे'),\n",
              " ('railway', 'रेल'),\n",
              " ('led', 'led'),\n",
              " ('students', 'छात्रों'),\n",
              " ('students', 'विद्यार्थी'),\n",
              " ('started', 'शुरू'),\n",
              " ('started', 'प्रारंभ'),\n",
              " ('news', 'खबर'),\n",
              " ('news', 'समाचार'),\n",
              " ('news', 'न्यूज़'),\n",
              " ('news', 'न्यूज'),\n",
              " ('described', 'वर्णित'),\n",
              " ('role', 'भूमिका'),\n",
              " ('election', 'चुनाव'),\n",
              " ('albums', 'एलबम'),\n",
              " ('albums', 'एल्बम'),\n",
              " ('present', 'मौजूद'),\n",
              " ('present', 'मौजूदा'),\n",
              " ('present', 'वर्तमान'),\n",
              " ...]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
        "\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "bilingual_lexicon = load_bilingual_lexicon(\"en-hi.txt\")\n",
        "bilingual_lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bfd09c-512b-484c-b1ba-45891b8582bf",
      "metadata": {
        "id": "c8bfd09c-512b-484c-b1ba-45891b8582bf"
      },
      "source": [
        "## Step 02: Embedding Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vaYz5tS33wHH",
      "metadata": {
        "id": "vaYz5tS33wHH"
      },
      "source": [
        "### 2-a: Extract Aligned Embedding Pairs from Bilingual Lexicon\n",
        "\n",
        "This function, `get_aligned_embeddings`, takes the bilingual lexicon along with source and target FastText models (English and Hindi in this case), and does following:\n",
        "\n",
        "- Retrieves the corresponding word vectors for each English-Hindi word pair.\n",
        "- Skips any word not found in the respective vocabulary.\n",
        "- Returns two NumPy arrays: one for source (English) embeddings and one for target (Hindi) embeddings.\n",
        "\n",
        "These aligned embeddings (`X_src` and `Y_tgt`) are typically used for supervised cross-lingual mapping or evaluation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab732ebd-729d-4696-894a-98fc4487de87",
      "metadata": {
        "id": "ab732ebd-729d-4696-894a-98fc4487de87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_aligned_embeddings(lexicon, model_src, model_tgt):\n",
        "    src_vecs = []\n",
        "    tgt_vecs = []\n",
        "    for src_word, tgt_word in lexicon:\n",
        "        try:\n",
        "            src_vec = model_src.get_word_vector(src_word)\n",
        "            tgt_vec = model_tgt.get_word_vector(tgt_word)\n",
        "            src_vecs.append(src_vec)\n",
        "            tgt_vecs.append(tgt_vec)\n",
        "        except KeyError:\n",
        "            # Ignore words not in vocabulary\n",
        "            continue\n",
        "    return np.array(src_vecs), np.array(tgt_vecs)\n",
        "\n",
        "X_src, Y_tgt = get_aligned_embeddings(bilingual_lexicon, model_en, model_hi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efec9ee-0d1d-40c3-8500-3e0a65ca49a0",
      "metadata": {
        "id": "7efec9ee-0d1d-40c3-8500-3e0a65ca49a0",
        "outputId": "03b0380c-db2d-4cf9-b966-2509f2d3cf5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 0.00823911, -0.08990277,  0.02652529, ..., -0.01159137,\n",
              "         -0.04112864,  0.03625222],\n",
              "        [-0.00102113,  0.04602651,  0.00604868, ..., -0.08220465,\n",
              "          0.01135671,  0.00796232],\n",
              "        [-0.00102113,  0.04602651,  0.00604868, ..., -0.08220465,\n",
              "          0.01135671,  0.00796232],\n",
              "        ...,\n",
              "        [-0.02027114, -0.02240974, -0.01055593, ...,  0.04405471,\n",
              "         -0.02558288,  0.00770057],\n",
              "        [-0.03047501, -0.06205894, -0.03088871, ...,  0.04897757,\n",
              "          0.01192696, -0.05418908],\n",
              "        [ 0.04086032, -0.03539041, -0.00380384, ...,  0.02875789,\n",
              "         -0.02330048, -0.04979837]], dtype=float32),\n",
              " array([[ 0.01132702, -0.07065436,  0.01902812, ...,  0.03552558,\n",
              "         -0.00812045, -0.02838861],\n",
              "        [-0.0096623 , -0.01804674,  0.06464785, ..., -0.11392715,\n",
              "         -0.09173848, -0.01580131],\n",
              "        [-0.04787989, -0.08300597, -0.04615875, ..., -0.00028435,\n",
              "          0.15839534,  0.0260338 ],\n",
              "        ...,\n",
              "        [-0.03423707,  0.03619131, -0.04007543, ..., -0.01021714,\n",
              "          0.0212449 , -0.01024995],\n",
              "        [ 0.0181387 ,  0.02884622, -0.0062998 , ...,  0.03205698,\n",
              "         -0.01406686,  0.06305543],\n",
              "        [ 0.00932745, -0.04363936, -0.00690154, ...,  0.00609323,\n",
              "         -0.02488353, -0.01306613]], dtype=float32))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_src, Y_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K-m4Gh4p3jiZ",
      "metadata": {
        "id": "K-m4Gh4p3jiZ"
      },
      "source": [
        "### 2-b: Procrustes Alignment (Orthogonal Mapping):\n",
        "\n",
        "Now, we implement the **Procrustes alignment** technique to learn a linear mapping from the source (English) embedding space to the target (Hindi) space using the aligned word pairs from the bilingual lexicon.\n",
        "\n",
        "Given two aligned embedding matrices:\n",
        "\n",
        "- $ X \\in \\mathbb{R}^{n \\times d} $: Source embeddings (English)  \n",
        "- $ Y \\in \\mathbb{R}^{n \\times d} $: Target embeddings (Hindi)\n",
        "\n",
        "We aim to find an **orthogonal matrix** $ W \\in \\mathbb{R}^{d \\times d} $ that minimizes the Frobenius norm:\n",
        "\n",
        "$ \\min_W \\| XW - Y \\|_F \\quad \\text{subject to} \\quad W^\\top W = I $\n",
        "\n",
        "This ensures that distances and angles between vectors are preserved during the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f67530-ff54-4603-ac2f-774b10e182ef",
      "metadata": {
        "id": "42f67530-ff54-4603-ac2f-774b10e182ef"
      },
      "outputs": [],
      "source": [
        "def procrustes_alignment(X, Y):\n",
        "    \"\"\"\n",
        "    Solves for the orthogonal matrix W that best maps X to Y (minimizing ||XW - Y||).\n",
        "    \"\"\"\n",
        "    # Centering is optional; FastText embeddings are usually zero-centered enough\n",
        "    # X -= X.mean(axis=0)\n",
        "    # Y -= Y.mean(axis=0)\n",
        "\n",
        "    # Compute matrix product\n",
        "    M = X.T @ Y\n",
        "\n",
        "    # SVD\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "\n",
        "    # Orthogonal mapping\n",
        "    W = U @ Vt\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407f25cc-ebde-4400-8fca-6cf3bfbfd82c",
      "metadata": {
        "id": "407f25cc-ebde-4400-8fca-6cf3bfbfd82c"
      },
      "source": [
        "Implementation Details\n",
        "\n",
        "- The matrix $M = X^\\top Y$ is computed.\n",
        "- We perform Singular Value Decomposition (SVD) on $M$:  \n",
        "  $M = U \\Sigma V^\\top$\n",
        "- The optimal orthogonal mapping is then:\n",
        "  $ W = UV^\\top $\n",
        "\n",
        "The learned matrix $ W $ is used to align all top-100k English word embeddings by:\n",
        "$ \\text{Aligned_English_Embeddings} = \\text{Original_English_Embeddings} \\cdot W $\n",
        "\n",
        "The resulting aligned embedding matrix is moved back to GPU as a PyTorch tensor for further similarity computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84279d95-c4e6-483b-93fe-a40db0118a70",
      "metadata": {
        "id": "84279d95-c4e6-483b-93fe-a40db0118a70"
      },
      "outputs": [],
      "source": [
        "W = procrustes_alignment(X_src, Y_tgt)\n",
        "\n",
        "# Apply to top-100k English embeddings\n",
        "aligned_embedding_matrix_en = embedding_matrix_en.cpu().numpy() @ W\n",
        "aligned_embedding_matrix_en = torch.tensor(aligned_embedding_matrix_en, dtype=torch.float32).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968b740a-209a-4192-a37c-f2767f7f9606",
      "metadata": {
        "id": "968b740a-209a-4192-a37c-f2767f7f9606",
        "outputId": "96a90bff-832a-4081-ecd5-71dd048309dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1328, -0.0356,  0.1568,  ...,  0.0843, -0.0254, -0.1301],\n",
              "        [ 0.0111, -0.0394,  0.0522,  ..., -0.0038,  0.0824, -0.2147],\n",
              "        [ 0.1528,  0.0331,  0.3174,  ...,  0.0601, -0.2201, -0.1145],\n",
              "        ...,\n",
              "        [-0.0345,  0.0214, -0.0257,  ..., -0.0096,  0.0347,  0.0053],\n",
              "        [-0.0402,  0.0102,  0.0621,  ...,  0.0196,  0.0892, -0.0470],\n",
              "        [ 0.0410, -0.0254, -0.0205,  ...,  0.0864,  0.0278, -0.0296]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aligned_embedding_matrix_en"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0895ad0c-ae7c-487f-bc0b-bb4d98b75548",
      "metadata": {
        "id": "0895ad0c-ae7c-487f-bc0b-bb4d98b75548"
      },
      "source": [
        "## Step 03: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb998b44-ade3-4f4b-a7ac-805f2cbb3667",
      "metadata": {
        "id": "fb998b44-ade3-4f4b-a7ac-805f2cbb3667"
      },
      "source": [
        "## 3-a,b: Perform word translation from English to Hindi using the aligned embeddings and Evaluate on MUSE Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d8bb7d-0625-49f8-b385-19d9ecab2e79",
      "metadata": {
        "id": "c2d8bb7d-0625-49f8-b385-19d9ecab2e79"
      },
      "source": [
        "### Load MUSE Test dictionary:\n",
        "Now we download the **English-Hindi test dictionary** (`en-hi.5000-6500.txt`) from the [MUSE](https://github.com/facebookresearch/MUSE) dataset, which contains 2032 bilingual word pairs reserved for evaluation.\n",
        "\n",
        "The `load_muse_dictionary` function reads each line and stores the aligned word pairs as a list of tuples. This dictionary will be commonly used to benchmark the quality of learned cross-lingual embedding mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8b175b-2b2e-4977-9b2b-89d184df0a58",
      "metadata": {
        "id": "ac8b175b-2b2e-4977-9b2b-89d184df0a58",
        "outputId": "39790182-95b7-4e95-c44a-53cfa98b64f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-04 12:55:49--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.158.20.43, 108.158.20.111, 108.158.20.21, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.158.20.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52464 (51K) [text/plain]\n",
            "Saving to: ‘en-hi.5000-6500.txt’\n",
            "\n",
            "en-hi.5000-6500.txt 100%[===================>]  51.23K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-04 12:55:50 (362 KB/s) - ‘en-hi.5000-6500.txt’ saved [52464/52464]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26ba4ca-6af5-492f-a989-4043dc5243e6",
      "metadata": {
        "id": "e26ba4ca-6af5-492f-a989-4043dc5243e6"
      },
      "outputs": [],
      "source": [
        "def load_muse_dictionary(file_path):\n",
        "    word_pairs = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            src, tgt = line.strip().split()\n",
        "            word_pairs.append((src, tgt))\n",
        "    return word_pairs\n",
        "\n",
        "muse_test_dict = load_muse_dictionary(\"en-hi.5000-6500.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94080cd-35e4-4222-baf2-ac00ac8b64d7",
      "metadata": {
        "id": "f94080cd-35e4-4222-baf2-ac00ac8b64d7"
      },
      "source": [
        "### Normalize Embeddings and Prepare Lookup Dictionaries\n",
        "\n",
        "To evaluate translation quality using cosine similarity, this step performs the following:\n",
        "\n",
        "- **Normalization**:  \n",
        "  Both the aligned English embeddings and original Hindi embeddings are L2-normalized to ensure cosine similarity is equivalent to dot product.\n",
        "\n",
        "- **Index Mappings**:  \n",
        "  - `word2idx_en`: Maps English words to their index in the top-100k vocabulary.  \n",
        "  - `word2idx_hi`: Maps Hindi words to their index.  \n",
        "  - `idx2word_hi`: Reverse lookup for Hindi word indices, useful for retrieving predicted translations.\n",
        "\n",
        "These pre-processed structures are essential for fast and accurate nearest neighbor search during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d92c26bc-8520-4e87-9f5a-44790b852c11",
      "metadata": {
        "id": "d92c26bc-8520-4e87-9f5a-44790b852c11"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Normalize both embeddings for cosine similarity\n",
        "def normalize_embeddings(embeddings):\n",
        "    norms = embeddings.norm(dim=1, keepdim=True)\n",
        "    return embeddings / norms\n",
        "\n",
        "normalized_en = normalize_embeddings(aligned_embedding_matrix_en)\n",
        "normalized_hi = normalize_embeddings(embedding_matrix_hi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f69ef20-dfe5-48a4-9b80-315bd5e33d09",
      "metadata": {
        "id": "7f69ef20-dfe5-48a4-9b80-315bd5e33d09"
      },
      "outputs": [],
      "source": [
        "word2idx_en = {word: idx for idx, word in enumerate(top_100k_words_en)}\n",
        "word2idx_hi = {word: idx for idx, word in enumerate(top_100k_words_hi)}\n",
        "idx2word_hi = {idx: word for word, idx in word2idx_hi.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f36a67d-2bfb-4f13-a362-f56c93f303d3",
      "metadata": {
        "id": "5f36a67d-2bfb-4f13-a362-f56c93f303d3"
      },
      "source": [
        "## 3-c: Evaluate Cross-Lingual Translation Quality\n",
        "\n",
        "This function evaluates the **word translation accuracy** using the aligned English embeddings and the original Hindi embeddings on the MUSE test set.\n",
        "\n",
        "#### Method: `evaluate_translation(..)` function\n",
        "\n",
        "- For each word pair `(en_word, hi_word)` in the test dictionary:\n",
        "  - Retrieve the L2-normalized English vector and compute cosine similarity with all Hindi embeddings using a dot product.\n",
        "  - Retrieve the **Top-K most similar Hindi words**.\n",
        "  - Count how often the correct Hindi translation is among the top-K predictions.\n",
        "\n",
        "#### Metrics Computed:\n",
        "- **Precision@1**: Fraction of test pairs where the correct translation is ranked **first**.\n",
        "- **Precision@5**: Fraction where it appears in the **top 5** predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a011342b-a218-4f1b-a29e-04db9720fb44",
      "metadata": {
        "id": "a011342b-a218-4f1b-a29e-04db9720fb44"
      },
      "outputs": [],
      "source": [
        "def evaluate_translation(test_dict, top_k=[1, 5]):\n",
        "    total = 0\n",
        "    correct_at_k = {k: 0 for k in top_k}\n",
        "    sim_scores = []\n",
        "\n",
        "    for en_word, hi_word in test_dict:\n",
        "        if en_word not in word2idx_en or hi_word not in word2idx_hi:\n",
        "            continue\n",
        "\n",
        "        total += 1\n",
        "        en_idx = word2idx_en[en_word]\n",
        "        en_vec = normalized_en[en_idx].unsqueeze(0)  # [1, 300]\n",
        "        sims = torch.matmul(en_vec, normalized_hi.T).squeeze(0)  # [100k]\n",
        "        topk_indices = torch.topk(sims, max(top_k)).indices.cpu().tolist()\n",
        "        predictions = [idx2word_hi[i] for i in topk_indices]\n",
        "\n",
        "        for k in top_k:\n",
        "            if hi_word in predictions[:k]:\n",
        "                correct_at_k[k] += 1\n",
        "\n",
        "        sim_scores.append((en_word, hi_word, sims[word2idx_hi[hi_word]].item()))\n",
        "\n",
        "    precision_at_k = {f'Precision@{k}': correct_at_k[k] / total for k in top_k}\n",
        "    return precision_at_k, sim_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6b3fed-dd93-4baa-8f5e-cad5b2f6bb5d",
      "metadata": {
        "id": "7b6b3fed-dd93-4baa-8f5e-cad5b2f6bb5d",
        "outputId": "6e5c50dd-6827-41bd-ad83-70d69759086c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation Precision Metrics:\n",
            "Precision@1: 0.3337\n",
            "Precision@5: 0.5938\n"
          ]
        }
      ],
      "source": [
        "precision_metrics, similarity_scores = evaluate_translation(muse_test_dict)\n",
        "\n",
        "print(\"Translation Precision Metrics:\")\n",
        "for k, v in precision_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b4928a-2b3c-4131-84b3-83cc5336ac61",
      "metadata": {
        "id": "17b4928a-2b3c-4131-84b3-83cc5336ac61"
      },
      "source": [
        "## 3-d: Inspect the Translation Pairs and calculate cosine similarity\n",
        "\n",
        "Now, we use the English-Hindi word pairs from the MUSE test dictionary by their **cosine similarity score**, in descending order.\n",
        "\n",
        "#### Output:\n",
        "- Displays the **Top 10 most confidently aligned word pairs** according to the model.\n",
        "- Helps qualitatively assess the success of the Procrustes alignment by observing semantically strong matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21af7e3-0a66-42b1-9ac6-025b1a3e7df9",
      "metadata": {
        "id": "f21af7e3-0a66-42b1-9ac6-025b1a3e7df9",
        "outputId": "446418e6-0f07-430c-b39b-ceb8f2efd46b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Most Similar Pairs (Cosine Score):\n",
            "pollution → प्रदूषण | Cosine Similarity: 0.7005\n",
            "clothes → कपड़े | Cosine Similarity: 0.6994\n",
            "healthy → स्वस्थ | Cosine Similarity: 0.6879\n",
            "kilometers → किलोमीटर | Cosine Similarity: 0.6792\n",
            "visa → वीजा | Cosine Similarity: 0.6735\n",
            "transparent → पारदर्शी | Cosine Similarity: 0.6733\n",
            "ideology → विचारधारा | Cosine Similarity: 0.6715\n",
            "mature → परिपक्व | Cosine Similarity: 0.6682\n",
            "investments → निवेश | Cosine Similarity: 0.6666\n",
            "bag → बैग | Cosine Similarity: 0.6655\n"
          ]
        }
      ],
      "source": [
        "similarity_scores.sort(key=lambda x: -x[2])  # sort by descending similarity\n",
        "\n",
        "# Top 10 most similar pairs\n",
        "print(\"\\nTop 10 Most Similar Pairs (Cosine Score):\")\n",
        "for en, hi, sim in similarity_scores[:10]:\n",
        "    print(f\"{en} → {hi} | Cosine Similarity: {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee060c7e-0c1d-44be-9b2c-754586d879fc",
      "metadata": {
        "id": "ee060c7e-0c1d-44be-9b2c-754586d879fc"
      },
      "source": [
        "## 3-e: Ablation Study: Effect of Lexicon Size\n",
        "\n",
        "We evaluate how the size of the bilingual lexicon impacts alignment quality. Procrustes alignment is trained using subsets of size **5,000**, **10,000**, **15,000**, **20,000**, **25,000**, **30,000**, and **35,000** word pairs from the English-Hindi dictionary.\n",
        "\n",
        "For each size, we compute **Precision@1** and **Precision@5** on the fixed MUSE test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e34e368-ddce-40d8-8c94-21509bb411e1",
      "metadata": {
        "id": "1e34e368-ddce-40d8-8c94-21509bb411e1",
        "outputId": "89f37f0a-7dea-452a-981e-4a2212cff17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running experiment with 5000 bilingual pairs ---\n",
            "Precision@1: 0.2256\n",
            "Precision@5: 0.4550\n",
            "\n",
            "--- Running experiment with 10000 bilingual pairs ---\n",
            "Precision@1: 0.3456\n",
            "Precision@5: 0.6019\n",
            "\n",
            "--- Running experiment with 20000 bilingual pairs ---\n",
            "Precision@1: 0.3538\n",
            "Precision@5: 0.6162\n",
            "\n",
            "--- Running experiment with 25000 bilingual pairs ---\n",
            "Precision@1: 0.3394\n",
            "Precision@5: 0.6038\n",
            "\n",
            "--- Running experiment with 30000 bilingual pairs ---\n",
            "Precision@1: 0.3400\n",
            "Precision@5: 0.6012\n",
            "\n",
            "--- Running experiment with 35000 bilingual pairs ---\n",
            "Precision@1: 0.3319\n",
            "Precision@5: 0.5994\n"
          ]
        }
      ],
      "source": [
        "def evaluate_translation(\n",
        "    test_dict,\n",
        "    top_k,\n",
        "    normalized_en,\n",
        "    normalized_hi,\n",
        "    word2idx_en,\n",
        "    word2idx_hi,\n",
        "    idx2word_hi\n",
        "):\n",
        "    total = 0\n",
        "    correct_at_k = {k: 0 for k in top_k}\n",
        "    sim_scores = []\n",
        "\n",
        "    for en_word, hi_word in test_dict:\n",
        "        if en_word not in word2idx_en or hi_word not in word2idx_hi:\n",
        "            continue\n",
        "\n",
        "        total += 1\n",
        "        en_idx = word2idx_en[en_word]\n",
        "        en_vec = normalized_en[en_idx].unsqueeze(0)  # [1, 300]\n",
        "        sims = torch.matmul(en_vec, normalized_hi.T).squeeze(0)  # [Vocab_size]\n",
        "        topk_indices = torch.topk(sims, max(top_k)).indices.cpu().tolist()\n",
        "        predictions = [idx2word_hi[i] for i in topk_indices]\n",
        "\n",
        "        for k in top_k:\n",
        "            if hi_word in predictions[:k]:\n",
        "                correct_at_k[k] += 1\n",
        "\n",
        "        sim_scores.append((en_word, hi_word, sims[word2idx_hi[hi_word]].item()))\n",
        "\n",
        "    precision_at_k = {f'Precision@{k}': correct_at_k[k] / total for k in top_k}\n",
        "    return precision_at_k, sim_scores\n",
        "\n",
        "\n",
        "def run_ablation_experiment(lexicon_path, muse_test_path, sizes=[5000, 10000, 20000]):\n",
        "    results = []\n",
        "\n",
        "    # Precompute fixed values\n",
        "    muse_dict = load_muse_dictionary(muse_test_path)\n",
        "    word2idx_en = {word: idx for idx, word in enumerate(top_100k_words_en)}\n",
        "    word2idx_hi = {word: idx for idx, word in enumerate(top_100k_words_hi)}\n",
        "    idx2word_hi = {idx: word for word, idx in word2idx_hi.items()}\n",
        "    normalized_hi = normalize_embeddings(embedding_matrix_hi)\n",
        "\n",
        "    for size in sizes:\n",
        "        # Step 1: Load lexicon subset\n",
        "        bilingual_lexicon = load_bilingual_lexicon(lexicon_path, max_pairs=size)\n",
        "\n",
        "        # Step 2: Extract aligned embeddings\n",
        "        X_src, Y_tgt = get_aligned_embeddings(bilingual_lexicon, model_en, model_hi)\n",
        "\n",
        "        # Step 3: Learn Procrustes mapping\n",
        "        W = procrustes_alignment(X_src, Y_tgt)\n",
        "\n",
        "        # Step 4: Align and normalize English embeddings\n",
        "        aligned_en = embedding_matrix_en.cpu().numpy() @ W\n",
        "        normalized_en = normalize_embeddings(torch.tensor(aligned_en, dtype=torch.float32).cuda())\n",
        "\n",
        "        # Step 5: Evaluate\n",
        "        precision, _ = evaluate_translation(\n",
        "            muse_dict,\n",
        "            top_k=[1, 5],\n",
        "            normalized_en=normalized_en,\n",
        "            normalized_hi=normalized_hi,\n",
        "            word2idx_en=word2idx_en,\n",
        "            word2idx_hi=word2idx_hi,\n",
        "            idx2word_hi=idx2word_hi\n",
        "        )\n",
        "\n",
        "        results.append((size, precision))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "ablation_results = run_ablation_experiment(\n",
        "    lexicon_path=\"en-hi.txt\",\n",
        "    muse_test_path=\"en-hi.5000-6500.txt\",\n",
        "    sizes=[5000, 10000, 20000, 25000, 30000, 35000]\n",
        ")\n",
        "\n",
        "for size, metrics in ablation_results:\n",
        "    print(f\"\\n--- Running experiment with {size} bilingual pairs ---\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08adb142-9213-4e91-8939-e639eee26c05",
      "metadata": {
        "id": "08adb142-9213-4e91-8939-e639eee26c05"
      },
      "source": [
        "## Optional Extra Credit Task: unsupervised alignment method such as Cross-Domain Similarity Local Scaling (CSLS) combined with adversarial training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j92CqzGd8IlD",
      "metadata": {
        "id": "j92CqzGd8IlD"
      },
      "source": [
        "### Convert `.bin` to `.vec` Format for MUSE\n",
        "\n",
        "The FastText binary models (`.bin`) for English and Hindi are converted to the text-based `.vec` format using `fasttext.load_model()` and saved manually. This format is required for compatibility with the [MUSE](https://github.com/facebookresearch/MUSE) toolkit.\n",
        "\n",
        "The conversion enables efficient use of **unsupervised alignment methods** such as **adversarial training** combined with **Cross-domain Similarity Local Scaling (CSLS)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83569a47-952f-4d31-858e-d470184e7551",
      "metadata": {
        "id": "83569a47-952f-4d31-858e-d470184e7551"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "def convert_bin_to_vec(bin_path, vec_path):\n",
        "    model = fasttext.load_model(bin_path)\n",
        "    with open(vec_path, 'w', encoding='utf-8') as f:\n",
        "        words = model.get_words()\n",
        "        dim = len(model.get_word_vector(words[0]))\n",
        "        f.write(f\"{len(words)} {dim}\\n\")\n",
        "        for w in words:\n",
        "            vec = model.get_word_vector(w)\n",
        "            vec_str = \" \".join([str(x) for x in vec])\n",
        "            f.write(f\"{w} {vec_str}\\n\")\n",
        "\n",
        "convert_bin_to_vec(\"cc.en.300.bin\", \"cc.en.300.vec\")\n",
        "convert_bin_to_vec(\"cc.hi.300.bin\", \"cc.hi.300.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QNmzcOY-8rR1",
      "metadata": {
        "id": "QNmzcOY-8rR1"
      },
      "source": [
        "### Set Up Environment and Clone MUSE Repository\n",
        "\n",
        "1. **Create a Python 3.10 virtual environment** for isolating dependencies.\n",
        "2. **Activate the virtual environment** (note: `source` won't work directly in Colab cells - use further steps on local, for better practice.).\n",
        "3. **Clone the [MUSE](https://github.com/facebookresearch/MUSE) repository** from Facebook Research.\n",
        "4. **Install FAISS with GPU support** for efficient nearest neighbor search during alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b394f72-2c3d-4ba5-8065-a8d2c0f96ecc",
      "metadata": {
        "id": "1b394f72-2c3d-4ba5-8065-a8d2c0f96ecc"
      },
      "outputs": [],
      "source": [
        "!python3.10 -m venv muse-venv\n",
        "!source /workspace/lipsync/LatentSync/expssssss/muse-venv/bin/activate\n",
        "!git clone https://github.com/facebookresearch/MUSE.git\n",
        "%cd MUSE\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727790e4-3324-4b81-8f13-ed7cded090bb",
      "metadata": {
        "id": "727790e4-3324-4b81-8f13-ed7cded090bb"
      },
      "source": [
        "Fix for the getargspec issue in MUSE (File to edit): `MUSE/src/utils.py`\n",
        "\n",
        "Locate this line (around line 218 in `utils.py`):\n",
        "`expected_args = inspect.getargspec(optim_fn.__init__)[0]`\n",
        "\n",
        "Replace it with: `expected_args = list(inspect.signature(optim_fn.__init__).parameters)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4qkwNwII9Lf3",
      "metadata": {
        "id": "4qkwNwII9Lf3"
      },
      "source": [
        "### Run Unsupervised Embedding Alignment with MUSE\n",
        "\n",
        "Execute the MUSE `unsupervised.py` script to align English and Hindi word embeddings using adversarial training followed by ** refinement iterations** and **Cross-Domain Similarity Local Scaling (CSLS)**.\n",
        "\n",
        "**Breakdown:**\n",
        "- `--src_lang`: Source language = English\n",
        "- `--tgt_lang`: Target language = Hindi\n",
        "- `--src_emb`: Path to the FastText `.vec` file for English\n",
        "- `--tgt_emb`: Path to the FastText `.vec` file for Hindi\n",
        "- `--n_refinement`: Perform 5 rounds of Procrustes refinement after initial adversarial mapping\n",
        "- `--export \"pth\"`: Export the learned mapping as PyTorch model weights\n",
        "- `--dico_eval`: Evaluate the alignment using a gold standard bilingual dictionary\n",
        "\n",
        "Now we leverages **unsupervised adversarial training combined with CSLS** to align two embedding spaces without requiring parallel data. While the training process appeared successful and reached an optimal stage, the evaluation results were unsatisfactory. Specifically, both **Precision@1** and **Precision@5** were **0.0**.\n",
        "\n",
        "To diagnose the issue, I experimented with different test dictionaries and also switched to **FastText embeddings trained on Wikipedia**. However, the results remained poor, indicating a deeper issue with alignment quality, compatibility between the embeddings and evaluation data or environmental setup. I believe with more debug and exploration, i'll be able to solve this issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "463f222e-6574-4836-a691-f93d845cf855",
      "metadata": {
        "id": "463f222e-6574-4836-a691-f93d845cf855",
        "outputId": "b9c1bed8-5199-4ce2-8001-462b3f7f30ca",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO - 04/04/25 14:52:20 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 04/04/25 14:52:20 - 0:00:00 - adversarial: True\n",
            "                                     batch_size: 32\n",
            "                                     cuda: True\n",
            "                                     dico_build: S2T\n",
            "                                     dico_eval: /workspace/lipsync/LatentSync/expssssss/en-hi.5000-6500.txt\n",
            "                                     dico_max_rank: 15000\n",
            "                                     dico_max_size: 0\n",
            "                                     dico_method: csls_knn_10\n",
            "                                     dico_min_size: 0\n",
            "                                     dico_threshold: 0\n",
            "                                     dis_clip_weights: 0\n",
            "                                     dis_dropout: 0.0\n",
            "                                     dis_hid_dim: 2048\n",
            "                                     dis_input_dropout: 0.1\n",
            "                                     dis_lambda: 1\n",
            "                                     dis_layers: 2\n",
            "                                     dis_most_frequent: 75000\n",
            "                                     dis_optimizer: sgd,lr=0.1\n",
            "                                     dis_smooth: 0.1\n",
            "                                     dis_steps: 5\n",
            "                                     emb_dim: 300\n",
            "                                     epoch_size: 1000000\n",
            "                                     exp_id: \n",
            "                                     exp_name: debug\n",
            "                                     exp_path: /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/cirhfcz5bk\n",
            "                                     export: pth\n",
            "                                     lr_decay: 0.98\n",
            "                                     lr_shrink: 0.5\n",
            "                                     map_beta: 0.001\n",
            "                                     map_id_init: True\n",
            "                                     map_optimizer: sgd,lr=0.1\n",
            "                                     max_vocab: 200000\n",
            "                                     min_lr: 1e-06\n",
            "                                     n_epochs: 5\n",
            "                                     n_refinement: 5\n",
            "                                     normalize_embeddings: \n",
            "                                     seed: -1\n",
            "                                     src_emb: data/wiki.en.vec\n",
            "                                     src_lang: en\n",
            "                                     tgt_emb: data/wiki.hi.vec\n",
            "                                     tgt_lang: hi\n",
            "                                     verbose: 2\n",
            "INFO - 04/04/25 14:52:20 - 0:00:00 - The experiment will be stored in /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/cirhfcz5bk\n",
            "INFO - 04/04/25 14:52:28 - 0:00:09 - Loaded 200000 pre-trained word embeddings.\n",
            "INFO - 04/04/25 14:52:37 - 0:00:18 - Loaded 158016 pre-trained word embeddings.\n",
            "INFO - 04/04/25 14:52:38 - 0:00:19 - ----> ADVERSARIAL TRAINING <----\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 14:52:38 - 0:00:19 - Starting adversarial training epoch 0...\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py:69: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  src_emb = self.src_emb(Variable(src_ids, volatile=True))\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  tgt_emb = self.tgt_emb(Variable(tgt_ids, volatile=True))\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  src_emb = self.mapping(Variable(src_emb.data, volatile=volatile))\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  tgt_emb = Variable(tgt_emb.data, volatile=volatile)\n",
            "INFO - 04/04/25 14:52:38 - 0:00:19 - 000000 - Discriminator loss: 0.6753 - 638 samples/s\n",
            "INFO - 04/04/25 14:52:40 - 0:00:20 - 004000 - Discriminator loss: 0.5604 - 5150 samples/s\n",
            "INFO - 04/04/25 14:52:41 - 0:00:22 - 008000 - Discriminator loss: 0.4986 - 5145 samples/s\n",
            "INFO - 04/04/25 14:52:43 - 0:00:23 - 012000 - Discriminator loss: 0.5323 - 5160 samples/s\n",
            "INFO - 04/04/25 14:52:45 - 0:00:25 - 016000 - Discriminator loss: 0.5478 - 5160 samples/s\n",
            "INFO - 04/04/25 14:52:46 - 0:00:26 - 020000 - Discriminator loss: 0.5533 - 5157 samples/s\n",
            "INFO - 04/04/25 14:52:48 - 0:00:28 - 024000 - Discriminator loss: 0.5509 - 5175 samples/s\n",
            "INFO - 04/04/25 14:52:49 - 0:00:30 - 028000 - Discriminator loss: 0.5407 - 5162 samples/s\n",
            "INFO - 04/04/25 14:52:51 - 0:00:31 - 032000 - Discriminator loss: 0.5297 - 5153 samples/s\n",
            "INFO - 04/04/25 14:52:52 - 0:00:33 - 036000 - Discriminator loss: 0.5275 - 5162 samples/s\n",
            "INFO - 04/04/25 14:52:54 - 0:00:34 - 040000 - Discriminator loss: 0.5284 - 5182 samples/s\n",
            "INFO - 04/04/25 14:52:55 - 0:00:36 - 044000 - Discriminator loss: 0.5275 - 5169 samples/s\n",
            "INFO - 04/04/25 14:52:57 - 0:00:37 - 048000 - Discriminator loss: 0.5248 - 5168 samples/s\n",
            "INFO - 04/04/25 14:52:58 - 0:00:39 - 052000 - Discriminator loss: 0.5259 - 5174 samples/s\n",
            "INFO - 04/04/25 14:53:00 - 0:00:40 - 056000 - Discriminator loss: 0.5225 - 5180 samples/s\n",
            "INFO - 04/04/25 14:53:02 - 0:00:42 - 060000 - Discriminator loss: 0.5150 - 5166 samples/s\n",
            "INFO - 04/04/25 14:53:03 - 0:00:43 - 064000 - Discriminator loss: 0.5161 - 5171 samples/s\n",
            "INFO - 04/04/25 14:53:05 - 0:00:45 - 068000 - Discriminator loss: 0.5138 - 5168 samples/s\n",
            "INFO - 04/04/25 14:53:06 - 0:00:47 - 072000 - Discriminator loss: 0.5118 - 5168 samples/s\n",
            "INFO - 04/04/25 14:53:08 - 0:00:48 - 076000 - Discriminator loss: 0.5073 - 5170 samples/s\n",
            "INFO - 04/04/25 14:53:09 - 0:00:50 - 080000 - Discriminator loss: 0.5078 - 5178 samples/s\n",
            "INFO - 04/04/25 14:53:11 - 0:00:51 - 084000 - Discriminator loss: 0.5002 - 5172 samples/s\n",
            "INFO - 04/04/25 14:53:12 - 0:00:53 - 088000 - Discriminator loss: 0.4989 - 5171 samples/s\n",
            "INFO - 04/04/25 14:53:14 - 0:00:54 - 092000 - Discriminator loss: 0.4990 - 5169 samples/s\n",
            "INFO - 04/04/25 14:53:15 - 0:00:56 - 096000 - Discriminator loss: 0.4948 - 5169 samples/s\n",
            "INFO - 04/04/25 14:53:17 - 0:00:57 - 100000 - Discriminator loss: 0.4934 - 5173 samples/s\n",
            "INFO - 04/04/25 14:53:19 - 0:00:59 - 104000 - Discriminator loss: 0.4942 - 5174 samples/s\n",
            "INFO - 04/04/25 14:53:20 - 0:01:00 - 108000 - Discriminator loss: 0.4925 - 5177 samples/s\n",
            "INFO - 04/04/25 14:53:22 - 0:01:02 - 112000 - Discriminator loss: 0.4849 - 5177 samples/s\n",
            "INFO - 04/04/25 14:53:23 - 0:01:04 - 116000 - Discriminator loss: 0.4904 - 5159 samples/s\n",
            "INFO - 04/04/25 14:53:25 - 0:01:05 - 120000 - Discriminator loss: 0.4842 - 5182 samples/s\n",
            "INFO - 04/04/25 14:53:26 - 0:01:07 - 124000 - Discriminator loss: 0.4851 - 5175 samples/s\n",
            "INFO - 04/04/25 14:53:28 - 0:01:08 - 128000 - Discriminator loss: 0.4853 - 5176 samples/s\n",
            "INFO - 04/04/25 14:53:29 - 0:01:10 - 132000 - Discriminator loss: 0.4811 - 5176 samples/s\n",
            "INFO - 04/04/25 14:53:31 - 0:01:11 - 136000 - Discriminator loss: 0.4802 - 5172 samples/s\n",
            "INFO - 04/04/25 14:53:32 - 0:01:13 - 140000 - Discriminator loss: 0.4765 - 5164 samples/s\n",
            "INFO - 04/04/25 14:53:34 - 0:01:14 - 144000 - Discriminator loss: 0.4806 - 5175 samples/s\n",
            "INFO - 04/04/25 14:53:36 - 0:01:16 - 148000 - Discriminator loss: 0.4774 - 5175 samples/s\n",
            "INFO - 04/04/25 14:53:37 - 0:01:17 - 152000 - Discriminator loss: 0.4765 - 5175 samples/s\n",
            "INFO - 04/04/25 14:53:39 - 0:01:19 - 156000 - Discriminator loss: 0.4751 - 5169 samples/s\n",
            "INFO - 04/04/25 14:53:40 - 0:01:21 - 160000 - Discriminator loss: 0.4722 - 5193 samples/s\n",
            "INFO - 04/04/25 14:53:42 - 0:01:22 - 164000 - Discriminator loss: 0.4734 - 5180 samples/s\n",
            "INFO - 04/04/25 14:53:43 - 0:01:24 - 168000 - Discriminator loss: 0.4731 - 5178 samples/s\n",
            "INFO - 04/04/25 14:53:45 - 0:01:25 - 172000 - Discriminator loss: 0.4683 - 5183 samples/s\n",
            "INFO - 04/04/25 14:53:46 - 0:01:27 - 176000 - Discriminator loss: 0.4723 - 5124 samples/s\n",
            "INFO - 04/04/25 14:53:48 - 0:01:28 - 180000 - Discriminator loss: 0.4716 - 5139 samples/s\n",
            "INFO - 04/04/25 14:53:49 - 0:01:30 - 184000 - Discriminator loss: 0.4737 - 5183 samples/s\n",
            "INFO - 04/04/25 14:53:51 - 0:01:31 - 188000 - Discriminator loss: 0.4684 - 5218 samples/s\n",
            "INFO - 04/04/25 14:53:53 - 0:01:33 - 192000 - Discriminator loss: 0.4692 - 5221 samples/s\n",
            "INFO - 04/04/25 14:53:54 - 0:01:34 - 196000 - Discriminator loss: 0.4657 - 5209 samples/s\n",
            "INFO - 04/04/25 14:53:56 - 0:01:36 - 200000 - Discriminator loss: 0.4638 - 5209 samples/s\n",
            "INFO - 04/04/25 14:53:57 - 0:01:38 - 204000 - Discriminator loss: 0.4662 - 5221 samples/s\n",
            "INFO - 04/04/25 14:53:59 - 0:01:39 - 208000 - Discriminator loss: 0.4594 - 5209 samples/s\n",
            "INFO - 04/04/25 14:54:00 - 0:01:41 - 212000 - Discriminator loss: 0.4627 - 5211 samples/s\n",
            "INFO - 04/04/25 14:54:02 - 0:01:42 - 216000 - Discriminator loss: 0.4621 - 5214 samples/s\n",
            "INFO - 04/04/25 14:54:03 - 0:01:44 - 220000 - Discriminator loss: 0.4605 - 5224 samples/s\n",
            "INFO - 04/04/25 14:54:05 - 0:01:45 - 224000 - Discriminator loss: 0.4604 - 5210 samples/s\n",
            "INFO - 04/04/25 14:54:06 - 0:01:47 - 228000 - Discriminator loss: 0.4568 - 5214 samples/s\n",
            "INFO - 04/04/25 14:54:08 - 0:01:48 - 232000 - Discriminator loss: 0.4590 - 5220 samples/s\n",
            "INFO - 04/04/25 14:54:09 - 0:01:50 - 236000 - Discriminator loss: 0.4589 - 5225 samples/s\n",
            "INFO - 04/04/25 14:54:11 - 0:01:51 - 240000 - Discriminator loss: 0.4569 - 5203 samples/s\n",
            "INFO - 04/04/25 14:54:12 - 0:01:53 - 244000 - Discriminator loss: 0.4537 - 5218 samples/s\n",
            "INFO - 04/04/25 14:54:14 - 0:01:54 - 248000 - Discriminator loss: 0.4544 - 5220 samples/s\n",
            "INFO - 04/04/25 14:54:16 - 0:01:56 - 252000 - Discriminator loss: 0.4551 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:17 - 0:01:57 - 256000 - Discriminator loss: 0.4537 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:19 - 0:01:59 - 260000 - Discriminator loss: 0.4546 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:20 - 0:02:01 - 264000 - Discriminator loss: 0.4536 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:22 - 0:02:02 - 268000 - Discriminator loss: 0.4527 - 5217 samples/s\n",
            "INFO - 04/04/25 14:54:23 - 0:02:04 - 272000 - Discriminator loss: 0.4533 - 5197 samples/s\n",
            "INFO - 04/04/25 14:54:25 - 0:02:05 - 276000 - Discriminator loss: 0.4528 - 5211 samples/s\n",
            "INFO - 04/04/25 14:54:26 - 0:02:07 - 280000 - Discriminator loss: 0.4520 - 5211 samples/s\n",
            "INFO - 04/04/25 14:54:28 - 0:02:08 - 284000 - Discriminator loss: 0.4504 - 5213 samples/s\n",
            "INFO - 04/04/25 14:54:29 - 0:02:10 - 288000 - Discriminator loss: 0.4513 - 5201 samples/s\n",
            "INFO - 04/04/25 14:54:31 - 0:02:11 - 292000 - Discriminator loss: 0.4524 - 5208 samples/s\n",
            "INFO - 04/04/25 14:54:32 - 0:02:13 - 296000 - Discriminator loss: 0.4494 - 5175 samples/s\n",
            "INFO - 04/04/25 14:54:34 - 0:02:14 - 300000 - Discriminator loss: 0.4511 - 5208 samples/s\n",
            "INFO - 04/04/25 14:54:36 - 0:02:16 - 304000 - Discriminator loss: 0.4476 - 5185 samples/s\n",
            "INFO - 04/04/25 14:54:37 - 0:02:17 - 308000 - Discriminator loss: 0.4471 - 5215 samples/s\n",
            "INFO - 04/04/25 14:54:39 - 0:02:19 - 312000 - Discriminator loss: 0.4478 - 5207 samples/s\n",
            "INFO - 04/04/25 14:54:40 - 0:02:21 - 316000 - Discriminator loss: 0.4443 - 5197 samples/s\n",
            "INFO - 04/04/25 14:54:42 - 0:02:22 - 320000 - Discriminator loss: 0.4476 - 5200 samples/s\n",
            "INFO - 04/04/25 14:54:43 - 0:02:24 - 324000 - Discriminator loss: 0.4490 - 5211 samples/s\n",
            "INFO - 04/04/25 14:54:45 - 0:02:25 - 328000 - Discriminator loss: 0.4432 - 5211 samples/s\n",
            "INFO - 04/04/25 14:54:46 - 0:02:27 - 332000 - Discriminator loss: 0.4439 - 5187 samples/s\n",
            "INFO - 04/04/25 14:54:48 - 0:02:28 - 336000 - Discriminator loss: 0.4432 - 5213 samples/s\n",
            "INFO - 04/04/25 14:54:49 - 0:02:30 - 340000 - Discriminator loss: 0.4436 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:51 - 0:02:31 - 344000 - Discriminator loss: 0.4401 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:52 - 0:02:33 - 348000 - Discriminator loss: 0.4416 - 5207 samples/s\n",
            "INFO - 04/04/25 14:54:54 - 0:02:34 - 352000 - Discriminator loss: 0.4422 - 5215 samples/s\n",
            "INFO - 04/04/25 14:54:56 - 0:02:36 - 356000 - Discriminator loss: 0.4419 - 5219 samples/s\n",
            "INFO - 04/04/25 14:54:57 - 0:02:37 - 360000 - Discriminator loss: 0.4415 - 5203 samples/s\n",
            "INFO - 04/04/25 14:54:59 - 0:02:39 - 364000 - Discriminator loss: 0.4388 - 5210 samples/s\n",
            "INFO - 04/04/25 14:55:00 - 0:02:40 - 368000 - Discriminator loss: 0.4384 - 5204 samples/s\n",
            "INFO - 04/04/25 14:55:02 - 0:02:42 - 372000 - Discriminator loss: 0.4382 - 5203 samples/s\n",
            "INFO - 04/04/25 14:55:03 - 0:02:44 - 376000 - Discriminator loss: 0.4383 - 5225 samples/s\n",
            "INFO - 04/04/25 14:55:05 - 0:02:45 - 380000 - Discriminator loss: 0.4392 - 5202 samples/s\n",
            "INFO - 04/04/25 14:55:06 - 0:02:47 - 384000 - Discriminator loss: 0.4384 - 5201 samples/s\n",
            "INFO - 04/04/25 14:55:08 - 0:02:48 - 388000 - Discriminator loss: 0.4386 - 5198 samples/s\n",
            "INFO - 04/04/25 14:55:09 - 0:02:50 - 392000 - Discriminator loss: 0.4386 - 5206 samples/s\n",
            "INFO - 04/04/25 14:55:11 - 0:02:51 - 396000 - Discriminator loss: 0.4390 - 5193 samples/s\n",
            "INFO - 04/04/25 14:55:12 - 0:02:53 - 400000 - Discriminator loss: 0.4353 - 5202 samples/s\n",
            "INFO - 04/04/25 14:55:14 - 0:02:54 - 404000 - Discriminator loss: 0.4340 - 5205 samples/s\n",
            "INFO - 04/04/25 14:55:15 - 0:02:56 - 408000 - Discriminator loss: 0.4360 - 5209 samples/s\n",
            "INFO - 04/04/25 14:55:17 - 0:02:57 - 412000 - Discriminator loss: 0.4349 - 5217 samples/s\n",
            "INFO - 04/04/25 14:55:19 - 0:02:59 - 416000 - Discriminator loss: 0.4355 - 5209 samples/s\n",
            "INFO - 04/04/25 14:55:20 - 0:03:00 - 420000 - Discriminator loss: 0.4344 - 5213 samples/s\n",
            "INFO - 04/04/25 14:55:22 - 0:03:02 - 424000 - Discriminator loss: 0.4334 - 5201 samples/s\n",
            "INFO - 04/04/25 14:55:23 - 0:03:04 - 428000 - Discriminator loss: 0.4326 - 5204 samples/s\n",
            "INFO - 04/04/25 14:55:25 - 0:03:05 - 432000 - Discriminator loss: 0.4313 - 5208 samples/s\n",
            "INFO - 04/04/25 14:55:26 - 0:03:07 - 436000 - Discriminator loss: 0.4329 - 5199 samples/s\n",
            "INFO - 04/04/25 14:55:28 - 0:03:08 - 440000 - Discriminator loss: 0.4314 - 5215 samples/s\n",
            "INFO - 04/04/25 14:55:29 - 0:03:10 - 444000 - Discriminator loss: 0.4323 - 5198 samples/s\n",
            "INFO - 04/04/25 14:55:31 - 0:03:11 - 448000 - Discriminator loss: 0.4312 - 5209 samples/s\n",
            "INFO - 04/04/25 14:55:32 - 0:03:13 - 452000 - Discriminator loss: 0.4306 - 5208 samples/s\n",
            "INFO - 04/04/25 14:55:34 - 0:03:14 - 456000 - Discriminator loss: 0.4305 - 5214 samples/s\n",
            "INFO - 04/04/25 14:55:35 - 0:03:16 - 460000 - Discriminator loss: 0.4309 - 5194 samples/s\n",
            "INFO - 04/04/25 14:55:37 - 0:03:17 - 464000 - Discriminator loss: 0.4298 - 5188 samples/s\n",
            "INFO - 04/04/25 14:55:39 - 0:03:19 - 468000 - Discriminator loss: 0.4297 - 5199 samples/s\n",
            "INFO - 04/04/25 14:55:40 - 0:03:20 - 472000 - Discriminator loss: 0.4304 - 5200 samples/s\n",
            "INFO - 04/04/25 14:55:42 - 0:03:22 - 476000 - Discriminator loss: 0.4305 - 5208 samples/s\n",
            "INFO - 04/04/25 14:55:43 - 0:03:24 - 480000 - Discriminator loss: 0.4304 - 5204 samples/s\n",
            "INFO - 04/04/25 14:55:45 - 0:03:25 - 484000 - Discriminator loss: 0.4276 - 5207 samples/s\n",
            "INFO - 04/04/25 14:55:46 - 0:03:27 - 488000 - Discriminator loss: 0.4292 - 5213 samples/s\n",
            "INFO - 04/04/25 14:55:48 - 0:03:28 - 492000 - Discriminator loss: 0.4289 - 5200 samples/s\n",
            "INFO - 04/04/25 14:55:49 - 0:03:30 - 496000 - Discriminator loss: 0.4281 - 5216 samples/s\n",
            "INFO - 04/04/25 14:55:51 - 0:03:31 - 500000 - Discriminator loss: 0.4275 - 5220 samples/s\n",
            "INFO - 04/04/25 14:55:52 - 0:03:33 - 504000 - Discriminator loss: 0.4266 - 5203 samples/s\n",
            "INFO - 04/04/25 14:55:54 - 0:03:34 - 508000 - Discriminator loss: 0.4273 - 5207 samples/s\n",
            "INFO - 04/04/25 14:55:55 - 0:03:36 - 512000 - Discriminator loss: 0.4261 - 5210 samples/s\n",
            "INFO - 04/04/25 14:55:57 - 0:03:37 - 516000 - Discriminator loss: 0.4259 - 5202 samples/s\n",
            "INFO - 04/04/25 14:55:59 - 0:03:39 - 520000 - Discriminator loss: 0.4254 - 5187 samples/s\n",
            "INFO - 04/04/25 14:56:00 - 0:03:40 - 524000 - Discriminator loss: 0.4256 - 5195 samples/s\n",
            "INFO - 04/04/25 14:56:02 - 0:03:42 - 528000 - Discriminator loss: 0.4240 - 5199 samples/s\n",
            "INFO - 04/04/25 14:56:03 - 0:03:43 - 532000 - Discriminator loss: 0.4236 - 5201 samples/s\n",
            "INFO - 04/04/25 14:56:05 - 0:03:45 - 536000 - Discriminator loss: 0.4219 - 5217 samples/s\n",
            "INFO - 04/04/25 14:56:06 - 0:03:47 - 540000 - Discriminator loss: 0.4218 - 5198 samples/s\n",
            "INFO - 04/04/25 14:56:08 - 0:03:48 - 544000 - Discriminator loss: 0.4232 - 5212 samples/s\n",
            "INFO - 04/04/25 14:56:09 - 0:03:50 - 548000 - Discriminator loss: 0.4228 - 5198 samples/s\n",
            "INFO - 04/04/25 14:56:11 - 0:03:51 - 552000 - Discriminator loss: 0.4226 - 5199 samples/s\n",
            "INFO - 04/04/25 14:56:12 - 0:03:53 - 556000 - Discriminator loss: 0.4225 - 5207 samples/s\n",
            "INFO - 04/04/25 14:56:14 - 0:03:54 - 560000 - Discriminator loss: 0.4220 - 5210 samples/s\n",
            "INFO - 04/04/25 14:56:15 - 0:03:56 - 564000 - Discriminator loss: 0.4214 - 5180 samples/s\n",
            "INFO - 04/04/25 14:56:17 - 0:03:57 - 568000 - Discriminator loss: 0.4185 - 5208 samples/s\n",
            "INFO - 04/04/25 14:56:19 - 0:03:59 - 572000 - Discriminator loss: 0.4200 - 5210 samples/s\n",
            "INFO - 04/04/25 14:56:20 - 0:04:00 - 576000 - Discriminator loss: 0.4203 - 5207 samples/s\n",
            "INFO - 04/04/25 14:56:22 - 0:04:02 - 580000 - Discriminator loss: 0.4182 - 5204 samples/s\n",
            "INFO - 04/04/25 14:56:23 - 0:04:03 - 584000 - Discriminator loss: 0.4192 - 5199 samples/s\n",
            "INFO - 04/04/25 14:56:25 - 0:04:05 - 588000 - Discriminator loss: 0.4181 - 5208 samples/s\n",
            "INFO - 04/04/25 14:56:26 - 0:04:07 - 592000 - Discriminator loss: 0.4196 - 5206 samples/s\n",
            "INFO - 04/04/25 14:56:28 - 0:04:08 - 596000 - Discriminator loss: 0.4181 - 5206 samples/s\n",
            "INFO - 04/04/25 14:56:29 - 0:04:10 - 600000 - Discriminator loss: 0.4182 - 5197 samples/s\n",
            "INFO - 04/04/25 14:56:31 - 0:04:11 - 604000 - Discriminator loss: 0.4194 - 5207 samples/s\n",
            "INFO - 04/04/25 14:56:32 - 0:04:13 - 608000 - Discriminator loss: 0.4198 - 5198 samples/s\n",
            "INFO - 04/04/25 14:56:34 - 0:04:14 - 612000 - Discriminator loss: 0.4164 - 5200 samples/s\n",
            "INFO - 04/04/25 14:56:35 - 0:04:16 - 616000 - Discriminator loss: 0.4161 - 5203 samples/s\n",
            "INFO - 04/04/25 14:56:37 - 0:04:17 - 620000 - Discriminator loss: 0.4141 - 5208 samples/s\n",
            "INFO - 04/04/25 14:56:39 - 0:04:19 - 624000 - Discriminator loss: 0.4149 - 5195 samples/s\n",
            "INFO - 04/04/25 14:56:40 - 0:04:20 - 628000 - Discriminator loss: 0.4140 - 5197 samples/s\n",
            "INFO - 04/04/25 14:56:42 - 0:04:22 - 632000 - Discriminator loss: 0.4141 - 5170 samples/s\n",
            "INFO - 04/04/25 14:56:43 - 0:04:23 - 636000 - Discriminator loss: 0.4158 - 5203 samples/s\n",
            "INFO - 04/04/25 14:56:45 - 0:04:25 - 640000 - Discriminator loss: 0.4140 - 5194 samples/s\n",
            "INFO - 04/04/25 14:56:46 - 0:04:27 - 644000 - Discriminator loss: 0.4145 - 5173 samples/s\n",
            "INFO - 04/04/25 14:56:48 - 0:04:28 - 648000 - Discriminator loss: 0.4128 - 5185 samples/s\n",
            "INFO - 04/04/25 14:56:49 - 0:04:30 - 652000 - Discriminator loss: 0.4123 - 5218 samples/s\n",
            "INFO - 04/04/25 14:56:51 - 0:04:31 - 656000 - Discriminator loss: 0.4136 - 5197 samples/s\n",
            "INFO - 04/04/25 14:56:52 - 0:04:33 - 660000 - Discriminator loss: 0.4146 - 5208 samples/s\n",
            "INFO - 04/04/25 14:56:54 - 0:04:34 - 664000 - Discriminator loss: 0.4148 - 5203 samples/s\n",
            "INFO - 04/04/25 14:56:55 - 0:04:36 - 668000 - Discriminator loss: 0.4135 - 5206 samples/s\n",
            "INFO - 04/04/25 14:56:57 - 0:04:37 - 672000 - Discriminator loss: 0.4138 - 5191 samples/s\n",
            "INFO - 04/04/25 14:56:59 - 0:04:39 - 676000 - Discriminator loss: 0.4122 - 5202 samples/s\n",
            "INFO - 04/04/25 14:57:00 - 0:04:40 - 680000 - Discriminator loss: 0.4112 - 5211 samples/s\n",
            "INFO - 04/04/25 14:57:02 - 0:04:42 - 684000 - Discriminator loss: 0.4105 - 5223 samples/s\n",
            "INFO - 04/04/25 14:57:03 - 0:04:43 - 688000 - Discriminator loss: 0.4122 - 5198 samples/s\n",
            "INFO - 04/04/25 14:57:05 - 0:04:45 - 692000 - Discriminator loss: 0.4108 - 5205 samples/s\n",
            "INFO - 04/04/25 14:57:06 - 0:04:47 - 696000 - Discriminator loss: 0.4109 - 5198 samples/s\n",
            "INFO - 04/04/25 14:57:08 - 0:04:48 - 700000 - Discriminator loss: 0.4086 - 5201 samples/s\n",
            "INFO - 04/04/25 14:57:09 - 0:04:50 - 704000 - Discriminator loss: 0.4103 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:11 - 0:04:51 - 708000 - Discriminator loss: 0.4103 - 5202 samples/s\n",
            "INFO - 04/04/25 14:57:12 - 0:04:53 - 712000 - Discriminator loss: 0.4087 - 5215 samples/s\n",
            "INFO - 04/04/25 14:57:14 - 0:04:54 - 716000 - Discriminator loss: 0.4086 - 5203 samples/s\n",
            "INFO - 04/04/25 14:57:15 - 0:04:56 - 720000 - Discriminator loss: 0.4095 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:17 - 0:04:57 - 724000 - Discriminator loss: 0.4088 - 5197 samples/s\n",
            "INFO - 04/04/25 14:57:18 - 0:04:59 - 728000 - Discriminator loss: 0.4082 - 5196 samples/s\n",
            "INFO - 04/04/25 14:57:20 - 0:05:00 - 732000 - Discriminator loss: 0.4073 - 5200 samples/s\n",
            "INFO - 04/04/25 14:57:22 - 0:05:02 - 736000 - Discriminator loss: 0.4079 - 5189 samples/s\n",
            "INFO - 04/04/25 14:57:23 - 0:05:03 - 740000 - Discriminator loss: 0.4077 - 5192 samples/s\n",
            "INFO - 04/04/25 14:57:25 - 0:05:05 - 744000 - Discriminator loss: 0.4088 - 5185 samples/s\n",
            "INFO - 04/04/25 14:57:26 - 0:05:07 - 748000 - Discriminator loss: 0.4081 - 5180 samples/s\n",
            "INFO - 04/04/25 14:57:28 - 0:05:08 - 752000 - Discriminator loss: 0.4081 - 5200 samples/s\n",
            "INFO - 04/04/25 14:57:29 - 0:05:10 - 756000 - Discriminator loss: 0.4059 - 5202 samples/s\n",
            "INFO - 04/04/25 14:57:31 - 0:05:11 - 760000 - Discriminator loss: 0.4080 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:32 - 0:05:13 - 764000 - Discriminator loss: 0.4064 - 5189 samples/s\n",
            "INFO - 04/04/25 14:57:34 - 0:05:14 - 768000 - Discriminator loss: 0.4063 - 5185 samples/s\n",
            "INFO - 04/04/25 14:57:35 - 0:05:16 - 772000 - Discriminator loss: 0.4061 - 5206 samples/s\n",
            "INFO - 04/04/25 14:57:37 - 0:05:17 - 776000 - Discriminator loss: 0.4075 - 5173 samples/s\n",
            "INFO - 04/04/25 14:57:39 - 0:05:19 - 780000 - Discriminator loss: 0.4075 - 5202 samples/s\n",
            "INFO - 04/04/25 14:57:40 - 0:05:20 - 784000 - Discriminator loss: 0.4070 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:42 - 0:05:22 - 788000 - Discriminator loss: 0.4057 - 5205 samples/s\n",
            "INFO - 04/04/25 14:57:43 - 0:05:23 - 792000 - Discriminator loss: 0.4030 - 5185 samples/s\n",
            "INFO - 04/04/25 14:57:45 - 0:05:25 - 796000 - Discriminator loss: 0.4052 - 5196 samples/s\n",
            "INFO - 04/04/25 14:57:46 - 0:05:27 - 800000 - Discriminator loss: 0.4036 - 5183 samples/s\n",
            "INFO - 04/04/25 14:57:48 - 0:05:28 - 804000 - Discriminator loss: 0.4054 - 5205 samples/s\n",
            "INFO - 04/04/25 14:57:49 - 0:05:30 - 808000 - Discriminator loss: 0.4029 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:51 - 0:05:31 - 812000 - Discriminator loss: 0.4021 - 5208 samples/s\n",
            "INFO - 04/04/25 14:57:52 - 0:05:33 - 816000 - Discriminator loss: 0.4028 - 5205 samples/s\n",
            "INFO - 04/04/25 14:57:54 - 0:05:34 - 820000 - Discriminator loss: 0.4020 - 5214 samples/s\n",
            "INFO - 04/04/25 14:57:55 - 0:05:36 - 824000 - Discriminator loss: 0.4025 - 5204 samples/s\n",
            "INFO - 04/04/25 14:57:57 - 0:05:37 - 828000 - Discriminator loss: 0.4039 - 5207 samples/s\n",
            "INFO - 04/04/25 14:57:59 - 0:05:39 - 832000 - Discriminator loss: 0.4023 - 5213 samples/s\n",
            "INFO - 04/04/25 14:58:00 - 0:05:40 - 836000 - Discriminator loss: 0.4033 - 5195 samples/s\n",
            "INFO - 04/04/25 14:58:02 - 0:05:42 - 840000 - Discriminator loss: 0.4029 - 5188 samples/s\n",
            "INFO - 04/04/25 14:58:03 - 0:05:43 - 844000 - Discriminator loss: 0.4024 - 5206 samples/s\n",
            "INFO - 04/04/25 14:58:05 - 0:05:45 - 848000 - Discriminator loss: 0.4033 - 5189 samples/s\n",
            "INFO - 04/04/25 14:58:06 - 0:05:47 - 852000 - Discriminator loss: 0.4031 - 5195 samples/s\n",
            "INFO - 04/04/25 14:58:08 - 0:05:48 - 856000 - Discriminator loss: 0.4019 - 5208 samples/s\n",
            "INFO - 04/04/25 14:58:09 - 0:05:50 - 860000 - Discriminator loss: 0.4022 - 5203 samples/s\n",
            "INFO - 04/04/25 14:58:11 - 0:05:51 - 864000 - Discriminator loss: 0.4007 - 5181 samples/s\n",
            "INFO - 04/04/25 14:58:12 - 0:05:53 - 868000 - Discriminator loss: 0.4000 - 5201 samples/s\n",
            "INFO - 04/04/25 14:58:14 - 0:05:54 - 872000 - Discriminator loss: 0.4024 - 5193 samples/s\n",
            "INFO - 04/04/25 14:58:15 - 0:05:56 - 876000 - Discriminator loss: 0.4001 - 5205 samples/s\n",
            "INFO - 04/04/25 14:58:17 - 0:05:57 - 880000 - Discriminator loss: 0.4002 - 5208 samples/s\n",
            "INFO - 04/04/25 14:58:19 - 0:05:59 - 884000 - Discriminator loss: 0.4010 - 5197 samples/s\n",
            "INFO - 04/04/25 14:58:20 - 0:06:00 - 888000 - Discriminator loss: 0.3991 - 5206 samples/s\n",
            "INFO - 04/04/25 14:58:22 - 0:06:02 - 892000 - Discriminator loss: 0.4002 - 5194 samples/s\n",
            "INFO - 04/04/25 14:58:23 - 0:06:03 - 896000 - Discriminator loss: 0.4006 - 5200 samples/s\n",
            "INFO - 04/04/25 14:58:25 - 0:06:05 - 900000 - Discriminator loss: 0.4003 - 5210 samples/s\n",
            "INFO - 04/04/25 14:58:26 - 0:06:07 - 904000 - Discriminator loss: 0.3994 - 5207 samples/s\n",
            "INFO - 04/04/25 14:58:28 - 0:06:08 - 908000 - Discriminator loss: 0.3977 - 5213 samples/s\n",
            "INFO - 04/04/25 14:58:29 - 0:06:10 - 912000 - Discriminator loss: 0.3979 - 5212 samples/s\n",
            "INFO - 04/04/25 14:58:31 - 0:06:11 - 916000 - Discriminator loss: 0.4005 - 5197 samples/s\n",
            "INFO - 04/04/25 14:58:32 - 0:06:13 - 920000 - Discriminator loss: 0.3977 - 5193 samples/s\n",
            "INFO - 04/04/25 14:58:34 - 0:06:14 - 924000 - Discriminator loss: 0.3978 - 5193 samples/s\n",
            "INFO - 04/04/25 14:58:35 - 0:06:16 - 928000 - Discriminator loss: 0.3987 - 5199 samples/s\n",
            "INFO - 04/04/25 14:58:37 - 0:06:17 - 932000 - Discriminator loss: 0.3980 - 5202 samples/s\n",
            "INFO - 04/04/25 14:58:39 - 0:06:19 - 936000 - Discriminator loss: 0.3984 - 5202 samples/s\n",
            "INFO - 04/04/25 14:58:40 - 0:06:20 - 940000 - Discriminator loss: 0.3979 - 5199 samples/s\n",
            "INFO - 04/04/25 14:58:42 - 0:06:22 - 944000 - Discriminator loss: 0.3967 - 5198 samples/s\n",
            "INFO - 04/04/25 14:58:43 - 0:06:23 - 948000 - Discriminator loss: 0.3985 - 5192 samples/s\n",
            "INFO - 04/04/25 14:58:45 - 0:06:25 - 952000 - Discriminator loss: 0.3982 - 5194 samples/s\n",
            "INFO - 04/04/25 14:58:46 - 0:06:27 - 956000 - Discriminator loss: 0.3974 - 5198 samples/s\n",
            "INFO - 04/04/25 14:58:48 - 0:06:28 - 960000 - Discriminator loss: 0.3975 - 5218 samples/s\n",
            "INFO - 04/04/25 14:58:49 - 0:06:30 - 964000 - Discriminator loss: 0.3965 - 5215 samples/s\n",
            "INFO - 04/04/25 14:58:51 - 0:06:31 - 968000 - Discriminator loss: 0.3947 - 5211 samples/s\n",
            "INFO - 04/04/25 14:58:52 - 0:06:33 - 972000 - Discriminator loss: 0.3977 - 5215 samples/s\n",
            "INFO - 04/04/25 14:58:54 - 0:06:34 - 976000 - Discriminator loss: 0.3953 - 5217 samples/s\n",
            "INFO - 04/04/25 14:58:55 - 0:06:36 - 980000 - Discriminator loss: 0.3961 - 5200 samples/s\n",
            "INFO - 04/04/25 14:58:57 - 0:06:37 - 984000 - Discriminator loss: 0.3948 - 5211 samples/s\n",
            "INFO - 04/04/25 14:58:58 - 0:06:39 - 988000 - Discriminator loss: 0.3958 - 5219 samples/s\n",
            "INFO - 04/04/25 14:59:00 - 0:06:40 - 992000 - Discriminator loss: 0.3962 - 5205 samples/s\n",
            "INFO - 04/04/25 14:59:02 - 0:06:42 - 996000 - Discriminator loss: 0.3959 - 5209 samples/s\n",
            "INFO - 04/04/25 14:59:03 - 0:06:44 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 14:59:03 - 0:06:44 - 1500 source words - nn - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 14:59:03 - 0:06:44 - 1500 source words - nn - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 14:59:03 - 0:06:44 - 1500 source words - nn - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 14:59:03 - 0:06:44 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python unsupervised.py --src_lang en --tgt_lang hi --src_emb /data/cc.en.300.vec --tgt_emb /data/cc.hi.300.vec --n_refinement 5 --export \"pth\" --dico_eval /data/en-hi.5000-6500.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caacd736-da20-4990-8996-46c502f44caf",
      "metadata": {
        "id": "caacd736-da20-4990-8996-46c502f44caf",
        "outputId": "bc1d5fb4-f0e0-48a0-d004-dec37220e54f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO - 04/04/25 15:24:12 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 04/04/25 15:24:12 - 0:00:00 - adversarial: True\n",
            "                                     batch_size: 32\n",
            "                                     cuda: True\n",
            "                                     dico_build: S2T\n",
            "                                     dico_eval: /workspace/lipsync/LatentSync/expssssss/en-hi.5000-6500.txt\n",
            "                                     dico_max_rank: 15000\n",
            "                                     dico_max_size: 0\n",
            "                                     dico_method: csls_knn_10\n",
            "                                     dico_min_size: 0\n",
            "                                     dico_threshold: 0\n",
            "                                     dis_clip_weights: 0\n",
            "                                     dis_dropout: 0.0\n",
            "                                     dis_hid_dim: 2048\n",
            "                                     dis_input_dropout: 0.1\n",
            "                                     dis_lambda: 1\n",
            "                                     dis_layers: 2\n",
            "                                     dis_most_frequent: 75000\n",
            "                                     dis_optimizer: sgd,lr=0.1\n",
            "                                     dis_smooth: 0.1\n",
            "                                     dis_steps: 5\n",
            "                                     emb_dim: 300\n",
            "                                     epoch_size: 1000000\n",
            "                                     exp_id: \n",
            "                                     exp_name: debug\n",
            "                                     exp_path: /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/szdk2p3tqp\n",
            "                                     export: pth\n",
            "                                     lr_decay: 0.98\n",
            "                                     lr_shrink: 0.5\n",
            "                                     map_beta: 0.001\n",
            "                                     map_id_init: True\n",
            "                                     map_optimizer: sgd,lr=0.1\n",
            "                                     max_vocab: 200000\n",
            "                                     min_lr: 1e-06\n",
            "                                     n_epochs: 5\n",
            "                                     n_refinement: 5\n",
            "                                     normalize_embeddings: \n",
            "                                     seed: -1\n",
            "                                     src_emb: /workspace/lipsync/LatentSync/expssssss/MUSE/data/wiki.en.vec\n",
            "                                     src_lang: en\n",
            "                                     tgt_emb: /workspace/lipsync/LatentSync/expssssss/MUSE/data/wiki.hi.vec\n",
            "                                     tgt_lang: hi\n",
            "                                     verbose: 2\n",
            "INFO - 04/04/25 15:24:12 - 0:00:00 - The experiment will be stored in /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/szdk2p3tqp\n",
            "INFO - 04/04/25 15:24:21 - 0:00:09 - Loaded 200000 pre-trained word embeddings.\n",
            "INFO - 04/04/25 15:24:30 - 0:00:18 - Loaded 158016 pre-trained word embeddings.\n",
            "INFO - 04/04/25 15:24:31 - 0:00:19 - ----> ADVERSARIAL TRAINING <----\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 15:24:31 - 0:00:19 - Starting adversarial training epoch 0...\n",
            "INFO - 04/04/25 15:24:31 - 0:00:19 - 000000 - Discriminator loss: 0.6780 - 652 samples/s\n",
            "INFO - 04/04/25 15:24:33 - 0:00:21 - 004000 - Discriminator loss: 0.3424 - 5324 samples/s\n",
            "INFO - 04/04/25 15:24:34 - 0:00:22 - 008000 - Discriminator loss: 0.3304 - 5303 samples/s\n",
            "INFO - 04/04/25 15:24:36 - 0:00:24 - 012000 - Discriminator loss: 0.3298 - 5327 samples/s\n",
            "INFO - 04/04/25 15:24:37 - 0:00:25 - 016000 - Discriminator loss: 0.3295 - 5346 samples/s\n",
            "INFO - 04/04/25 15:24:39 - 0:00:27 - 020000 - Discriminator loss: 0.3292 - 5325 samples/s\n",
            "INFO - 04/04/25 15:24:40 - 0:00:28 - 024000 - Discriminator loss: 0.3290 - 5342 samples/s\n",
            "INFO - 04/04/25 15:24:42 - 0:00:30 - 028000 - Discriminator loss: 0.3288 - 5361 samples/s\n",
            "INFO - 04/04/25 15:24:43 - 0:00:31 - 032000 - Discriminator loss: 0.3287 - 5351 samples/s\n",
            "INFO - 04/04/25 15:24:45 - 0:00:33 - 036000 - Discriminator loss: 0.3285 - 5347 samples/s\n",
            "INFO - 04/04/25 15:24:46 - 0:00:34 - 040000 - Discriminator loss: 0.3284 - 5326 samples/s\n",
            "INFO - 04/04/25 15:24:48 - 0:00:36 - 044000 - Discriminator loss: 0.3281 - 5353 samples/s\n",
            "INFO - 04/04/25 15:24:49 - 0:00:37 - 048000 - Discriminator loss: 0.3281 - 5358 samples/s\n",
            "INFO - 04/04/25 15:24:51 - 0:00:39 - 052000 - Discriminator loss: 0.3280 - 5357 samples/s\n",
            "INFO - 04/04/25 15:24:52 - 0:00:40 - 056000 - Discriminator loss: 0.3278 - 5368 samples/s\n",
            "INFO - 04/04/25 15:24:54 - 0:00:42 - 060000 - Discriminator loss: 0.3277 - 5362 samples/s\n",
            "INFO - 04/04/25 15:24:55 - 0:00:43 - 064000 - Discriminator loss: 0.3276 - 5358 samples/s\n",
            "INFO - 04/04/25 15:24:57 - 0:00:45 - 068000 - Discriminator loss: 0.3274 - 5359 samples/s\n",
            "INFO - 04/04/25 15:24:58 - 0:00:46 - 072000 - Discriminator loss: 0.3273 - 5358 samples/s\n",
            "INFO - 04/04/25 15:25:00 - 0:00:48 - 076000 - Discriminator loss: 0.3272 - 5362 samples/s\n",
            "INFO - 04/04/25 15:25:01 - 0:00:49 - 080000 - Discriminator loss: 0.3271 - 5381 samples/s\n",
            "INFO - 04/04/25 15:25:03 - 0:00:51 - 084000 - Discriminator loss: 0.3270 - 5337 samples/s\n",
            "INFO - 04/04/25 15:25:04 - 0:00:52 - 088000 - Discriminator loss: 0.3268 - 5376 samples/s\n",
            "INFO - 04/04/25 15:25:06 - 0:00:54 - 092000 - Discriminator loss: 0.3267 - 5354 samples/s\n",
            "INFO - 04/04/25 15:25:07 - 0:00:55 - 096000 - Discriminator loss: 0.3266 - 5366 samples/s\n",
            "INFO - 04/04/25 15:25:09 - 0:00:57 - 100000 - Discriminator loss: 0.3265 - 5381 samples/s\n",
            "INFO - 04/04/25 15:25:10 - 0:00:58 - 104000 - Discriminator loss: 0.3264 - 5374 samples/s\n",
            "INFO - 04/04/25 15:25:12 - 0:01:00 - 108000 - Discriminator loss: 0.3263 - 5381 samples/s\n",
            "INFO - 04/04/25 15:25:13 - 0:01:01 - 112000 - Discriminator loss: 0.3261 - 5376 samples/s\n",
            "INFO - 04/04/25 15:25:15 - 0:01:03 - 116000 - Discriminator loss: 0.3261 - 5378 samples/s\n",
            "INFO - 04/04/25 15:25:16 - 0:01:04 - 120000 - Discriminator loss: 0.3260 - 5372 samples/s\n",
            "INFO - 04/04/25 15:25:18 - 0:01:06 - 124000 - Discriminator loss: 0.3259 - 5385 samples/s\n",
            "INFO - 04/04/25 15:25:19 - 0:01:07 - 128000 - Discriminator loss: 0.3258 - 5382 samples/s\n",
            "INFO - 04/04/25 15:25:21 - 0:01:09 - 132000 - Discriminator loss: 0.3258 - 5383 samples/s\n",
            "INFO - 04/04/25 15:25:22 - 0:01:10 - 136000 - Discriminator loss: 0.3257 - 5382 samples/s\n",
            "INFO - 04/04/25 15:25:23 - 0:01:12 - 140000 - Discriminator loss: 0.3257 - 5376 samples/s\n",
            "INFO - 04/04/25 15:25:25 - 0:01:13 - 144000 - Discriminator loss: 0.3256 - 5382 samples/s\n",
            "INFO - 04/04/25 15:25:26 - 0:01:14 - 148000 - Discriminator loss: 0.3256 - 5384 samples/s\n",
            "INFO - 04/04/25 15:25:28 - 0:01:16 - 152000 - Discriminator loss: 0.3255 - 5383 samples/s\n",
            "INFO - 04/04/25 15:25:29 - 0:01:17 - 156000 - Discriminator loss: 0.3255 - 5388 samples/s\n",
            "INFO - 04/04/25 15:25:31 - 0:01:19 - 160000 - Discriminator loss: 0.3255 - 5389 samples/s\n",
            "INFO - 04/04/25 15:25:32 - 0:01:20 - 164000 - Discriminator loss: 0.3254 - 5366 samples/s\n",
            "INFO - 04/04/25 15:25:34 - 0:01:22 - 168000 - Discriminator loss: 0.3254 - 5371 samples/s\n",
            "INFO - 04/04/25 15:25:35 - 0:01:23 - 172000 - Discriminator loss: 0.3254 - 5368 samples/s\n",
            "INFO - 04/04/25 15:25:37 - 0:01:25 - 176000 - Discriminator loss: 0.3254 - 5380 samples/s\n",
            "INFO - 04/04/25 15:25:38 - 0:01:26 - 180000 - Discriminator loss: 0.3254 - 5374 samples/s\n",
            "INFO - 04/04/25 15:25:40 - 0:01:28 - 184000 - Discriminator loss: 0.3253 - 5383 samples/s\n",
            "INFO - 04/04/25 15:25:41 - 0:01:29 - 188000 - Discriminator loss: 0.3253 - 5377 samples/s\n",
            "INFO - 04/04/25 15:25:43 - 0:01:31 - 192000 - Discriminator loss: 0.3253 - 5387 samples/s\n",
            "INFO - 04/04/25 15:25:44 - 0:01:32 - 196000 - Discriminator loss: 0.3253 - 5379 samples/s\n",
            "INFO - 04/04/25 15:25:46 - 0:01:34 - 200000 - Discriminator loss: 0.3253 - 5365 samples/s\n",
            "INFO - 04/04/25 15:25:47 - 0:01:35 - 204000 - Discriminator loss: 0.3253 - 5385 samples/s\n",
            "INFO - 04/04/25 15:25:49 - 0:01:37 - 208000 - Discriminator loss: 0.3253 - 5391 samples/s\n",
            "INFO - 04/04/25 15:25:50 - 0:01:38 - 212000 - Discriminator loss: 0.3253 - 5379 samples/s\n",
            "INFO - 04/04/25 15:25:52 - 0:01:40 - 216000 - Discriminator loss: 0.3252 - 5386 samples/s\n",
            "INFO - 04/04/25 15:25:53 - 0:01:41 - 220000 - Discriminator loss: 0.3252 - 5391 samples/s\n",
            "INFO - 04/04/25 15:25:55 - 0:01:43 - 224000 - Discriminator loss: 0.3252 - 5390 samples/s\n",
            "INFO - 04/04/25 15:25:56 - 0:01:44 - 228000 - Discriminator loss: 0.3252 - 5379 samples/s\n",
            "INFO - 04/04/25 15:25:58 - 0:01:46 - 232000 - Discriminator loss: 0.3252 - 5388 samples/s\n",
            "INFO - 04/04/25 15:25:59 - 0:01:47 - 236000 - Discriminator loss: 0.3252 - 5384 samples/s\n",
            "INFO - 04/04/25 15:26:01 - 0:01:49 - 240000 - Discriminator loss: 0.3252 - 5384 samples/s\n",
            "INFO - 04/04/25 15:26:02 - 0:01:50 - 244000 - Discriminator loss: 0.3252 - 5375 samples/s\n",
            "INFO - 04/04/25 15:26:04 - 0:01:52 - 248000 - Discriminator loss: 0.3252 - 5382 samples/s\n",
            "INFO - 04/04/25 15:26:05 - 0:01:53 - 252000 - Discriminator loss: 0.3252 - 5391 samples/s\n",
            "INFO - 04/04/25 15:26:07 - 0:01:55 - 256000 - Discriminator loss: 0.3252 - 5356 samples/s\n",
            "INFO - 04/04/25 15:26:08 - 0:01:56 - 260000 - Discriminator loss: 0.3252 - 5386 samples/s\n",
            "INFO - 04/04/25 15:26:10 - 0:01:58 - 264000 - Discriminator loss: 0.3252 - 5382 samples/s\n",
            "INFO - 04/04/25 15:26:11 - 0:01:59 - 268000 - Discriminator loss: 0.3252 - 5386 samples/s\n",
            "INFO - 04/04/25 15:26:13 - 0:02:01 - 272000 - Discriminator loss: 0.3252 - 5381 samples/s\n",
            "INFO - 04/04/25 15:26:14 - 0:02:02 - 276000 - Discriminator loss: 0.3252 - 5383 samples/s\n",
            "INFO - 04/04/25 15:26:16 - 0:02:04 - 280000 - Discriminator loss: 0.3252 - 5378 samples/s\n",
            "INFO - 04/04/25 15:26:17 - 0:02:05 - 284000 - Discriminator loss: 0.3252 - 5369 samples/s\n",
            "INFO - 04/04/25 15:26:18 - 0:02:07 - 288000 - Discriminator loss: 0.3252 - 5385 samples/s\n",
            "INFO - 04/04/25 15:26:20 - 0:02:08 - 292000 - Discriminator loss: 0.3252 - 5387 samples/s\n",
            "INFO - 04/04/25 15:26:21 - 0:02:10 - 296000 - Discriminator loss: 0.3252 - 5384 samples/s\n",
            "INFO - 04/04/25 15:26:23 - 0:02:11 - 300000 - Discriminator loss: 0.3252 - 5382 samples/s\n",
            "INFO - 04/04/25 15:26:24 - 0:02:12 - 304000 - Discriminator loss: 0.3252 - 5385 samples/s\n",
            "INFO - 04/04/25 15:26:26 - 0:02:14 - 308000 - Discriminator loss: 0.3252 - 5380 samples/s\n",
            "INFO - 04/04/25 15:26:27 - 0:02:15 - 312000 - Discriminator loss: 0.3252 - 5378 samples/s\n",
            "INFO - 04/04/25 15:26:29 - 0:02:17 - 316000 - Discriminator loss: 0.3252 - 5383 samples/s\n",
            "INFO - 04/04/25 15:26:30 - 0:02:18 - 320000 - Discriminator loss: 0.3252 - 5376 samples/s\n",
            "INFO - 04/04/25 15:26:32 - 0:02:20 - 324000 - Discriminator loss: 0.3252 - 5365 samples/s\n",
            "INFO - 04/04/25 15:26:33 - 0:02:21 - 328000 - Discriminator loss: 0.3252 - 5467 samples/s\n",
            "INFO - 04/04/25 15:26:35 - 0:02:23 - 332000 - Discriminator loss: 0.3252 - 5453 samples/s\n",
            "INFO - 04/04/25 15:26:36 - 0:02:24 - 336000 - Discriminator loss: 0.3252 - 5437 samples/s\n",
            "INFO - 04/04/25 15:26:38 - 0:02:26 - 340000 - Discriminator loss: 0.3252 - 5461 samples/s\n",
            "INFO - 04/04/25 15:26:39 - 0:02:27 - 344000 - Discriminator loss: 0.3252 - 5445 samples/s\n",
            "INFO - 04/04/25 15:26:41 - 0:02:29 - 348000 - Discriminator loss: 0.3252 - 5440 samples/s\n",
            "INFO - 04/04/25 15:26:42 - 0:02:30 - 352000 - Discriminator loss: 0.3252 - 5471 samples/s\n",
            "INFO - 04/04/25 15:26:44 - 0:02:32 - 356000 - Discriminator loss: 0.3252 - 5460 samples/s\n",
            "INFO - 04/04/25 15:26:45 - 0:02:33 - 360000 - Discriminator loss: 0.3252 - 5474 samples/s\n",
            "INFO - 04/04/25 15:26:47 - 0:02:35 - 364000 - Discriminator loss: 0.3252 - 5451 samples/s\n",
            "INFO - 04/04/25 15:26:48 - 0:02:36 - 368000 - Discriminator loss: 0.3252 - 5475 samples/s\n",
            "INFO - 04/04/25 15:26:49 - 0:02:38 - 372000 - Discriminator loss: 0.3252 - 5471 samples/s\n",
            "INFO - 04/04/25 15:26:51 - 0:02:39 - 376000 - Discriminator loss: 0.3251 - 5474 samples/s\n",
            "INFO - 04/04/25 15:26:52 - 0:02:40 - 380000 - Discriminator loss: 0.3251 - 5475 samples/s\n",
            "INFO - 04/04/25 15:26:54 - 0:02:42 - 384000 - Discriminator loss: 0.3251 - 5470 samples/s\n",
            "INFO - 04/04/25 15:26:55 - 0:02:43 - 388000 - Discriminator loss: 0.3251 - 5477 samples/s\n",
            "INFO - 04/04/25 15:26:57 - 0:02:45 - 392000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:26:58 - 0:02:46 - 396000 - Discriminator loss: 0.3251 - 5476 samples/s\n",
            "INFO - 04/04/25 15:27:00 - 0:02:48 - 400000 - Discriminator loss: 0.3251 - 5478 samples/s\n",
            "INFO - 04/04/25 15:27:01 - 0:02:49 - 404000 - Discriminator loss: 0.3251 - 5475 samples/s\n",
            "INFO - 04/04/25 15:27:03 - 0:02:51 - 408000 - Discriminator loss: 0.3251 - 5469 samples/s\n",
            "INFO - 04/04/25 15:27:04 - 0:02:52 - 412000 - Discriminator loss: 0.3251 - 5476 samples/s\n",
            "INFO - 04/04/25 15:27:06 - 0:02:54 - 416000 - Discriminator loss: 0.3251 - 5474 samples/s\n",
            "INFO - 04/04/25 15:27:07 - 0:02:55 - 420000 - Discriminator loss: 0.3251 - 5475 samples/s\n",
            "INFO - 04/04/25 15:27:08 - 0:02:57 - 424000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:27:10 - 0:02:58 - 428000 - Discriminator loss: 0.3251 - 5472 samples/s\n",
            "INFO - 04/04/25 15:27:11 - 0:02:59 - 432000 - Discriminator loss: 0.3251 - 5470 samples/s\n",
            "INFO - 04/04/25 15:27:13 - 0:03:01 - 436000 - Discriminator loss: 0.3251 - 5473 samples/s\n",
            "INFO - 04/04/25 15:27:14 - 0:03:02 - 440000 - Discriminator loss: 0.3251 - 5473 samples/s\n",
            "INFO - 04/04/25 15:27:16 - 0:03:04 - 444000 - Discriminator loss: 0.3251 - 5477 samples/s\n",
            "INFO - 04/04/25 15:27:17 - 0:03:05 - 448000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:27:19 - 0:03:07 - 452000 - Discriminator loss: 0.3251 - 5461 samples/s\n",
            "INFO - 04/04/25 15:27:20 - 0:03:08 - 456000 - Discriminator loss: 0.3251 - 5470 samples/s\n",
            "INFO - 04/04/25 15:27:22 - 0:03:10 - 460000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:27:23 - 0:03:11 - 464000 - Discriminator loss: 0.3251 - 5473 samples/s\n",
            "INFO - 04/04/25 15:27:25 - 0:03:13 - 468000 - Discriminator loss: 0.3251 - 5473 samples/s\n",
            "INFO - 04/04/25 15:27:26 - 0:03:14 - 472000 - Discriminator loss: 0.3251 - 5478 samples/s\n",
            "INFO - 04/04/25 15:27:27 - 0:03:16 - 476000 - Discriminator loss: 0.3251 - 5468 samples/s\n",
            "INFO - 04/04/25 15:27:29 - 0:03:17 - 480000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:27:30 - 0:03:18 - 484000 - Discriminator loss: 0.3251 - 5471 samples/s\n",
            "INFO - 04/04/25 15:27:32 - 0:03:20 - 488000 - Discriminator loss: 0.3251 - 5458 samples/s\n",
            "INFO - 04/04/25 15:27:33 - 0:03:21 - 492000 - Discriminator loss: 0.3251 - 5464 samples/s\n",
            "INFO - 04/04/25 15:27:35 - 0:03:23 - 496000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:27:36 - 0:03:24 - 500000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:27:38 - 0:03:26 - 504000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:27:39 - 0:03:27 - 508000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:27:41 - 0:03:29 - 512000 - Discriminator loss: 0.3251 - 5468 samples/s\n",
            "INFO - 04/04/25 15:27:42 - 0:03:30 - 516000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:27:44 - 0:03:32 - 520000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:27:45 - 0:03:33 - 524000 - Discriminator loss: 0.3251 - 5468 samples/s\n",
            "INFO - 04/04/25 15:27:47 - 0:03:35 - 528000 - Discriminator loss: 0.3251 - 5447 samples/s\n",
            "INFO - 04/04/25 15:27:48 - 0:03:36 - 532000 - Discriminator loss: 0.3251 - 5471 samples/s\n",
            "INFO - 04/04/25 15:27:49 - 0:03:37 - 536000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:27:51 - 0:03:39 - 540000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:27:52 - 0:03:40 - 544000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:27:54 - 0:03:42 - 548000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:27:55 - 0:03:43 - 552000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:27:57 - 0:03:45 - 556000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:27:58 - 0:03:46 - 560000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:28:00 - 0:03:48 - 564000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:28:01 - 0:03:49 - 568000 - Discriminator loss: 0.3251 - 5468 samples/s\n",
            "INFO - 04/04/25 15:28:03 - 0:03:51 - 572000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:28:04 - 0:03:52 - 576000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:28:06 - 0:03:54 - 580000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:28:07 - 0:03:55 - 584000 - Discriminator loss: 0.3251 - 5461 samples/s\n",
            "INFO - 04/04/25 15:28:08 - 0:03:57 - 588000 - Discriminator loss: 0.3251 - 5457 samples/s\n",
            "INFO - 04/04/25 15:28:10 - 0:03:58 - 592000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:28:11 - 0:03:59 - 596000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:28:13 - 0:04:01 - 600000 - Discriminator loss: 0.3251 - 5464 samples/s\n",
            "INFO - 04/04/25 15:28:14 - 0:04:02 - 604000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:28:16 - 0:04:04 - 608000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:28:17 - 0:04:05 - 612000 - Discriminator loss: 0.3251 - 5456 samples/s\n",
            "INFO - 04/04/25 15:28:19 - 0:04:07 - 616000 - Discriminator loss: 0.3251 - 5474 samples/s\n",
            "INFO - 04/04/25 15:28:20 - 0:04:08 - 620000 - Discriminator loss: 0.3251 - 5464 samples/s\n",
            "INFO - 04/04/25 15:28:22 - 0:04:10 - 624000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:28:23 - 0:04:11 - 628000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:28:25 - 0:04:13 - 632000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:28:26 - 0:04:14 - 636000 - Discriminator loss: 0.3251 - 5454 samples/s\n",
            "INFO - 04/04/25 15:28:28 - 0:04:16 - 640000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:28:29 - 0:04:17 - 644000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:28:30 - 0:04:19 - 648000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:28:32 - 0:04:20 - 652000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:28:33 - 0:04:21 - 656000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:28:35 - 0:04:23 - 660000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:28:36 - 0:04:24 - 664000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:28:38 - 0:04:26 - 668000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:28:39 - 0:04:27 - 672000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:28:41 - 0:04:29 - 676000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:28:42 - 0:04:30 - 680000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:28:44 - 0:04:32 - 684000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:28:45 - 0:04:33 - 688000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:28:47 - 0:04:35 - 692000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:28:48 - 0:04:36 - 696000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:28:50 - 0:04:38 - 700000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:28:51 - 0:04:39 - 704000 - Discriminator loss: 0.3251 - 5460 samples/s\n",
            "INFO - 04/04/25 15:28:52 - 0:04:41 - 708000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:28:54 - 0:04:42 - 712000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:28:55 - 0:04:43 - 716000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:28:57 - 0:04:45 - 720000 - Discriminator loss: 0.3251 - 5457 samples/s\n",
            "INFO - 04/04/25 15:28:58 - 0:04:46 - 724000 - Discriminator loss: 0.3251 - 5468 samples/s\n",
            "INFO - 04/04/25 15:29:00 - 0:04:48 - 728000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:29:01 - 0:04:49 - 732000 - Discriminator loss: 0.3251 - 5456 samples/s\n",
            "INFO - 04/04/25 15:29:03 - 0:04:51 - 736000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:29:04 - 0:04:52 - 740000 - Discriminator loss: 0.3251 - 5457 samples/s\n",
            "INFO - 04/04/25 15:29:06 - 0:04:54 - 744000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:29:07 - 0:04:55 - 748000 - Discriminator loss: 0.3251 - 5466 samples/s\n",
            "INFO - 04/04/25 15:29:09 - 0:04:57 - 752000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:29:10 - 0:04:58 - 756000 - Discriminator loss: 0.3251 - 5471 samples/s\n",
            "INFO - 04/04/25 15:29:12 - 0:05:00 - 760000 - Discriminator loss: 0.3251 - 5456 samples/s\n",
            "INFO - 04/04/25 15:29:13 - 0:05:01 - 764000 - Discriminator loss: 0.3251 - 5467 samples/s\n",
            "INFO - 04/04/25 15:29:14 - 0:05:03 - 768000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:29:16 - 0:05:04 - 772000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:29:17 - 0:05:05 - 776000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:29:19 - 0:05:07 - 780000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:29:20 - 0:05:08 - 784000 - Discriminator loss: 0.3251 - 5471 samples/s\n",
            "INFO - 04/04/25 15:29:22 - 0:05:10 - 788000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:29:23 - 0:05:11 - 792000 - Discriminator loss: 0.3251 - 5456 samples/s\n",
            "INFO - 04/04/25 15:29:25 - 0:05:13 - 796000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:29:26 - 0:05:14 - 800000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:29:28 - 0:05:16 - 804000 - Discriminator loss: 0.3251 - 5460 samples/s\n",
            "INFO - 04/04/25 15:29:29 - 0:05:17 - 808000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:29:31 - 0:05:19 - 812000 - Discriminator loss: 0.3251 - 5463 samples/s\n",
            "INFO - 04/04/25 15:29:32 - 0:05:20 - 816000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:29:34 - 0:05:22 - 820000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:29:35 - 0:05:23 - 824000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:29:37 - 0:05:25 - 828000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:29:38 - 0:05:26 - 832000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:29:39 - 0:05:27 - 836000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:29:41 - 0:05:29 - 840000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:29:42 - 0:05:30 - 844000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:29:44 - 0:05:32 - 848000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:29:45 - 0:05:33 - 852000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:29:47 - 0:05:35 - 856000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:29:48 - 0:05:36 - 860000 - Discriminator loss: 0.3251 - 5454 samples/s\n",
            "INFO - 04/04/25 15:29:50 - 0:05:38 - 864000 - Discriminator loss: 0.3251 - 5452 samples/s\n",
            "INFO - 04/04/25 15:29:51 - 0:05:39 - 868000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:29:53 - 0:05:41 - 872000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:29:54 - 0:05:42 - 876000 - Discriminator loss: 0.3251 - 5458 samples/s\n",
            "INFO - 04/04/25 15:29:56 - 0:05:44 - 880000 - Discriminator loss: 0.3251 - 5463 samples/s\n",
            "INFO - 04/04/25 15:29:57 - 0:05:45 - 884000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:29:59 - 0:05:47 - 888000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:30:00 - 0:05:48 - 892000 - Discriminator loss: 0.3251 - 5457 samples/s\n",
            "INFO - 04/04/25 15:30:01 - 0:05:50 - 896000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:30:03 - 0:05:51 - 900000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:30:04 - 0:05:52 - 904000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:30:06 - 0:05:54 - 908000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:30:07 - 0:05:55 - 912000 - Discriminator loss: 0.3251 - 5452 samples/s\n",
            "INFO - 04/04/25 15:30:09 - 0:05:57 - 916000 - Discriminator loss: 0.3251 - 5461 samples/s\n",
            "INFO - 04/04/25 15:30:10 - 0:05:58 - 920000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:30:12 - 0:06:00 - 924000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:30:13 - 0:06:01 - 928000 - Discriminator loss: 0.3251 - 5462 samples/s\n",
            "INFO - 04/04/25 15:30:15 - 0:06:03 - 932000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:30:16 - 0:06:04 - 936000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:30:18 - 0:06:06 - 940000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:30:19 - 0:06:07 - 944000 - Discriminator loss: 0.3251 - 5458 samples/s\n",
            "INFO - 04/04/25 15:30:21 - 0:06:09 - 948000 - Discriminator loss: 0.3251 - 5452 samples/s\n",
            "INFO - 04/04/25 15:30:22 - 0:06:10 - 952000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:30:23 - 0:06:12 - 956000 - Discriminator loss: 0.3251 - 5460 samples/s\n",
            "INFO - 04/04/25 15:30:25 - 0:06:13 - 960000 - Discriminator loss: 0.3251 - 5465 samples/s\n",
            "INFO - 04/04/25 15:30:26 - 0:06:14 - 964000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:30:28 - 0:06:16 - 968000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:30:29 - 0:06:17 - 972000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:30:31 - 0:06:19 - 976000 - Discriminator loss: 0.3251 - 5455 samples/s\n",
            "INFO - 04/04/25 15:30:32 - 0:06:20 - 980000 - Discriminator loss: 0.3251 - 5459 samples/s\n",
            "INFO - 04/04/25 15:30:34 - 0:06:22 - 984000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:30:35 - 0:06:23 - 988000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:30:37 - 0:06:25 - 992000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:30:38 - 0:06:26 - 996000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:30:40 - 0:06:28 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 15:30:40 - 0:06:28 - 1500 source words - nn - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 15:30:40 - 0:06:28 - 1500 source words - nn - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 15:30:40 - 0:06:28 - 1500 source words - nn - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 15:30:40 - 0:06:28 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 15:32:57 - 0:08:45 - 1500 source words - csls_knn_10 - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 15:32:57 - 0:08:45 - 1500 source words - csls_knn_10 - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 15:32:57 - 0:08:45 - 1500 source words - csls_knn_10 - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 15:32:58 - 0:08:46 - Building the train dictionary ...\n",
            "INFO - 04/04/25 15:32:58 - 0:08:46 - New train dictionary of 1407 pairs.\n",
            "INFO - 04/04/25 15:32:58 - 0:08:46 - Mean cosine (nn method, S2T build, 10000 max size): 0.22328\n",
            "INFO - 04/04/25 15:33:06 - 0:08:54 - Building the train dictionary ...\n",
            "INFO - 04/04/25 15:33:06 - 0:08:54 - New train dictionary of 1602 pairs.\n",
            "INFO - 04/04/25 15:33:06 - 0:08:54 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.21824\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/evaluation/evaluator.py:232: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.src_emb.weight[i:i + bs].data, volatile=True)\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/evaluation/evaluator.py:237: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.tgt_emb.weight[i:i + bs].data, volatile=True)\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - Discriminator source / target predictions: 0.89969 / 0.09988\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - Discriminator source / target / global accuracy: 1.00000 / 0.99999 / 0.99999\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - __log__:{\"n_epoch\": 0, \"precision_at_1-nn\": 0.0, \"precision_at_5-nn\": 0.0, \"precision_at_10-nn\": 0.0, \"precision_at_1-csls_knn_10\": 0.0, \"precision_at_5-csls_knn_10\": 0.0, \"precision_at_10-csls_knn_10\": 0.0, \"mean_cosine-nn-S2T-10000\": 0.22328391671180725, \"mean_cosine-csls_knn_10-S2T-10000\": 0.2182425558567047, \"dis_accu\": 0.9999944136574902, \"dis_src_pred\": 0.8996894083184004, \"dis_tgt_pred\": 0.09988006196756545}\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - * Best value for \"mean_cosine-csls_knn_10-S2T-10000\": 0.21824\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - * Saving the mapping to /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/szdk2p3tqp/best_mapping.pth ...\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - End of epoch 0.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - Decreasing learning rate: 0.10000000 -> 0.09800000\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - Starting adversarial training epoch 1...\n",
            "INFO - 04/04/25 15:33:07 - 0:08:55 - 000000 - Discriminator loss: 0.3251 - 6093 samples/s\n",
            "INFO - 04/04/25 15:33:09 - 0:08:57 - 004000 - Discriminator loss: 0.3251 - 5482 samples/s\n",
            "INFO - 04/04/25 15:33:10 - 0:08:58 - 008000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:33:11 - 0:09:00 - 012000 - Discriminator loss: 0.3251 - 5424 samples/s\n",
            "INFO - 04/04/25 15:33:13 - 0:09:01 - 016000 - Discriminator loss: 0.3251 - 5418 samples/s\n",
            "INFO - 04/04/25 15:33:14 - 0:09:02 - 020000 - Discriminator loss: 0.3251 - 5420 samples/s\n",
            "INFO - 04/04/25 15:33:16 - 0:09:04 - 024000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:33:17 - 0:09:05 - 028000 - Discriminator loss: 0.3251 - 5420 samples/s\n",
            "INFO - 04/04/25 15:33:19 - 0:09:07 - 032000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:33:20 - 0:09:08 - 036000 - Discriminator loss: 0.3251 - 5419 samples/s\n",
            "INFO - 04/04/25 15:33:22 - 0:09:10 - 040000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:33:23 - 0:09:11 - 044000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:33:25 - 0:09:13 - 048000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:33:26 - 0:09:14 - 052000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:33:28 - 0:09:16 - 056000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:33:29 - 0:09:17 - 060000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:33:31 - 0:09:19 - 064000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:33:32 - 0:09:20 - 068000 - Discriminator loss: 0.3251 - 5415 samples/s\n",
            "INFO - 04/04/25 15:33:34 - 0:09:22 - 072000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:33:35 - 0:09:23 - 076000 - Discriminator loss: 0.3251 - 5418 samples/s\n",
            "INFO - 04/04/25 15:33:37 - 0:09:25 - 080000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:33:38 - 0:09:26 - 084000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:33:39 - 0:09:28 - 088000 - Discriminator loss: 0.3251 - 5427 samples/s\n",
            "INFO - 04/04/25 15:33:41 - 0:09:29 - 092000 - Discriminator loss: 0.3251 - 5425 samples/s\n",
            "INFO - 04/04/25 15:33:42 - 0:09:30 - 096000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:33:44 - 0:09:32 - 100000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:33:45 - 0:09:33 - 104000 - Discriminator loss: 0.3251 - 5456 samples/s\n",
            "INFO - 04/04/25 15:33:47 - 0:09:35 - 108000 - Discriminator loss: 0.3251 - 5406 samples/s\n",
            "INFO - 04/04/25 15:33:48 - 0:09:36 - 112000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:33:50 - 0:09:38 - 116000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:33:51 - 0:09:39 - 120000 - Discriminator loss: 0.3251 - 5420 samples/s\n",
            "INFO - 04/04/25 15:33:53 - 0:09:41 - 124000 - Discriminator loss: 0.3251 - 5447 samples/s\n",
            "INFO - 04/04/25 15:33:54 - 0:09:42 - 128000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:33:56 - 0:09:44 - 132000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:33:57 - 0:09:45 - 136000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:33:59 - 0:09:47 - 140000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:34:00 - 0:09:48 - 144000 - Discriminator loss: 0.3251 - 5412 samples/s\n",
            "INFO - 04/04/25 15:34:02 - 0:09:50 - 148000 - Discriminator loss: 0.3251 - 5423 samples/s\n",
            "INFO - 04/04/25 15:34:03 - 0:09:51 - 152000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:34:04 - 0:09:53 - 156000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:34:06 - 0:09:54 - 160000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:34:07 - 0:09:55 - 164000 - Discriminator loss: 0.3251 - 5423 samples/s\n",
            "INFO - 04/04/25 15:34:09 - 0:09:57 - 168000 - Discriminator loss: 0.3251 - 5429 samples/s\n",
            "INFO - 04/04/25 15:34:10 - 0:09:58 - 172000 - Discriminator loss: 0.3251 - 5424 samples/s\n",
            "INFO - 04/04/25 15:34:12 - 0:10:00 - 176000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:34:13 - 0:10:01 - 180000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:34:15 - 0:10:03 - 184000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:34:16 - 0:10:04 - 188000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:34:18 - 0:10:06 - 192000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:34:19 - 0:10:07 - 196000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:34:21 - 0:10:09 - 200000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:34:22 - 0:10:10 - 204000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:34:24 - 0:10:12 - 208000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:34:25 - 0:10:13 - 212000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:34:27 - 0:10:15 - 216000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:34:28 - 0:10:16 - 220000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:34:29 - 0:10:18 - 224000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:34:31 - 0:10:19 - 228000 - Discriminator loss: 0.3251 - 5429 samples/s\n",
            "INFO - 04/04/25 15:34:32 - 0:10:20 - 232000 - Discriminator loss: 0.3251 - 5424 samples/s\n",
            "INFO - 04/04/25 15:34:34 - 0:10:22 - 236000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:34:35 - 0:10:23 - 240000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:34:37 - 0:10:25 - 244000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:34:38 - 0:10:26 - 248000 - Discriminator loss: 0.3251 - 5416 samples/s\n",
            "INFO - 04/04/25 15:34:40 - 0:10:28 - 252000 - Discriminator loss: 0.3251 - 5406 samples/s\n",
            "INFO - 04/04/25 15:34:41 - 0:10:29 - 256000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:34:43 - 0:10:31 - 260000 - Discriminator loss: 0.3251 - 5427 samples/s\n",
            "INFO - 04/04/25 15:34:44 - 0:10:32 - 264000 - Discriminator loss: 0.3251 - 5421 samples/s\n",
            "INFO - 04/04/25 15:34:46 - 0:10:34 - 268000 - Discriminator loss: 0.3251 - 5418 samples/s\n",
            "INFO - 04/04/25 15:34:47 - 0:10:35 - 272000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:34:49 - 0:10:37 - 276000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:34:50 - 0:10:38 - 280000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:34:52 - 0:10:40 - 284000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:34:53 - 0:10:41 - 288000 - Discriminator loss: 0.3251 - 5451 samples/s\n",
            "INFO - 04/04/25 15:34:55 - 0:10:43 - 292000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:34:56 - 0:10:44 - 296000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:34:57 - 0:10:46 - 300000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:34:59 - 0:10:47 - 304000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:35:00 - 0:10:48 - 308000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:35:02 - 0:10:50 - 312000 - Discriminator loss: 0.3251 - 5429 samples/s\n",
            "INFO - 04/04/25 15:35:03 - 0:10:51 - 316000 - Discriminator loss: 0.3251 - 5415 samples/s\n",
            "INFO - 04/04/25 15:35:05 - 0:10:53 - 320000 - Discriminator loss: 0.3251 - 5421 samples/s\n",
            "INFO - 04/04/25 15:35:06 - 0:10:54 - 324000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:35:08 - 0:10:56 - 328000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:35:09 - 0:10:57 - 332000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:35:11 - 0:10:59 - 336000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:35:12 - 0:11:00 - 340000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:35:14 - 0:11:02 - 344000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:35:15 - 0:11:03 - 348000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:35:17 - 0:11:05 - 352000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:35:18 - 0:11:06 - 356000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:35:20 - 0:11:08 - 360000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:35:21 - 0:11:09 - 364000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:35:23 - 0:11:11 - 368000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:35:24 - 0:11:12 - 372000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:35:25 - 0:11:13 - 376000 - Discriminator loss: 0.3251 - 5453 samples/s\n",
            "INFO - 04/04/25 15:35:27 - 0:11:15 - 380000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:35:28 - 0:11:16 - 384000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:35:30 - 0:11:18 - 388000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:35:31 - 0:11:19 - 392000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:35:33 - 0:11:21 - 396000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:35:34 - 0:11:22 - 400000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:35:36 - 0:11:24 - 404000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:35:37 - 0:11:25 - 408000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:35:39 - 0:11:27 - 412000 - Discriminator loss: 0.3251 - 5421 samples/s\n",
            "INFO - 04/04/25 15:35:40 - 0:11:28 - 416000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:35:42 - 0:11:30 - 420000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:35:43 - 0:11:31 - 424000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:35:45 - 0:11:33 - 428000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:35:46 - 0:11:34 - 432000 - Discriminator loss: 0.3251 - 5406 samples/s\n",
            "INFO - 04/04/25 15:35:48 - 0:11:36 - 436000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:35:49 - 0:11:37 - 440000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:35:50 - 0:11:39 - 444000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:35:52 - 0:11:40 - 448000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:35:53 - 0:11:41 - 452000 - Discriminator loss: 0.3251 - 5416 samples/s\n",
            "INFO - 04/04/25 15:35:55 - 0:11:43 - 456000 - Discriminator loss: 0.3251 - 5425 samples/s\n",
            "INFO - 04/04/25 15:35:56 - 0:11:44 - 460000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:35:58 - 0:11:46 - 464000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:35:59 - 0:11:47 - 468000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:36:01 - 0:11:49 - 472000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:02 - 0:11:50 - 476000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:36:04 - 0:11:52 - 480000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:36:05 - 0:11:53 - 484000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:36:07 - 0:11:55 - 488000 - Discriminator loss: 0.3251 - 5426 samples/s\n",
            "INFO - 04/04/25 15:36:08 - 0:11:56 - 492000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:36:10 - 0:11:58 - 496000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:36:11 - 0:11:59 - 500000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:36:13 - 0:12:01 - 504000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:36:14 - 0:12:02 - 508000 - Discriminator loss: 0.3251 - 5452 samples/s\n",
            "INFO - 04/04/25 15:36:15 - 0:12:04 - 512000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:36:17 - 0:12:05 - 516000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:36:18 - 0:12:06 - 520000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:36:20 - 0:12:08 - 524000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:21 - 0:12:09 - 528000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:36:23 - 0:12:11 - 532000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:36:24 - 0:12:12 - 536000 - Discriminator loss: 0.3251 - 5447 samples/s\n",
            "INFO - 04/04/25 15:36:26 - 0:12:14 - 540000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:36:27 - 0:12:15 - 544000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:36:29 - 0:12:17 - 548000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:36:30 - 0:12:18 - 552000 - Discriminator loss: 0.3251 - 5427 samples/s\n",
            "INFO - 04/04/25 15:36:32 - 0:12:20 - 556000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:36:33 - 0:12:21 - 560000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:36:35 - 0:12:23 - 564000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:36:36 - 0:12:24 - 568000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:38 - 0:12:26 - 572000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:36:39 - 0:12:27 - 576000 - Discriminator loss: 0.3251 - 5479 samples/s\n",
            "INFO - 04/04/25 15:36:40 - 0:12:29 - 580000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:42 - 0:12:30 - 584000 - Discriminator loss: 0.3251 - 5421 samples/s\n",
            "INFO - 04/04/25 15:36:43 - 0:12:31 - 588000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:36:45 - 0:12:33 - 592000 - Discriminator loss: 0.3251 - 5449 samples/s\n",
            "INFO - 04/04/25 15:36:46 - 0:12:34 - 596000 - Discriminator loss: 0.3251 - 5425 samples/s\n",
            "INFO - 04/04/25 15:36:48 - 0:12:36 - 600000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:36:49 - 0:12:37 - 604000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:51 - 0:12:39 - 608000 - Discriminator loss: 0.3251 - 5485 samples/s\n",
            "INFO - 04/04/25 15:36:52 - 0:12:40 - 612000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:36:54 - 0:12:42 - 616000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:36:55 - 0:12:43 - 620000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:36:57 - 0:12:45 - 624000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:36:58 - 0:12:46 - 628000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:37:00 - 0:12:48 - 632000 - Discriminator loss: 0.3251 - 5415 samples/s\n",
            "INFO - 04/04/25 15:37:01 - 0:12:49 - 636000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:37:03 - 0:12:51 - 640000 - Discriminator loss: 0.3251 - 5417 samples/s\n",
            "INFO - 04/04/25 15:37:04 - 0:12:52 - 644000 - Discriminator loss: 0.3251 - 5414 samples/s\n",
            "INFO - 04/04/25 15:37:05 - 0:12:54 - 648000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:37:07 - 0:12:55 - 652000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:37:08 - 0:12:56 - 656000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:37:10 - 0:12:58 - 660000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:37:11 - 0:12:59 - 664000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:37:13 - 0:13:01 - 668000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:37:14 - 0:13:02 - 672000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:37:16 - 0:13:04 - 676000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:37:17 - 0:13:05 - 680000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:37:19 - 0:13:07 - 684000 - Discriminator loss: 0.3251 - 5427 samples/s\n",
            "INFO - 04/04/25 15:37:20 - 0:13:08 - 688000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:37:22 - 0:13:10 - 692000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:37:23 - 0:13:11 - 696000 - Discriminator loss: 0.3251 - 5434 samples/s\n",
            "INFO - 04/04/25 15:37:25 - 0:13:13 - 700000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:37:26 - 0:13:14 - 704000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:37:28 - 0:13:16 - 708000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:37:29 - 0:13:17 - 712000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:37:30 - 0:13:19 - 716000 - Discriminator loss: 0.3251 - 5454 samples/s\n",
            "INFO - 04/04/25 15:37:32 - 0:13:20 - 720000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:37:33 - 0:13:21 - 724000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:37:35 - 0:13:23 - 728000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:37:36 - 0:13:24 - 732000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:37:38 - 0:13:26 - 736000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:37:39 - 0:13:27 - 740000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:37:41 - 0:13:29 - 744000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:37:42 - 0:13:30 - 748000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:37:44 - 0:13:32 - 752000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:37:45 - 0:13:33 - 756000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:37:47 - 0:13:35 - 760000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:37:48 - 0:13:36 - 764000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:37:50 - 0:13:38 - 768000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:37:51 - 0:13:39 - 772000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:37:53 - 0:13:41 - 776000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:37:54 - 0:13:42 - 780000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:37:56 - 0:13:44 - 784000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:37:57 - 0:13:45 - 788000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:37:58 - 0:13:47 - 792000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:38:00 - 0:13:48 - 796000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:38:01 - 0:13:49 - 800000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:38:03 - 0:13:51 - 804000 - Discriminator loss: 0.3251 - 5429 samples/s\n",
            "INFO - 04/04/25 15:38:04 - 0:13:52 - 808000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:38:06 - 0:13:54 - 812000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:38:07 - 0:13:55 - 816000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:38:09 - 0:13:57 - 820000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:38:10 - 0:13:58 - 824000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:38:12 - 0:14:00 - 828000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:38:13 - 0:14:01 - 832000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:38:15 - 0:14:03 - 836000 - Discriminator loss: 0.3251 - 5444 samples/s\n",
            "INFO - 04/04/25 15:38:16 - 0:14:04 - 840000 - Discriminator loss: 0.3251 - 5426 samples/s\n",
            "INFO - 04/04/25 15:38:18 - 0:14:06 - 844000 - Discriminator loss: 0.3251 - 5439 samples/s\n",
            "INFO - 04/04/25 15:38:19 - 0:14:07 - 848000 - Discriminator loss: 0.3251 - 5424 samples/s\n",
            "INFO - 04/04/25 15:38:21 - 0:14:09 - 852000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:38:22 - 0:14:10 - 856000 - Discriminator loss: 0.3251 - 5432 samples/s\n",
            "INFO - 04/04/25 15:38:23 - 0:14:12 - 860000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:38:25 - 0:14:13 - 864000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:38:26 - 0:14:14 - 868000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:38:28 - 0:14:16 - 872000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:38:29 - 0:14:17 - 876000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:38:31 - 0:14:19 - 880000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:38:32 - 0:14:20 - 884000 - Discriminator loss: 0.3251 - 5416 samples/s\n",
            "INFO - 04/04/25 15:38:34 - 0:14:22 - 888000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:38:35 - 0:14:23 - 892000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:38:37 - 0:14:25 - 896000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:38:38 - 0:14:26 - 900000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:38:40 - 0:14:28 - 904000 - Discriminator loss: 0.3251 - 5421 samples/s\n",
            "INFO - 04/04/25 15:38:41 - 0:14:29 - 908000 - Discriminator loss: 0.3251 - 5422 samples/s\n",
            "INFO - 04/04/25 15:38:43 - 0:14:31 - 912000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:38:44 - 0:14:32 - 916000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:38:46 - 0:14:34 - 920000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:38:47 - 0:14:35 - 924000 - Discriminator loss: 0.3251 - 5438 samples/s\n",
            "INFO - 04/04/25 15:38:49 - 0:14:37 - 928000 - Discriminator loss: 0.3251 - 5420 samples/s\n",
            "INFO - 04/04/25 15:38:50 - 0:14:38 - 932000 - Discriminator loss: 0.3251 - 5442 samples/s\n",
            "INFO - 04/04/25 15:38:51 - 0:14:40 - 936000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:38:53 - 0:14:41 - 940000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:38:54 - 0:14:42 - 944000 - Discriminator loss: 0.3251 - 5433 samples/s\n",
            "INFO - 04/04/25 15:38:56 - 0:14:44 - 948000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:38:57 - 0:14:45 - 952000 - Discriminator loss: 0.3251 - 5446 samples/s\n",
            "INFO - 04/04/25 15:38:59 - 0:14:47 - 956000 - Discriminator loss: 0.3251 - 5430 samples/s\n",
            "INFO - 04/04/25 15:39:00 - 0:14:48 - 960000 - Discriminator loss: 0.3251 - 5445 samples/s\n",
            "INFO - 04/04/25 15:39:02 - 0:14:50 - 964000 - Discriminator loss: 0.3251 - 5431 samples/s\n",
            "INFO - 04/04/25 15:39:03 - 0:14:51 - 968000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:39:05 - 0:14:53 - 972000 - Discriminator loss: 0.3251 - 5428 samples/s\n",
            "INFO - 04/04/25 15:39:06 - 0:14:54 - 976000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:39:08 - 0:14:56 - 980000 - Discriminator loss: 0.3251 - 5441 samples/s\n",
            "INFO - 04/04/25 15:39:09 - 0:14:57 - 984000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:39:11 - 0:14:59 - 988000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "INFO - 04/04/25 15:39:12 - 0:15:00 - 992000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:39:14 - 0:15:02 - 996000 - Discriminator loss: 0.3251 - 5440 samples/s\n",
            "INFO - 04/04/25 15:39:15 - 0:15:03 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 15:39:15 - 0:15:03 - 1500 source words - nn - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 15:39:15 - 0:15:03 - 1500 source words - nn - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 15:39:15 - 0:15:03 - 1500 source words - nn - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 15:39:15 - 0:15:03 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 15:39:19 - 0:15:07 - 1500 source words - csls_knn_10 - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 15:39:19 - 0:15:07 - 1500 source words - csls_knn_10 - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 15:39:19 - 0:15:07 - 1500 source words - csls_knn_10 - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 15:39:20 - 0:15:08 - Building the train dictionary ...\n",
            "INFO - 04/04/25 15:39:20 - 0:15:08 - New train dictionary of 1407 pairs.\n",
            "INFO - 04/04/25 15:39:20 - 0:15:08 - Mean cosine (nn method, S2T build, 10000 max size): 0.22328\n",
            "INFO - 04/04/25 15:39:28 - 0:15:16 - Building the train dictionary ...\n",
            "INFO - 04/04/25 15:39:28 - 0:15:16 - New train dictionary of 1602 pairs.\n",
            "INFO - 04/04/25 15:39:28 - 0:15:16 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.21824\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - Discriminator source / target predictions: 0.89954 / 0.09988\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - Discriminator source / target / global accuracy: 1.00000 / 0.99999 / 0.99999\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - __log__:{\"n_epoch\": 1, \"precision_at_1-nn\": 0.0, \"precision_at_5-nn\": 0.0, \"precision_at_10-nn\": 0.0, \"precision_at_1-csls_knn_10\": 0.0, \"precision_at_5-csls_knn_10\": 0.0, \"precision_at_10-csls_knn_10\": 0.0, \"mean_cosine-nn-S2T-10000\": 0.22328391671180725, \"mean_cosine-csls_knn_10-S2T-10000\": 0.2182425558567047, \"dis_accu\": 0.9999944136574902, \"dis_src_pred\": 0.8995435695624352, \"dis_tgt_pred\": 0.09988279191699971}\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - End of epoch 1.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - Decreasing learning rate: 0.09800000 -> 0.09604000\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - Starting adversarial training epoch 2...\n",
            "INFO - 04/04/25 15:39:29 - 0:15:17 - 000000 - Discriminator loss: 0.3251 - 5375 samples/s\n",
            "INFO - 04/04/25 15:39:31 - 0:15:19 - 004000 - Discriminator loss: 0.3251 - 5400 samples/s\n",
            "INFO - 04/04/25 15:39:32 - 0:15:20 - 008000 - Discriminator loss: 0.3251 - 5318 samples/s\n",
            "INFO - 04/04/25 15:39:34 - 0:15:22 - 012000 - Discriminator loss: 0.3251 - 5335 samples/s\n",
            "INFO - 04/04/25 15:39:35 - 0:15:23 - 016000 - Discriminator loss: 0.3251 - 5341 samples/s\n",
            "INFO - 04/04/25 15:39:37 - 0:15:25 - 020000 - Discriminator loss: 0.3251 - 5350 samples/s\n",
            "INFO - 04/04/25 15:39:38 - 0:15:26 - 024000 - Discriminator loss: 0.3251 - 5347 samples/s\n",
            "INFO - 04/04/25 15:39:40 - 0:15:28 - 028000 - Discriminator loss: 0.3251 - 5357 samples/s\n",
            "INFO - 04/04/25 15:39:41 - 0:15:29 - 032000 - Discriminator loss: 0.3251 - 5333 samples/s\n",
            "INFO - 04/04/25 15:39:43 - 0:15:31 - 036000 - Discriminator loss: 0.3251 - 5369 samples/s\n",
            "INFO - 04/04/25 15:39:44 - 0:15:32 - 040000 - Discriminator loss: 0.3251 - 5362 samples/s\n",
            "INFO - 04/04/25 15:39:45 - 0:15:34 - 044000 - Discriminator loss: 0.3251 - 5361 samples/s\n",
            "INFO - 04/04/25 15:39:47 - 0:15:35 - 048000 - Discriminator loss: 0.3251 - 5333 samples/s\n",
            "INFO - 04/04/25 15:39:48 - 0:15:37 - 052000 - Discriminator loss: 0.3251 - 5359 samples/s\n",
            "INFO - 04/04/25 15:39:50 - 0:15:38 - 056000 - Discriminator loss: 0.3251 - 5346 samples/s\n",
            "INFO - 04/04/25 15:39:51 - 0:15:40 - 060000 - Discriminator loss: 0.3251 - 5361 samples/s\n",
            "INFO - 04/04/25 15:39:53 - 0:15:41 - 064000 - Discriminator loss: 0.3251 - 5356 samples/s\n",
            "INFO - 04/04/25 15:39:54 - 0:15:43 - 068000 - Discriminator loss: 0.3251 - 5370 samples/s\n",
            "INFO - 04/04/25 15:39:56 - 0:15:44 - 072000 - Discriminator loss: 0.3251 - 5374 samples/s\n",
            "INFO - 04/04/25 15:39:57 - 0:15:45 - 076000 - Discriminator loss: 0.3251 - 5378 samples/s\n",
            "INFO - 04/04/25 15:39:59 - 0:15:47 - 080000 - Discriminator loss: 0.3251 - 5367 samples/s\n",
            "INFO - 04/04/25 15:40:00 - 0:15:48 - 084000 - Discriminator loss: 0.3251 - 5366 samples/s\n",
            "INFO - 04/04/25 15:40:02 - 0:15:50 - 088000 - Discriminator loss: 0.3251 - 5350 samples/s\n",
            "INFO - 04/04/25 15:40:03 - 0:15:51 - 092000 - Discriminator loss: 0.3251 - 5373 samples/s\n",
            "INFO - 04/04/25 15:40:05 - 0:15:53 - 096000 - Discriminator loss: 0.3251 - 5346 samples/s\n",
            "INFO - 04/04/25 15:40:06 - 0:15:54 - 100000 - Discriminator loss: 0.3251 - 5334 samples/s\n",
            "INFO - 04/04/25 15:40:08 - 0:15:56 - 104000 - Discriminator loss: 0.3251 - 5346 samples/s\n",
            "INFO - 04/04/25 15:40:09 - 0:15:57 - 108000 - Discriminator loss: 0.3251 - 5348 samples/s\n",
            "INFO - 04/04/25 15:40:11 - 0:15:59 - 112000 - Discriminator loss: 0.3251 - 5354 samples/s\n",
            "INFO - 04/04/25 15:40:12 - 0:16:00 - 116000 - Discriminator loss: 0.3251 - 5352 samples/s\n",
            "INFO - 04/04/25 15:40:14 - 0:16:02 - 120000 - Discriminator loss: 0.3251 - 5346 samples/s\n",
            "INFO - 04/04/25 15:40:15 - 0:16:03 - 124000 - Discriminator loss: 0.3251 - 5343 samples/s\n",
            "INFO - 04/04/25 15:40:17 - 0:16:05 - 128000 - Discriminator loss: 0.3251 - 5349 samples/s\n",
            "INFO - 04/04/25 15:40:18 - 0:16:06 - 132000 - Discriminator loss: 0.3251 - 5362 samples/s\n",
            "INFO - 04/04/25 15:40:20 - 0:16:08 - 136000 - Discriminator loss: 0.3251 - 5436 samples/s\n",
            "INFO - 04/04/25 15:40:21 - 0:16:09 - 140000 - Discriminator loss: 0.3251 - 5450 samples/s\n",
            "INFO - 04/04/25 15:40:23 - 0:16:11 - 144000 - Discriminator loss: 0.3251 - 5437 samples/s\n",
            "INFO - 04/04/25 15:40:24 - 0:16:12 - 148000 - Discriminator loss: 0.3251 - 5447 samples/s\n",
            "INFO - 04/04/25 15:40:26 - 0:16:14 - 152000 - Discriminator loss: 0.3251 - 5447 samples/s\n",
            "INFO - 04/04/25 15:40:27 - 0:16:15 - 156000 - Discriminator loss: 0.3251 - 5448 samples/s\n",
            "INFO - 04/04/25 15:40:29 - 0:16:17 - 160000 - Discriminator loss: 0.3251 - 5443 samples/s\n",
            "INFO - 04/04/25 15:40:30 - 0:16:18 - 164000 - Discriminator loss: 0.3251 - 5435 samples/s\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/lipsync/LatentSync/expssssss/MUSE/unsupervised.py\", line 121, in <module>\n",
            "    n_words_proc += trainer.mapping_step(stats)\n",
            "  File \"/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py\", line 123, in mapping_step\n",
            "    if (loss != loss).data.any():\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python unsupervised.py --src_lang en --tgt_lang hi --src_emb /data/wiki.en.vec --tgt_emb /data/wiki.hi.vec --n_refinement 5 --export \"pth\" --dico_eval /data/en-hi.5000-6500.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcdf836-4c84-4cc1-bf62-0b189cd736cc",
      "metadata": {
        "id": "5bcdf836-4c84-4cc1-bf62-0b189cd736cc",
        "outputId": "95c299a9-be6a-4dff-be3d-59877e572015",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO - 04/04/25 16:14:09 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 04/04/25 16:14:09 - 0:00:00 - adversarial: True\n",
            "                                     batch_size: 32\n",
            "                                     cuda: True\n",
            "                                     dico_build: S2T|T2S\n",
            "                                     dico_eval: /workspace/lipsync/LatentSync/expssssss/en-hi.5000-6500.txt\n",
            "                                     dico_max_rank: 15000\n",
            "                                     dico_max_size: 5000\n",
            "                                     dico_method: csls_knn_10\n",
            "                                     dico_min_size: 500\n",
            "                                     dico_threshold: 0\n",
            "                                     dis_clip_weights: 0\n",
            "                                     dis_dropout: 0.0\n",
            "                                     dis_hid_dim: 1024\n",
            "                                     dis_input_dropout: 0.2\n",
            "                                     dis_lambda: 1\n",
            "                                     dis_layers: 3\n",
            "                                     dis_most_frequent: 75000\n",
            "                                     dis_optimizer: sgd,lr=0.1\n",
            "                                     dis_smooth: 0.1\n",
            "                                     dis_steps: 10\n",
            "                                     emb_dim: 300\n",
            "                                     epoch_size: 1000000\n",
            "                                     exp_id: \n",
            "                                     exp_name: debug\n",
            "                                     exp_path: /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/skznf4jakj\n",
            "                                     export: pth\n",
            "                                     lr_decay: 0.98\n",
            "                                     lr_shrink: 0.5\n",
            "                                     map_beta: 0.001\n",
            "                                     map_id_init: False\n",
            "                                     map_optimizer: sgd,lr=0.1\n",
            "                                     max_vocab: 200000\n",
            "                                     min_lr: 1e-06\n",
            "                                     n_epochs: 5\n",
            "                                     n_refinement: 10\n",
            "                                     normalize_embeddings: center\n",
            "                                     seed: -1\n",
            "                                     src_emb: data/wiki.en.vec\n",
            "                                     src_lang: en\n",
            "                                     tgt_emb: data/wiki.hi.vec\n",
            "                                     tgt_lang: hi\n",
            "                                     verbose: 2\n",
            "INFO - 04/04/25 16:14:09 - 0:00:00 - The experiment will be stored in /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/skznf4jakj\n",
            "INFO - 04/04/25 16:14:18 - 0:00:09 - Loaded 200000 pre-trained word embeddings.\n",
            "INFO - 04/04/25 16:14:27 - 0:00:18 - Loaded 158016 pre-trained word embeddings.\n",
            "INFO - 04/04/25 16:14:28 - 0:00:19 - ----> ADVERSARIAL TRAINING <----\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 16:14:28 - 0:00:19 - Starting adversarial training epoch 0...\n",
            "INFO - 04/04/25 16:14:28 - 0:00:19 - 000000 - Discriminator loss: 0.6935 - 602 samples/s\n",
            "INFO - 04/04/25 16:14:31 - 0:00:22 - 004000 - Discriminator loss: 0.4193 - 2551 samples/s\n",
            "INFO - 04/04/25 16:14:34 - 0:00:25 - 008000 - Discriminator loss: 0.3403 - 2600 samples/s\n",
            "INFO - 04/04/25 16:14:37 - 0:00:28 - 012000 - Discriminator loss: 0.3356 - 2617 samples/s\n",
            "INFO - 04/04/25 16:14:40 - 0:00:31 - 016000 - Discriminator loss: 0.3335 - 2611 samples/s\n",
            "INFO - 04/04/25 16:14:43 - 0:00:34 - 020000 - Discriminator loss: 0.3323 - 2621 samples/s\n",
            "INFO - 04/04/25 16:14:46 - 0:00:37 - 024000 - Discriminator loss: 0.3315 - 2620 samples/s\n",
            "INFO - 04/04/25 16:14:49 - 0:00:40 - 028000 - Discriminator loss: 0.3310 - 2616 samples/s\n",
            "INFO - 04/04/25 16:14:53 - 0:00:43 - 032000 - Discriminator loss: 0.3306 - 2620 samples/s\n",
            "INFO - 04/04/25 16:14:56 - 0:00:46 - 036000 - Discriminator loss: 0.3303 - 2612 samples/s\n",
            "INFO - 04/04/25 16:14:59 - 0:00:50 - 040000 - Discriminator loss: 0.3299 - 2625 samples/s\n",
            "INFO - 04/04/25 16:15:02 - 0:00:53 - 044000 - Discriminator loss: 0.3296 - 2616 samples/s\n",
            "INFO - 04/04/25 16:15:05 - 0:00:56 - 048000 - Discriminator loss: 0.3294 - 2620 samples/s\n",
            "INFO - 04/04/25 16:15:08 - 0:00:59 - 052000 - Discriminator loss: 0.3291 - 2623 samples/s\n",
            "INFO - 04/04/25 16:15:11 - 0:01:02 - 056000 - Discriminator loss: 0.3287 - 2624 samples/s\n",
            "INFO - 04/04/25 16:15:14 - 0:01:05 - 060000 - Discriminator loss: 0.3285 - 2625 samples/s\n",
            "INFO - 04/04/25 16:15:17 - 0:01:08 - 064000 - Discriminator loss: 0.3282 - 2626 samples/s\n",
            "INFO - 04/04/25 16:15:20 - 0:01:11 - 068000 - Discriminator loss: 0.3280 - 2617 samples/s\n",
            "INFO - 04/04/25 16:15:23 - 0:01:14 - 072000 - Discriminator loss: 0.3278 - 2623 samples/s\n",
            "INFO - 04/04/25 16:15:26 - 0:01:17 - 076000 - Discriminator loss: 0.3276 - 2622 samples/s\n",
            "INFO - 04/04/25 16:15:29 - 0:01:20 - 080000 - Discriminator loss: 0.3273 - 2624 samples/s\n",
            "INFO - 04/04/25 16:15:32 - 0:01:23 - 084000 - Discriminator loss: 0.3270 - 2625 samples/s\n",
            "INFO - 04/04/25 16:15:35 - 0:01:26 - 088000 - Discriminator loss: 0.3269 - 2620 samples/s\n",
            "INFO - 04/04/25 16:15:38 - 0:01:29 - 092000 - Discriminator loss: 0.3268 - 2625 samples/s\n",
            "INFO - 04/04/25 16:15:41 - 0:01:32 - 096000 - Discriminator loss: 0.3267 - 2625 samples/s\n",
            "INFO - 04/04/25 16:15:44 - 0:01:35 - 100000 - Discriminator loss: 0.3265 - 2627 samples/s\n",
            "INFO - 04/04/25 16:15:47 - 0:01:38 - 104000 - Discriminator loss: 0.3264 - 2617 samples/s\n",
            "INFO - 04/04/25 16:15:51 - 0:01:41 - 108000 - Discriminator loss: 0.3263 - 2621 samples/s\n",
            "INFO - 04/04/25 16:15:54 - 0:01:44 - 112000 - Discriminator loss: 0.3261 - 2624 samples/s\n",
            "INFO - 04/04/25 16:15:57 - 0:01:47 - 116000 - Discriminator loss: 0.3260 - 2619 samples/s\n",
            "INFO - 04/04/25 16:16:00 - 0:01:51 - 120000 - Discriminator loss: 0.3259 - 2617 samples/s\n",
            "INFO - 04/04/25 16:16:03 - 0:01:54 - 124000 - Discriminator loss: 0.3258 - 2626 samples/s\n",
            "INFO - 04/04/25 16:16:06 - 0:01:57 - 128000 - Discriminator loss: 0.3259 - 2624 samples/s\n",
            "INFO - 04/04/25 16:16:09 - 0:02:00 - 132000 - Discriminator loss: 0.3257 - 2624 samples/s\n",
            "INFO - 04/04/25 16:16:12 - 0:02:03 - 136000 - Discriminator loss: 0.3257 - 2628 samples/s\n",
            "INFO - 04/04/25 16:16:15 - 0:02:06 - 140000 - Discriminator loss: 0.3257 - 2621 samples/s\n",
            "INFO - 04/04/25 16:16:18 - 0:02:09 - 144000 - Discriminator loss: 0.3256 - 2625 samples/s\n",
            "INFO - 04/04/25 16:16:21 - 0:02:12 - 148000 - Discriminator loss: 0.3255 - 2624 samples/s\n",
            "INFO - 04/04/25 16:16:24 - 0:02:15 - 152000 - Discriminator loss: 0.3255 - 2621 samples/s\n",
            "INFO - 04/04/25 16:16:27 - 0:02:18 - 156000 - Discriminator loss: 0.3255 - 2627 samples/s\n",
            "INFO - 04/04/25 16:16:30 - 0:02:21 - 160000 - Discriminator loss: 0.3255 - 2630 samples/s\n",
            "INFO - 04/04/25 16:16:33 - 0:02:24 - 164000 - Discriminator loss: 0.3254 - 2617 samples/s\n",
            "INFO - 04/04/25 16:16:36 - 0:02:27 - 168000 - Discriminator loss: 0.3254 - 2612 samples/s\n",
            "INFO - 04/04/25 16:16:39 - 0:02:30 - 172000 - Discriminator loss: 0.3254 - 2611 samples/s\n",
            "INFO - 04/04/25 16:16:42 - 0:02:33 - 176000 - Discriminator loss: 0.3254 - 2610 samples/s\n",
            "INFO - 04/04/25 16:16:45 - 0:02:36 - 180000 - Discriminator loss: 0.3254 - 2614 samples/s\n",
            "INFO - 04/04/25 16:16:49 - 0:02:39 - 184000 - Discriminator loss: 0.3253 - 2610 samples/s\n",
            "INFO - 04/04/25 16:16:52 - 0:02:42 - 188000 - Discriminator loss: 0.3253 - 2616 samples/s\n",
            "INFO - 04/04/25 16:16:55 - 0:02:46 - 192000 - Discriminator loss: 0.3253 - 2615 samples/s\n",
            "INFO - 04/04/25 16:16:58 - 0:02:49 - 196000 - Discriminator loss: 0.3253 - 2615 samples/s\n",
            "INFO - 04/04/25 16:17:01 - 0:02:52 - 200000 - Discriminator loss: 0.3253 - 2618 samples/s\n",
            "INFO - 04/04/25 16:17:04 - 0:02:55 - 204000 - Discriminator loss: 0.3253 - 2607 samples/s\n",
            "INFO - 04/04/25 16:17:07 - 0:02:58 - 208000 - Discriminator loss: 0.3253 - 2612 samples/s\n",
            "INFO - 04/04/25 16:17:10 - 0:03:01 - 212000 - Discriminator loss: 0.3253 - 2610 samples/s\n",
            "INFO - 04/04/25 16:17:13 - 0:03:04 - 216000 - Discriminator loss: 0.3253 - 2608 samples/s\n",
            "INFO - 04/04/25 16:17:16 - 0:03:07 - 220000 - Discriminator loss: 0.3253 - 2610 samples/s\n",
            "INFO - 04/04/25 16:17:19 - 0:03:10 - 224000 - Discriminator loss: 0.3253 - 2611 samples/s\n",
            "INFO - 04/04/25 16:17:22 - 0:03:13 - 228000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:17:25 - 0:03:16 - 232000 - Discriminator loss: 0.3253 - 2614 samples/s\n",
            "INFO - 04/04/25 16:17:28 - 0:03:19 - 236000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:17:31 - 0:03:22 - 240000 - Discriminator loss: 0.3253 - 2613 samples/s\n",
            "INFO - 04/04/25 16:17:34 - 0:03:25 - 244000 - Discriminator loss: 0.3252 - 2610 samples/s\n",
            "INFO - 04/04/25 16:17:38 - 0:03:28 - 248000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:17:41 - 0:03:31 - 252000 - Discriminator loss: 0.3252 - 2616 samples/s\n",
            "INFO - 04/04/25 16:17:44 - 0:03:35 - 256000 - Discriminator loss: 0.3252 - 2613 samples/s\n",
            "INFO - 04/04/25 16:17:47 - 0:03:38 - 260000 - Discriminator loss: 0.3252 - 2609 samples/s\n",
            "INFO - 04/04/25 16:17:50 - 0:03:41 - 264000 - Discriminator loss: 0.3252 - 2610 samples/s\n",
            "INFO - 04/04/25 16:17:53 - 0:03:44 - 268000 - Discriminator loss: 0.3252 - 2613 samples/s\n",
            "INFO - 04/04/25 16:17:56 - 0:03:47 - 272000 - Discriminator loss: 0.3252 - 2605 samples/s\n",
            "INFO - 04/04/25 16:17:59 - 0:03:50 - 276000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:18:02 - 0:03:53 - 280000 - Discriminator loss: 0.3252 - 2607 samples/s\n",
            "INFO - 04/04/25 16:18:05 - 0:03:56 - 284000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:18:08 - 0:03:59 - 288000 - Discriminator loss: 0.3252 - 2615 samples/s\n",
            "INFO - 04/04/25 16:18:11 - 0:04:02 - 292000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:18:14 - 0:04:05 - 296000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:18:17 - 0:04:08 - 300000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:18:20 - 0:04:11 - 304000 - Discriminator loss: 0.3252 - 2613 samples/s\n",
            "INFO - 04/04/25 16:18:23 - 0:04:14 - 308000 - Discriminator loss: 0.3253 - 2615 samples/s\n",
            "INFO - 04/04/25 16:18:26 - 0:04:17 - 312000 - Discriminator loss: 0.3252 - 2617 samples/s\n",
            "INFO - 04/04/25 16:18:30 - 0:04:20 - 316000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:18:33 - 0:04:24 - 320000 - Discriminator loss: 0.3252 - 2607 samples/s\n",
            "INFO - 04/04/25 16:18:36 - 0:04:27 - 324000 - Discriminator loss: 0.3252 - 2608 samples/s\n",
            "INFO - 04/04/25 16:18:39 - 0:04:30 - 328000 - Discriminator loss: 0.3252 - 2618 samples/s\n",
            "INFO - 04/04/25 16:18:42 - 0:04:33 - 332000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:18:45 - 0:04:36 - 336000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:18:48 - 0:04:39 - 340000 - Discriminator loss: 0.3252 - 2607 samples/s\n",
            "INFO - 04/04/25 16:18:51 - 0:04:42 - 344000 - Discriminator loss: 0.3252 - 2605 samples/s\n",
            "INFO - 04/04/25 16:18:54 - 0:04:45 - 348000 - Discriminator loss: 0.3252 - 2616 samples/s\n",
            "INFO - 04/04/25 16:18:57 - 0:04:48 - 352000 - Discriminator loss: 0.3252 - 2608 samples/s\n",
            "INFO - 04/04/25 16:19:00 - 0:04:51 - 356000 - Discriminator loss: 0.3252 - 2608 samples/s\n",
            "INFO - 04/04/25 16:19:03 - 0:04:54 - 360000 - Discriminator loss: 0.3252 - 2609 samples/s\n",
            "INFO - 04/04/25 16:19:06 - 0:04:57 - 364000 - Discriminator loss: 0.3252 - 2610 samples/s\n",
            "INFO - 04/04/25 16:19:09 - 0:05:00 - 368000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:19:12 - 0:05:03 - 372000 - Discriminator loss: 0.3252 - 2615 samples/s\n",
            "INFO - 04/04/25 16:19:16 - 0:05:06 - 376000 - Discriminator loss: 0.3252 - 2604 samples/s\n",
            "INFO - 04/04/25 16:19:19 - 0:05:09 - 380000 - Discriminator loss: 0.3252 - 2610 samples/s\n",
            "INFO - 04/04/25 16:19:22 - 0:05:13 - 384000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:19:25 - 0:05:16 - 388000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:19:28 - 0:05:19 - 392000 - Discriminator loss: 0.3252 - 2616 samples/s\n",
            "INFO - 04/04/25 16:19:31 - 0:05:22 - 396000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:19:34 - 0:05:25 - 400000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:19:37 - 0:05:28 - 404000 - Discriminator loss: 0.3252 - 2618 samples/s\n",
            "INFO - 04/04/25 16:19:40 - 0:05:31 - 408000 - Discriminator loss: 0.3251 - 2619 samples/s\n",
            "INFO - 04/04/25 16:19:43 - 0:05:34 - 412000 - Discriminator loss: 0.3252 - 2616 samples/s\n",
            "INFO - 04/04/25 16:19:46 - 0:05:37 - 416000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:19:49 - 0:05:40 - 420000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:19:52 - 0:05:43 - 424000 - Discriminator loss: 0.3252 - 2620 samples/s\n",
            "INFO - 04/04/25 16:19:55 - 0:05:46 - 428000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:19:58 - 0:05:49 - 432000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:20:01 - 0:05:52 - 436000 - Discriminator loss: 0.3252 - 2608 samples/s\n",
            "INFO - 04/04/25 16:20:05 - 0:05:55 - 440000 - Discriminator loss: 0.3252 - 2609 samples/s\n",
            "INFO - 04/04/25 16:20:08 - 0:05:58 - 444000 - Discriminator loss: 0.3252 - 2612 samples/s\n",
            "INFO - 04/04/25 16:20:11 - 0:06:02 - 448000 - Discriminator loss: 0.3252 - 2620 samples/s\n",
            "INFO - 04/04/25 16:20:14 - 0:06:05 - 452000 - Discriminator loss: 0.3252 - 2618 samples/s\n",
            "INFO - 04/04/25 16:20:17 - 0:06:08 - 456000 - Discriminator loss: 0.3251 - 2621 samples/s\n",
            "INFO - 04/04/25 16:20:20 - 0:06:11 - 460000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:20:23 - 0:06:14 - 464000 - Discriminator loss: 0.3252 - 2615 samples/s\n",
            "INFO - 04/04/25 16:20:26 - 0:06:17 - 468000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:20:29 - 0:06:20 - 472000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:20:32 - 0:06:23 - 476000 - Discriminator loss: 0.3252 - 2606 samples/s\n",
            "INFO - 04/04/25 16:20:35 - 0:06:26 - 480000 - Discriminator loss: 0.3252 - 2613 samples/s\n",
            "INFO - 04/04/25 16:20:38 - 0:06:29 - 484000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:20:41 - 0:06:32 - 488000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:20:44 - 0:06:35 - 492000 - Discriminator loss: 0.3251 - 2617 samples/s\n",
            "INFO - 04/04/25 16:20:47 - 0:06:38 - 496000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:20:50 - 0:06:41 - 500000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:20:53 - 0:06:44 - 504000 - Discriminator loss: 0.3251 - 2615 samples/s\n",
            "INFO - 04/04/25 16:20:57 - 0:06:47 - 508000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:21:00 - 0:06:50 - 512000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:21:03 - 0:06:54 - 516000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:21:06 - 0:06:57 - 520000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:21:09 - 0:07:00 - 524000 - Discriminator loss: 0.3251 - 2619 samples/s\n",
            "INFO - 04/04/25 16:21:12 - 0:07:03 - 528000 - Discriminator loss: 0.3252 - 2622 samples/s\n",
            "INFO - 04/04/25 16:21:15 - 0:07:06 - 532000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:21:18 - 0:07:09 - 536000 - Discriminator loss: 0.3252 - 2611 samples/s\n",
            "INFO - 04/04/25 16:21:21 - 0:07:12 - 540000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:21:24 - 0:07:15 - 544000 - Discriminator loss: 0.3251 - 2615 samples/s\n",
            "INFO - 04/04/25 16:21:27 - 0:07:18 - 548000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:21:30 - 0:07:21 - 552000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:21:33 - 0:07:24 - 556000 - Discriminator loss: 0.3251 - 2607 samples/s\n",
            "INFO - 04/04/25 16:21:36 - 0:07:27 - 560000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:21:39 - 0:07:30 - 564000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:21:42 - 0:07:33 - 568000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:21:46 - 0:07:37 - 572000 - Discriminator loss: 0.3251 - 2463 samples/s\n",
            "INFO - 04/04/25 16:21:49 - 0:07:40 - 576000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:21:53 - 0:07:44 - 580000 - Discriminator loss: 0.3252 - 2228 samples/s\n",
            "INFO - 04/04/25 16:21:56 - 0:07:47 - 584000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:22:00 - 0:07:51 - 588000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:22:04 - 0:07:54 - 592000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:22:07 - 0:07:58 - 596000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:22:11 - 0:08:02 - 600000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:22:14 - 0:08:05 - 604000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:22:18 - 0:08:09 - 608000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:22:22 - 0:08:12 - 612000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:22:25 - 0:08:16 - 616000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:22:29 - 0:08:20 - 620000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:22:32 - 0:08:23 - 624000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:22:36 - 0:08:27 - 628000 - Discriminator loss: 0.3251 - 2223 samples/s\n",
            "INFO - 04/04/25 16:22:39 - 0:08:30 - 632000 - Discriminator loss: 0.3251 - 2236 samples/s\n",
            "INFO - 04/04/25 16:22:43 - 0:08:34 - 636000 - Discriminator loss: 0.3251 - 2235 samples/s\n",
            "INFO - 04/04/25 16:22:47 - 0:08:37 - 640000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:22:50 - 0:08:41 - 644000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:22:54 - 0:08:45 - 648000 - Discriminator loss: 0.3251 - 2237 samples/s\n",
            "INFO - 04/04/25 16:22:57 - 0:08:48 - 652000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:23:01 - 0:08:52 - 656000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:23:05 - 0:08:55 - 660000 - Discriminator loss: 0.3251 - 2234 samples/s\n",
            "INFO - 04/04/25 16:23:08 - 0:08:59 - 664000 - Discriminator loss: 0.3251 - 2231 samples/s\n",
            "INFO - 04/04/25 16:23:12 - 0:09:03 - 668000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:23:15 - 0:09:06 - 672000 - Discriminator loss: 0.3251 - 2234 samples/s\n",
            "INFO - 04/04/25 16:23:19 - 0:09:10 - 676000 - Discriminator loss: 0.3251 - 2229 samples/s\n",
            "INFO - 04/04/25 16:23:22 - 0:09:13 - 680000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:23:26 - 0:09:17 - 684000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:23:30 - 0:09:21 - 688000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:23:33 - 0:09:24 - 692000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:23:37 - 0:09:28 - 696000 - Discriminator loss: 0.3251 - 2231 samples/s\n",
            "INFO - 04/04/25 16:23:40 - 0:09:31 - 700000 - Discriminator loss: 0.3251 - 2236 samples/s\n",
            "INFO - 04/04/25 16:23:44 - 0:09:35 - 704000 - Discriminator loss: 0.3251 - 2233 samples/s\n",
            "INFO - 04/04/25 16:23:48 - 0:09:38 - 708000 - Discriminator loss: 0.3251 - 2229 samples/s\n",
            "INFO - 04/04/25 16:23:51 - 0:09:42 - 712000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:23:55 - 0:09:46 - 716000 - Discriminator loss: 0.3251 - 2229 samples/s\n",
            "INFO - 04/04/25 16:23:58 - 0:09:49 - 720000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:24:02 - 0:09:53 - 724000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:24:05 - 0:09:56 - 728000 - Discriminator loss: 0.3251 - 2232 samples/s\n",
            "INFO - 04/04/25 16:24:09 - 0:10:00 - 732000 - Discriminator loss: 0.3251 - 2230 samples/s\n",
            "INFO - 04/04/25 16:24:13 - 0:10:04 - 736000 - Discriminator loss: 0.3251 - 2234 samples/s\n",
            "INFO - 04/04/25 16:24:16 - 0:10:07 - 740000 - Discriminator loss: 0.3251 - 2228 samples/s\n",
            "INFO - 04/04/25 16:24:19 - 0:10:10 - 744000 - Discriminator loss: 0.3251 - 2602 samples/s\n",
            "INFO - 04/04/25 16:24:22 - 0:10:13 - 748000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:24:25 - 0:10:16 - 752000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:24:28 - 0:10:19 - 756000 - Discriminator loss: 0.3251 - 2615 samples/s\n",
            "INFO - 04/04/25 16:24:32 - 0:10:22 - 760000 - Discriminator loss: 0.3251 - 2617 samples/s\n",
            "INFO - 04/04/25 16:24:35 - 0:10:25 - 764000 - Discriminator loss: 0.3251 - 2619 samples/s\n",
            "INFO - 04/04/25 16:24:38 - 0:10:29 - 768000 - Discriminator loss: 0.3252 - 2614 samples/s\n",
            "INFO - 04/04/25 16:24:41 - 0:10:32 - 772000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:24:44 - 0:10:35 - 776000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:24:47 - 0:10:38 - 780000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:24:50 - 0:10:41 - 784000 - Discriminator loss: 0.3251 - 2609 samples/s\n",
            "INFO - 04/04/25 16:24:53 - 0:10:44 - 788000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:24:56 - 0:10:47 - 792000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:24:59 - 0:10:50 - 796000 - Discriminator loss: 0.3251 - 2606 samples/s\n",
            "INFO - 04/04/25 16:25:02 - 0:10:53 - 800000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:25:05 - 0:10:56 - 804000 - Discriminator loss: 0.3251 - 2621 samples/s\n",
            "INFO - 04/04/25 16:25:08 - 0:10:59 - 808000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:25:11 - 0:11:02 - 812000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:25:14 - 0:11:05 - 816000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:25:17 - 0:11:08 - 820000 - Discriminator loss: 0.3251 - 2608 samples/s\n",
            "INFO - 04/04/25 16:25:21 - 0:11:11 - 824000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:25:24 - 0:11:14 - 828000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:25:27 - 0:11:18 - 832000 - Discriminator loss: 0.3252 - 2615 samples/s\n",
            "INFO - 04/04/25 16:25:30 - 0:11:21 - 836000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:25:33 - 0:11:24 - 840000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:25:36 - 0:11:27 - 844000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:25:39 - 0:11:30 - 848000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:25:42 - 0:11:33 - 852000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:25:45 - 0:11:36 - 856000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:25:48 - 0:11:39 - 860000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:25:51 - 0:11:42 - 864000 - Discriminator loss: 0.3251 - 2608 samples/s\n",
            "INFO - 04/04/25 16:25:54 - 0:11:45 - 868000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:25:57 - 0:11:48 - 872000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:26:00 - 0:11:51 - 876000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:26:03 - 0:11:54 - 880000 - Discriminator loss: 0.3251 - 2607 samples/s\n",
            "INFO - 04/04/25 16:26:06 - 0:11:57 - 884000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:26:10 - 0:12:00 - 888000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:26:13 - 0:12:03 - 892000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:26:16 - 0:12:07 - 896000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:26:19 - 0:12:10 - 900000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:26:22 - 0:12:13 - 904000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:26:25 - 0:12:16 - 908000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:26:28 - 0:12:19 - 912000 - Discriminator loss: 0.3251 - 2611 samples/s\n",
            "INFO - 04/04/25 16:26:31 - 0:12:22 - 916000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:26:34 - 0:12:25 - 920000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:26:37 - 0:12:28 - 924000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:26:40 - 0:12:31 - 928000 - Discriminator loss: 0.3251 - 2610 samples/s\n",
            "INFO - 04/04/25 16:26:43 - 0:12:34 - 932000 - Discriminator loss: 0.3251 - 2605 samples/s\n",
            "INFO - 04/04/25 16:26:46 - 0:12:37 - 936000 - Discriminator loss: 0.3251 - 2614 samples/s\n",
            "INFO - 04/04/25 16:26:49 - 0:12:40 - 940000 - Discriminator loss: 0.3251 - 2612 samples/s\n",
            "INFO - 04/04/25 16:26:52 - 0:12:43 - 944000 - Discriminator loss: 0.3251 - 2615 samples/s\n",
            "INFO - 04/04/25 16:26:55 - 0:12:46 - 948000 - Discriminator loss: 0.3251 - 2617 samples/s\n",
            "INFO - 04/04/25 16:26:59 - 0:12:49 - 952000 - Discriminator loss: 0.3251 - 2606 samples/s\n",
            "INFO - 04/04/25 16:27:02 - 0:12:52 - 956000 - Discriminator loss: 0.3251 - 2618 samples/s\n",
            "INFO - 04/04/25 16:27:05 - 0:12:55 - 960000 - Discriminator loss: 0.3251 - 2620 samples/s\n",
            "INFO - 04/04/25 16:27:08 - 0:12:59 - 964000 - Discriminator loss: 0.3251 - 2615 samples/s\n",
            "INFO - 04/04/25 16:27:11 - 0:13:02 - 968000 - Discriminator loss: 0.3251 - 2620 samples/s\n",
            "INFO - 04/04/25 16:27:14 - 0:13:05 - 972000 - Discriminator loss: 0.3251 - 2616 samples/s\n",
            "INFO - 04/04/25 16:27:17 - 0:13:08 - 976000 - Discriminator loss: 0.3251 - 2613 samples/s\n",
            "INFO - 04/04/25 16:27:20 - 0:13:11 - 980000 - Discriminator loss: 0.3251 - 2601 samples/s\n",
            "INFO - 04/04/25 16:27:23 - 0:13:14 - 984000 - Discriminator loss: 0.3251 - 2588 samples/s\n",
            "INFO - 04/04/25 16:27:26 - 0:13:17 - 988000 - Discriminator loss: 0.3251 - 2591 samples/s\n",
            "INFO - 04/04/25 16:27:29 - 0:13:20 - 992000 - Discriminator loss: 0.3251 - 2598 samples/s\n",
            "INFO - 04/04/25 16:27:32 - 0:13:23 - 996000 - Discriminator loss: 0.3251 - 2591 samples/s\n",
            "INFO - 04/04/25 16:27:36 - 0:13:26 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 16:27:36 - 0:13:26 - 1500 source words - nn - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 16:27:36 - 0:13:26 - 1500 source words - nn - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 16:27:36 - 0:13:26 - 1500 source words - nn - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 16:27:36 - 0:13:26 - Found 2032 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n",
            "INFO - 04/04/25 16:27:39 - 0:13:30 - 1500 source words - csls_knn_10 - Precision at k = 1: 0.000000\n",
            "INFO - 04/04/25 16:27:39 - 0:13:30 - 1500 source words - csls_knn_10 - Precision at k = 5: 0.000000\n",
            "INFO - 04/04/25 16:27:39 - 0:13:30 - 1500 source words - csls_knn_10 - Precision at k = 10: 0.000000\n",
            "INFO - 04/04/25 16:27:40 - 0:13:31 - Building the train dictionary ...\n",
            "INFO - 04/04/25 16:27:40 - 0:13:31 - New train dictionary of 1034 pairs.\n",
            "INFO - 04/04/25 16:27:40 - 0:13:31 - Mean cosine (nn method, S2T build, 10000 max size): 0.23987\n",
            "INFO - 04/04/25 16:27:49 - 0:13:39 - Building the train dictionary ...\n",
            "INFO - 04/04/25 16:27:49 - 0:13:39 - New train dictionary of 1033 pairs.\n",
            "INFO - 04/04/25 16:27:49 - 0:13:39 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.23812\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/evaluation/evaluator.py:232: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.src_emb.weight[i:i + bs].data, volatile=True)\n",
            "/workspace/lipsync/LatentSync/expssssss/MUSE/src/evaluation/evaluator.py:237: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.tgt_emb.weight[i:i + bs].data, volatile=True)\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - Discriminator source / target predictions: 0.89977 / 0.09989\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - Discriminator source / target / global accuracy: 1.00000 / 0.99997 / 0.99999\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - __log__:{\"n_epoch\": 0, \"precision_at_1-nn\": 0.0, \"precision_at_5-nn\": 0.0, \"precision_at_10-nn\": 0.0, \"precision_at_1-csls_knn_10\": 0.0, \"precision_at_5-csls_knn_10\": 0.0, \"precision_at_10-csls_knn_10\": 0.0, \"mean_cosine-nn-S2T-10000\": 0.23987124860286713, \"mean_cosine-csls_knn_10-S2T-10000\": 0.23812450468540192, \"dis_accu\": 0.9999888273149803, \"dis_src_pred\": 0.8997680760836602, \"dis_tgt_pred\": 0.09988534371218648}\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - * Best value for \"mean_cosine-csls_knn_10-S2T-10000\": 0.23812\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - * Saving the mapping to /workspace/lipsync/LatentSync/expssssss/MUSE/dumped/debug/skznf4jakj/best_mapping.pth ...\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - End of epoch 0.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - Decreasing learning rate: 0.10000000 -> 0.09800000\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - Starting adversarial training epoch 1...\n",
            "INFO - 04/04/25 16:27:49 - 0:13:40 - 000000 - Discriminator loss: 0.3251 - 2989 samples/s\n",
            "INFO - 04/04/25 16:27:53 - 0:13:43 - 004000 - Discriminator loss: 0.3251 - 2596 samples/s\n",
            "INFO - 04/04/25 16:27:56 - 0:13:47 - 008000 - Discriminator loss: 0.3251 - 2587 samples/s\n",
            "INFO - 04/04/25 16:27:59 - 0:13:50 - 012000 - Discriminator loss: 0.3251 - 2590 samples/s\n",
            "INFO - 04/04/25 16:28:02 - 0:13:53 - 016000 - Discriminator loss: 0.3251 - 2587 samples/s\n",
            "INFO - 04/04/25 16:28:05 - 0:13:56 - 020000 - Discriminator loss: 0.3251 - 2586 samples/s\n",
            "INFO - 04/04/25 16:28:08 - 0:13:59 - 024000 - Discriminator loss: 0.3251 - 2581 samples/s\n",
            "INFO - 04/04/25 16:28:11 - 0:14:02 - 028000 - Discriminator loss: 0.3251 - 2589 samples/s\n",
            "INFO - 04/04/25 16:28:14 - 0:14:05 - 032000 - Discriminator loss: 0.3251 - 2582 samples/s\n",
            "INFO - 04/04/25 16:28:17 - 0:14:08 - 036000 - Discriminator loss: 0.3251 - 2588 samples/s\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/lipsync/LatentSync/expssssss/MUSE/unsupervised.py\", line 118, in <module>\n",
            "    trainer.dis_step(stats)\n",
            "  File \"/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py\", line 91, in dis_step\n",
            "    x, y = self.get_dis_xy(volatile=True)\n",
            "  File \"/workspace/lipsync/LatentSync/expssssss/MUSE/src/trainer.py\", line 80, in get_dis_xy\n",
            "    y = Variable(y.cuda() if self.params.cuda else y)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python unsupervised.py \\\n",
        "  --src_lang en --tgt_lang hi \\\n",
        "  --src_emb /data/wiki.en.vec --tgt_emb /data/wiki.hi.vec \\\n",
        "  --normalize_embeddings center \\\n",
        "  --map_id_init 0 \\\n",
        "  --n_refinement 10 \\\n",
        "  --dico_method csls_knn_10 \\\n",
        "  --dico_build \"S2T|T2S\" \\\n",
        "  --dico_eval /data/en-hi.5000-6500.txt \\\n",
        "  --dico_min_size 500 --dico_max_size 5000 \\\n",
        "  --dis_layers 3 --dis_hid_dim 1024 --dis_input_dropout 0.2 --dis_steps 10 \\\n",
        "  --export pth"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7974e8c1-7391-431f-96bb-91827cd7f3a2",
        "AjHe8tYks5au",
        "Kov9u8O5s-aa",
        "H_hBFG8ovjye",
        "b69af3ab-691d-4bb0-ae29-006f8d7cedaf",
        "bfXBH_F9wJzj",
        "QVbkP_OtzGnx",
        "vaYz5tS33wHH",
        "K-m4Gh4p3jiZ",
        "fb998b44-ade3-4f4b-a7ac-805f2cbb3667",
        "c2d8bb7d-0625-49f8-b385-19d9ecab2e79",
        "5f36a67d-2bfb-4f13-a362-f56c93f303d3",
        "17b4928a-2b3c-4131-84b3-83cc5336ac61",
        "ee060c7e-0c1d-44be-9b2c-754586d879fc",
        "j92CqzGd8IlD",
        "QNmzcOY-8rR1",
        "4qkwNwII9Lf3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "arv_p3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
